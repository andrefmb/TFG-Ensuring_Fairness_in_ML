% Plantilla para un Trabajo Fin de Grado de la Universidad de Granada,
% adaptada para el Doble Grado en Ingeniería Informática y Matemáticas.
%
%  Autor: Mario Román.
%  Licencia: GNU GPLv2.
%
% Esta plantilla es una adaptación al castellano de la plantilla
% classicthesis de André Miede, que puede obtenerse en:
%  https://ctan.org/tex-archive/macros/latex/contrib/classicthesis?lang=en
% La plantilla original se licencia en GNU GPLv2.
%
% Esta plantilla usa símbolos de la Universidad de Granada sujetos a la normativa
% de identidad visual corporativa, que puede encontrarse en:
% http://secretariageneral.ugr.es/pages/ivc/normativa
%
% La compilación se realiza con las siguientes instrucciones:
%   pdflatex --shell-escape main.tex
%   bibtex main
%   pdflatex --shell-escape main.tex

% Opciones del tipo de documento
\documentclass[oneside,openright,titlepage,numbers=noenddot,openany,headinclude,footinclude=true,
cleardoublepage=empty,abstractoff,BCOR=5mm,paper=a4,fontsize=12pt,main=spanish]{scrreprt}

% Paquetes de latex que se cargan al inicio. Cubren la entrada de
% texto, gráficos, código fuente y símbolos.
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{multirow}
\usepackage{upgreek}
\usepackage{graphicx} % Inclusión de imágenes.
\usepackage{grffile}  % Distintos formatos para imágenes.
\usepackage{longtable} % Tablas multipágina.
\usepackage{wrapfig} % Coloca texto alrededor de una figura.
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage[colorlinks=true]{hyperref}
\usepackage{tikz} % Diagramas conmutativos.
\usetikzlibrary{cd,shapes.geometric}
\usepackage{minted} % Código fuente.
\usepackage[T1]{fontenc}
\usepackage{natbib}
\usepackage[toc,page]{appendix}

\tikzcdset{m/.style={column sep=0pt,
    every arrow/.style={draw,thick,-latex,minimum height=1.0em},
    /tikz/w/.style={fill=white},
    cells={nodes={circle,inner sep=-0.5pt,fill=gray!30,draw,minimum height=2em}}}}  

% Plantilla classicthesis
\usepackage[beramono,eulerchapternumbers,linedheaders,parts,a5paper,dottedtoc,
manychapters,pdfspacing]{classicthesis}

% Geometría y espaciado de párrafos.
\setcounter{secnumdepth}{0}
\usepackage{enumitem}
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\setlist[enumerate]{topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex}
\usepackage[top=1in, bottom=1.5in, left=1in, right=1in]{geometry}
\setlength\itemsep{0em}
\setlength{\parindent}{0pt}
\usepackage{parskip}

% Profundidad de la tabla de contenidos.
\setcounter{secnumdepth}{3}

% Usa el paquete minted para mostrar trozos de código.
% Pueden seleccionarse el lenguaje apropiado y el estilo del código.
\usepackage{minted}
\usemintedstyle{colorful}
\setminted{fontsize=\small}
\setminted[python]{linenos=false,fontsize=\small}
\renewcommand{\theFancyVerbLine}{\sffamily\textcolor[rgb]{0.5,0.5,1.0}{\oldstylenums{\arabic{FancyVerbLine}}}}

% Archivos de configuración.
\input{macros}  % En macros.tex se almacenan las opciones y comandos para escribir matemáticas.
\graphicspath{ {./images/} }
\input{classicthesis-config} % En classicthesis-config.tex se almacenan las opciones propias de la plantilla.

% Color institucional UGR
% \definecolor{ugrColor}{HTML}{ed1c3e} % Versión clara.
\definecolor{ugrColor}{HTML}{c6474b}  % Usado en el título.
\definecolor{ugrColor2}{HTML}{c6474b} % Usado en las secciones.

% Datos de portada
\usepackage{titling} % Facilita los datos de la portada
\author{Daniel Bolaños Martínez} 
\date{\today}
\title{Herramientas para Garantizar \\Justicia en Aprendizaje Automático}

% Portada
\include{titlepage}
\usepackage{wallpaper}
\usepackage[main=spanish]{babel}


\begin{document}

\ThisULCornerWallPaper{1}{ugrA4.pdf}
\maketitle

\include{preliminares/titulo}

\newpage
\vspace*{\fill}
\doclicenseThis
The source code of this text and developed programs are available in the Github repository \href{https://github.com/danibolanos/TFG-Guarantee_Fairness_in_ML.git}{danibolanos/TFG-Guarantee\_Fairness\_in\_ML}

\include{./preliminares/declaracion-originalidad}

\include{./preliminares/agradecimientos}

\include{./preliminares/resumen}

\include{./preliminares/abstract}

\tableofcontents

%\listoffigures

\chapter{Introducción}

Actualmente, los algoritmos de aprendizaje automático se utilizan en ámbitos diversos con un gran impacto en la sociedad, como son: el proceso de concesión de créditos bancarios (\cite{prestamo2018}), selección de personal para un puesto de trabajo (\cite{contratar2015}) o decisión de una condena en justicia penal (\cite{condena2016}). Estos ejemplos de aplicación son propensos a la discriminación, la cual está prohibida por la legislación internacional (\cite{ley1964}). Las causas de las disparidades en los sistemas de aprendizaje automático provienen esencialmente del sesgo humano que existe en los conjuntos de datos de entrenamiento debido a razones históricas. Algunas de las posibles causas (\cite{bigdata2016}) son:

\begin{itemize}
    \item Los sistemas de aprendizaje automático mantienen la discriminación existente en los datos antiguos debido al sesgo humano. Por ejemplo, si un sistema de contratación a la hora de seleccionar a los aspirantes al cargo utiliza como etiquetas de predicción las decisiones tomadas por un directivo en lugar de sus capacidades reales, en la mayoría de los casos se podrían rechazar candidatos con un alto nivel de rendimiento.
    \item Normalmente la cantidad de ejemplos y la información que ofrecen sus características son menores para el grupo minoritario, por lo que es menos probable que se modelen correctamente con respecto a los individuos del grupo mayoritario. Esto tendrá desventajas a la hora de predecir sobre nuevos ejemplos del grupo desfavorecido.
    \item Si ya existe un sesgo inicial, probablemente se agravará con el tiempo. Por ejemplo, en el registro policial de delitos solo constan delitos observados por la policía. El departamento de policía tiende a enviar más agentes a lugares donde se ha detectado una mayor tasa de delincuencia inicialmente y, por tanto, será más probable que se detecten delitos en esas regiones. 
    \item Aunque los atributos sensibles no se utilicen en el entrenamiento de un sistema de aprendizaje automático, podrán existir atributos derivados de estos. Si se incluyen estas características, el sesgo seguirá presente. A veces, es muy difícil determinar si un atributo relevante está correlacionado con los atributos sensibles y si debemos incluirlo o no en el proceso de entrenamiento.
\end{itemize}

Estos problemas han llevado al desarrollo de numerosas investigaciones sobre la equidad en el ámbito del aprendizaje automático, enfocadas en cómo surge la discriminación, cómo puede medirse y cómo puede mitigarse. El objetivo de la equidad será por tanto, diseñar algoritmos que hagan predicciones justas, evitando perjudicar a un determinado grupo de la población.

A pesar de estos avances, en algunos escenarios, la discriminación en los modelos sigue siendo difícil de abordar y mucho más de entender debido principalmente a: 

\begin{itemize}
    \item Las aplicaciones generalmente actúan como modelos de caja negra en las cuales tenemos acceso restringido al clasificador por cuestiones de privacidad o derechos de propiedad intelectual de los datos utilizados (\cite{blackbox2014}). Por ejemplo, si estamos usando una API de predicción.
	\item Los modelos se despliegan en una población dónde la distribución de los datos no refleja los patrones contenidos en los datos de entrenamiento (\cite{distributionmodel2017}). Por ejemplo, podrían aparecer diferencias entre los individuos del grupo debido a un cambio en la distribución de la población de interés.
\end{itemize}

En este trabajo, estudiaremos cómo se formaliza el concepto de equidad en el ámbito del aprendizaje automático y presentaremos estas formalizaciones en las diferentes ramas de la ingeniería informática y las matemáticas. Las medidas de equidad parten de las ideas de justicia distributiva del ámbito de las ciencias sociales y, por ello, pueden aparecer conceptos que entren en conflicto con otros, por lo que las predicciones producidas por los algoritmos y modelos que las utilizan también diferirán enormemente. 

Desde el punto de vista práctico, es importante estudiar estos criterios de equidad y sus implicaciones realizando un análisis teórico y empírico a partir de las nociones de la literatura de las ciencias sociales. Este análisis tendrá la intención de ayudar a determinar la bondad de las formalizaciones existentes y poder valorar las ventajas e inconvenientes de cada criterio con el objetivo de mejorar o construir nuevas formalizaciones de equidad en un futuro. 

\section{Análisis del problema}

La equidad en el aprendizaje automático, tiene como objetivo estudiar y mitigar la discriminación en los procesos de toma de decisiones algorítmicas. Actualmente podemos encontrar tres enfoques dentro de la mitigación de los sesgos. Los métodos de preprocesamiento que intentan corregir los sesgos de los datos introducidos en el modelo, los métodos de procesamiento interno que tratan de realizar la correción de los sesgos producidos durante el proceso de aprendizaje y finalmente, los métodos de posprocesamiento que tienen como objetivo corregir el resultado de un modelo sesgado.

A estos enfoques se le suman la gran cantidad de criterios de justicia que surgen a partir de la literatura de las ciencias sociales aplicadas al aprendizaje automático. Podemos hacer una primera aproximación a estas familias y por ende a la formalización de los conceptos de equidad, intentando dar respuesta a las siguientes dos preguntas:

\begin{itemize}
    \item ¿Paridad o preferencia?: si buscamos equidad para lograr una paridad entre los individuos del grupo o en cambio queremos satisfacer unas preferencias dentro del mismo.
    \item ¿Tratamiento o impacto?: si tratamos de mantener la equidad durante el tratamiento de los datos o por el contrario en los resultados producidos por el modelo (impacto).
\end{itemize}

Dando respuesta a las preguntas anteriores, surgen los siguientes criterios de equidad que se resumen en el Cuadro \ref{tab:table1} (\cite{formalizing2018}).\\

\begin{table}[h]
\centering
\resizebox{12.0cm}{!} {
\begin{tabular}{|c|c|c|}
\hline
                     & \textbf{Paridad}                                                                                        & \textbf{Preferencia}   \\ \hline
\textbf{Tratamiento} & \begin{tabular}[c]{@{}c@{}}Equidad por desconocimiento\\ Medidas causales\end{tabular}                       & Tratamiento preferente \\ \hline
\textbf{Impacto}     & \begin{tabular}[c]{@{}c@{}}Equidad de grupo\\ Equidad individual\end{tabular} & Impacto preferente     \\ \hline
\end{tabular}
}
\caption{Formalización de los criterios de equidad.}
\label{tab:table1}
\end{table}

Debido a la gran cantidad de opciones que se nos presentan, nos gustaría saber qué nociones de equidad tienen menos limitaciones en la práctica y qué método de mitigación de sesgo aporta menos complejidad en los algoritmos de predicción. De esta forma, podremos conocer las virtudes de cada familia de equidad y ser capaces de desarrollar nuevos conceptos o adaptar cada uno al caso de estudio que mejores resultados nos proporcione.

\section{Objetivos del trabajo}

Nuestro objetivo principal es hallar modelos en el ámbito del aprendizaje automático que tengan una buena relación equidad-precisión. Queremos encontrar una mitigación del sesgo que no aumente la complejidad de los modelos utilizados, no necesite conocimiento específico del dominio y tenga el menor número de limitaciones en su aplicación sobre problemas del mundo real.

Abordaremos estas tareas realizando un análisis teórico de las nociones de equidad y de los algoritmos construidos en base a ellas. Estudiaremos qué criterios aportan más beneficios sobre un modelo de caja negra del que únicamente podamos conocer los datos antes o después de ser procesados, e indagaremos sobre los métodos de optimización de las nociones de justicia con el propósito de satisfacer la equidad en el modelo minimizando la pérdida de rendimiento del mismo.

Como resultado de los objetivos planteados, sugerimos una revisión bibliográfica exhaustiva
de los métodos de mitigación del sesgo y de las medidas de equidad que se nos presentan en diferentes artículos como \cite{formalizing2018} o \cite{definitions2018}. Nos proponemos detallar las herramientas matemáticas que nos serán útiles para la formalización de las medidas de equidad, haciendo especial hincapié en la inferencia causal como base sobre la que se construye el modelo de equidad contrafactual que estudiaremos en este trabajo.

Finalmente realizaremos un análisis de la herramienta de software Aequitas (\cite{aequitas2019}) dedicado al estudio de equidad sobre problemas del mundo real. Haremos pruebas sobre un conjunto de datos y evaluaremos sus carencias respecto a la teoría. Además replicaremos un ejemplo utilizando la noción de equidad contrafactual (\cite{counterfactual2018}) en el lenguaje de programación Python y aportaremos herramientas de visualización para facilitar la interpretación de los resultados a otros investigadores o científicos de datos interesados en el tema.

\section{Contribuciones}

En resumen, las contribuciones principales de este proyecto son:

\begin{itemize}
    \item Discutir y formalizar las distintas familias de medidas de equidad y algoritmos de mitigación de sesgo de mayor interés en este ámbito.
    \item Facilitar una demostración alternativa del teorema de imposibilidad para mostrar las incompatibilidades entre los diferentes criterios de equidad de grupo.
    \item Proporcionar un análisis empírico utilizando el software Aequitas para las evaluaciones de las medidas de justicia estudiadas teóricamente.
    \item Replicar un ejemplo del mundo real sobre un modelo causal en el lenguaje de programación Python basándonos en el concepto de equidad contrafactual, disponible en: \href{https://github.com/danibolanos/TFG-Guarantee_Fairness_in_ML.git}{danibolanos/TFG-Guarantee\_Fairness\_in\_ML}
    \item Implementar gráficas en Python para la interpretación de los resultados al aplicar las técnicas de equidad contrafactual.
    \item \textcolor{red}{Comparar nociones de equidad/Esquema prototipo}.
\end{itemize}

\section{Esquema general}

\textcolor{red}{Completar y revisar al final}

En la Sección , formalizaremos los conceptos de equidad para cada una de las familias propuestas por la literatura del aprendizaje automático. En la Sección $X$ discutiremos las limitaciones de algunos conceptos vistos en la sección anterior. En la Parte \ref{part:analisis_exp} crearemos y discutiremos herramientas para el análisis de los conceptos de equidad y analizaremos empíricamente cada concepto en base a herramientas de visualización de sus resultados. Por último, en la Parte \ref{part:debate_fut}, discutiremos diferentes vías para la formalización de conceptos de equidad futuros.

\ctparttext{
  \color{black}
  \begin{center}
    Definiciones y resultados relativos a teoría de probabilidad, estadística y teoría de grafos.
  \end{center}
}
\part{Principios matemáticos básicos}

\chapter{Teoría de la Probabilidad}

En este capítulo incluiremos definiciones y resultados previos de la teoría de probabilidad que utilizaremos a lo largo del desarrollo del trabajo. La fuente principal utilizada en este capítulo parte del trabajo contenido en \cite{probability2014}.

\section{Espacio de probabilidad}

Construiremos la teoría asumiendo que existe un conjunto no vacío $\Omega$ que representa al conjunto de todos los posibles resultados de un experimento. Llamaremos \textit{suceso} a cualquier subconjunto de $\Omega$.\\

\begin{definition}[$\sigma$-álgebra]
Sea $\mathcal{P}(\Omega)$ el conjunto de partes de $\Omega$. Llamaremos \textit{$\sigma$-álgebra} a $\mathcal{A} \subset \mathcal{P}(\Omega)$ que satisfaga:

\begin{itemize}
\item $\mathcal{A}$ contiene al conjunto vacío: $\emptyset \in \mathcal{A}$.
\item $\mathcal{A}$ es cerrado bajo complementarios: si $A \in \mathcal{A}$, entonces $\Omega\ \backslash\ A \in \mathcal{A}$.
\item $\mathcal{A}$ es cerrado bajo uniones numerables: si $A_i \in \mathcal{A}$ para todo $i \in \mathbb{N}$ y $B=\cup_{i\in \mathbb{N}}A_i$, entonces $B \in \mathcal{A}$.
\end{itemize}
\end{definition}\

De las propiedades anteriores deducimos que $\Omega \in \mathcal{A}$ y que $\mathcal{A}$ también es cerrado bajo intersecciones numerables.\\

\begin{definition}[Medida de probabilidad] Una \textit{medida de probabilidad} $P$ sobre un espacio de medida $(\Omega, \mathcal{A})$ es una función $P\colon \mathcal{A} \to [0,1]$ que verifica:

\begin{itemize}
    \item $P(\Omega) = 1$.
    \item Si $A \subset \Omega$, entonces $P(A) \geq 0$.
    \item $P$ es $\sigma$-aditiva, es decir: dada $\{A_i\}_{i\in \mathbb{N}}$ una sucesión de conjuntos disjuntos dos a dos en $\mathcal{A}$, entonces
  \[
  P\Big(\bigcup_{i\in \mathbb{N}}A_i \Big) = \sum_{i \in \mathbb{N}}P(A_i).
  \]
\end{itemize}
\end{definition}\

Denotaremos por \textit{suceso seguro} al suceso que siempre va a ocurrir. A partir de la primera condición sabemos que el suceso seguro tiene la máxima probabilidad posible. La segunda condición garantiza la no negatividad de la probabilidad. Por último, la tercera condición implica que dado un conjunto de sucesos disjuntos dos a dos, la probabilidad de que ocurra cualquiera de ellos es igual a la suma de las probabilidades de cada uno.\\

\begin{proposition} Toda medida de probabilidad, $P$, cumple:

\begin{itemize}
\item $P(\emptyset) = 0$.
\item Dados $A,B \in \mathcal{A}$, entonces $P(A \cup B) = P(A) + P(B) - P(A \cap B)$.
\end{itemize}
\end{proposition}\

\begin{definition}[Espacio de probabilidad]
Definimos como \textit{espacio de medida} a la tupla $(\Omega, \mathcal{A}, \mu)$ donde $\mu \colon \mathcal{A}\to \Ro$ es una medida en $(\Omega, \mathcal{A})$ y \textit{espacio de probabilidad} a la tupla formada por $(\Omega, \mathcal{A}, P)$ donde $P$ una medida de probabilidad en $(\Omega, \mathcal{A})$.
\end{definition}

\section{Variables aleatorias}

Una \textit{variable aleatoria} es una función que asigna un valor, normalmente numérico, al resultado de un experimento aleatorio. Dada una variable aleatoria no es posible saber su valor exacto al ser medida, aunque sí conocemos una distribución de probabilidad para describir la probabilidad de que se den los diferentes valores. A continuación, formalizaremos el concepto de variable aleatoria.\\

\begin{definition}[Función medible]
Sean $(\Omega_1,\mathcal{A})$ y $(\Omega_2, \mathcal{S})$ dos espacios de medida. Una \textit{función medible} $n$-dimensional es una función $X\colon \Omega_1 \to \Omega_2$ que verifica: $$X^{-1}(S)=\{\omega \in \Omega \ : \ X(\omega)\in S\} \in \mathcal{A}, \ \text{para todo} \ S \in \mathcal{S}.$$
\end{definition}\

\begin{definition}[Variable aleatoria]
Sea $(\Omega_1, \mathcal{A},P)$ un espacio de probabilidad y $(\Omega_2, \mathcal{S})$ un espacio de medida. Una \textit{variable aleatoria} $\nm{X}=(X_1,\dots,X_n)$ es una función medible $\nm{X}\colon \Omega_1 \to \Omega_2$ del espacio de probabilidad al espacio de medida.
\end{definition}\

Diremos que la variable aleatoria es \textit{unidimensional} si $n=1$ y \textit{multivariante} cuando $n > 1$. Cuando tengamos una variable aleatoria multivariante $\nm{X}=(X_1,\dots,X_n)$, llamaremos a $\nm{X}$ variable aleatoria conjunta o \textit{vector aleatorio} y a cada $X_i$ con $i=1,\dots,n$ variable aleatoria marginal.\\

%Utilizando la estabilidad de las funciones medibles por operaciones algebraicas básicas (suma, producto, cociente y producto por escalares), tenemos que las variables aleatorias, también son estables mediante estas operaciones alebraicas.\\

\begin{definition}[Probabilidad inducida]
Sea $(\Omega_1, \mathcal{A},P)$ un espacio de probabilidad y $(\Omega_2, \mathcal{S})$ un espacio de medida. La \textit{probabilidad inducida} por una variable aleatoria $X$ viene dada por la función: $$P_X(S)=P(X^{-1}(S)), \ \text{para todo} \ S \in \mathcal{S}.$$
\end{definition}\

\begin{example}
Consideramos el lanzamiento de una moneda. Los posibles resultados del experimento serán cara o cruz, los cuales serán nuestros sucesos aleatorios. Definiremos nuestra variable aleatoria como: $$X= \left \{
\begin{array}{l l}
0, & \mbox{si sale cara}, \\
1, & \mbox{si sale cruz}.
\end{array}
\right.$$
\end{example}\

\begin{definition}[Función de distribución]
La \textit{función de distribución} acumulada de una variable aleatoria $X$ es una función $F\colon \R\to [0,1]$ definida como: $$F(x)=P(X\leq x).$$
\end{definition}\

\begin{proposition} La función de distribución acumulada $F$ asociada a la variable aleatoria $X$ satisface las siguientes propiedades:

\begin{itemize}
    \item $\displaystyle \lim_{x\to +\infty} F(x)=1$.
    \item $\displaystyle \lim_{x\to -\infty} F(x)=0$.
    \item Es creciente, es decir, si $x_1 \leq x_2$, entonces $F(x_1) \leq F(x_2)$.
    \item Es continua por la derecha, es decir, $\displaystyle \lim_{x\to a^+} F(x)=F(a^+)$.
\end{itemize}
\end{proposition}\

Si la imagen de la variable aleatoria $X$ es numerable, diremos que la variable aleatoria es \textit{discreta} y viene descrita por la función de probabilidad $p$ que devuelve la probabilidad de $X$ de ser igual a cierto valor $x$. 

Si la imagen de la variable aleatoria $X$ es infinita no numerable, diremos que la variable aleatoria es \textit{continua} y viene descrita por la función de densidad $f$ que caracteriza la posibilidad relativa de que $X$ tome un valor cercano a $x$.\\

\begin{definition}[Función de probabilidad]
Sea $X$ una variable aleatoria discreta con posibles valores $\{x_1,\dots,x_n\}$ su \textit{función de probabilidad} se define como 

$$f(x)= \left \{
\begin{array}{l l}
P(X=x), & \mbox{si } x\in \{x_1,\dots,x_n\}, \\
0, & \mbox{en otro caso}.
\end{array}
\right.$$
\end{definition}\

\begin{definition}[Función de densidad]
Sea $X$ una variable aleatoria continua se dice que la función integrable no-negativa $f$ es su \textit{función de densidad} si para todo $x \in \R$, $$P(X\leq x)=\int_{-\infty}^x f(u) \ du.$$
\end{definition}

\subsection{Distribución conjunta}

Una \textit{distribución conjunta} es la distribución de probabilidad de la intersección de las realizaciones de dos o más variables aleatorias cualesquiera. A continuación, definiremos algunos conceptos que ya discutimos para una única variable aleatoria para el caso multivariante.\\

\begin{definition}[Función de distribución conjunta]
La \textit{función de distribución conjunta} de un vector aleatorio $\nm{X}=(X_1,\dots,X_n)$ es una función $F_{\nm{X}}\colon \R^n \to [0,1]$ definida como: $$F_{\nm{X}}(x_1,\dots,x_n)=P(X_1\leq x_1,\dots,X_n\leq x_n).$$
\end{definition}\

\begin{definition}[Función de probabilidad conjunta]
Sea un vector aleatorio discreto $\nm{X}=(X_1,\dots,X_n)$ con posibles valores en el conjunto producto $$\mathcal{X}=\Big\lbrace \{x_{11},\dots,x_{1n}\}\times \cdots \times \{x_{n1},\dots,x_{nn}\} \Big\rbrace$$ su \textit{función de probabilidad conjunta} se define como

$$f_{\nm{X}}(x_1,\dots,x_n)= \left \{
\begin{array}{l l}
P(X_1=x_1,\dots,X_n=x_n), & \mbox{si } (x_1,\dots,x_n)\in \mathcal{X}, \\
0, & \mbox{en otro caso}.
\end{array}
\right.$$
\end{definition}\

\begin{definition}[Función de densidad conjunta]
Sea $\nm{X}=(X_1,\dots,X_n)$ un vector aleatorio continuo se dice que la función integrable no-negativa $f_{\nm{X}}$ es su \textit{función de densidad conjunta} si para todo $(x_1,\dots,x_n) \in \R^n$, $$P(X_1\leq x_1,\dots, X_n \leq x_n)=\int_{-\infty}^{x_1} \cdots \int_{-\infty}^{x_n} f_{\nm{X}}(u_1,\dots,u_n) \ du_1 \cdots du_n.$$ 
\end{definition}

\subsection{Distribución marginal}

La \textit{distribución marginal} de un subconjunto de un vector aleatorio es la distribución de probabilidad de las variables contenidas en el subconjunto. Procederemos a definir algunos conceptos que serán útiles cuando necesitemos calcular la distribución para alguna componente de $\nm{X}$.\\

\begin{definition}[Función de distribución marginal]
Sea $\nm{X}=(X_1,\dots,X_n)$ un vector aleatorio, la \textit{función de distribución marginal} de $X_1$ se define como: $$F_{X_1}(x_1)=\lim_{x_2\to +\infty}\cdots \lim_{x_n\to +\infty} F_{\nm{X}}(x_1,\dots,x_n).$$
\end{definition}\

\begin{definition}[Función de probabilidad marginal]
Sea un vector aleatorio discreto $\nm{X}=(X_1,\dots,X_n)$, la \textit{función de probabilidad marginal} de $X_1$ se define como: $$f_{X_1} (x_1)=\sum_{x_2} \cdots \sum_{x_n} f_{\nm{X}}(x_1,\dots,x_n),$$ y la \textit{función de probabilidad marginal} del subconjunto $(X_1,X_2)$ viene dada por:$$f_{X_1,X_2} (x_1,x_2)=\sum_{x_3} \cdots \sum_{x_n} f_{\nm{X}}(x_1,\dots,x_n).$$
\end{definition}\

\begin{definition}[Función de densidad marginal]
Sea $\nm{X}=(X_1,\dots,X_n)$ un vector aleatorio continuo, la \textit{función de densidad marginal} de $X_1$ se define como: $$f_{X_1}(x_1)=\int_{x_2} \cdots \int_{x_n} f_{\nm{X}}(x_1,\dots,x_n) \ dx_2\cdots dx_n,$$
y la \textit{función de densidad marginal} del subconjunto $(X_1,X_2)$ viene dada por:
$$f_{X_1,X_2}(x_1,x_2)=\int_{x_3} \cdots \int_{x_n} f_{\nm{X}}(x_1,\dots,x_n) \ dx_3\cdots dx_n.$$
\end{definition}

\section{Probabilidad condicional}

Llamamos \textit{probabilidad condicional} a la probabilidad de que ocurra un suceso $A$, sabiendo que también sucede otro suceso $B$. Algunos resultados relacionados y que usaremos en el trabajo son el Teorema de probabilidad total y el Teorema de Bayes.\\

\begin{definition}[Probabilidad condicional] \label{def:probcond}
Para cualesquiera dos sucesos $A,B \in \mathcal{A}$ tales que $P(B) > 0$. Definimos la \textit{probabilidad condicional} de $A$ sobre $B$ como: $$P(A \mid B)=\ddfrac{P(A\cap B)}{P(B)}.$$
\end{definition}\

\begin{theorem}[Teorema de la probabilidad total] \label{th:probtotal}
Sea $\{A_i \ : \ i=1,\dots,n\}$ una partición sobre $\Omega$ y sea $B\in \mathcal{A}$ un suceso arbitrario del que se conocen las probabilidades condicionales $P(B \mid A_i)$, entonces la probabilidad del suceso $B$ viene dada por $$P(B)=\sum_{i=1}^n P(A_i) P(B \mid A_i).$$
\end{theorem}

\begin{proof}
Utilizando que $\{A_i\in \mathcal{A} \ : \ i=1,\dots,n\}$ es una partición del espacio muestral $\Omega$ y por tanto cumple: 

\begin{itemize}
    \item $\displaystyle \Omega=\bigcup_{i=1}^n A_i$.
    \item $\displaystyle A_i \cap A_j = \emptyset, \text{ para todo } A_i\neq A_j.$
\end{itemize}

Podemos escribir el suceso $B$ como $$B=\bigcup_{i=1}^n B \cap A_i.$$ 
Como los conjuntos $A_i$ son disjuntos dos a dos, entonces los conjuntos $B\cap A_i$ también lo son. En consecuencia $$P(B)= P(B\cap A_1)+\dots+P(B\cap A_n).$$
Por último, como $B,A_i\in \mathcal{A}$ usando la Definición \ref{def:probcond}, tenemos $$P(B)=P(B \mid A_1) P(A_1)+\dots+P(B \mid A_n) P(A_n)=\sum_{i=1}^n P(B \mid A_i) P(A_i).$$
\end{proof}

\begin{theorem}[Teorema de Bayes] \label{th:bayes}
Para cualesquiera dos sucesos $A,B \in \mathcal{A}$ tales que $P(B)>0$, se tiene $$P(A \mid B)=\ddfrac{P(B \mid A) P(A)}{P(B)}.$$
\end{theorem}

\begin{proof}
 Utilizando la Definición \ref{def:probcond}, tenemos que $P(B \mid A) P(A)=P(B\cap A)$ y sabiendo que $P(B \cap A)=P(A \cap B)$ concluimos: $$\ddfrac{P(B \mid A) P(A)}{P(B)}=\ddfrac{P(B \cap A)}{P(B)}=P(A \mid B).$$
\end{proof}

\begin{notation}
A partir de este punto, denotaremos $P(A\cap B)$ como $P(A,B)$.
\end{notation}

\subsection{Distribución condicional}

Una \textit{distribución condicional} se define como la distribución de una de las variables condicionada a cada valor de una o más variables aleatorias. Definiremos el concepto de función de probabilidad y función de densidad condicional, que nos serán útiles en el futuro.\\

\begin{definition}[Función de probabilidad condicional] \label{def:funcprobcond}
Sean $X,Y$ variables aleatorias discretas con función de probabilidad conjunta $f_{X,Y}$ y sea $f_{Y}(y)$ la función de probabilidad marginal de $Y$. Llamaremos \textit{función de probabilidad condicional} de $X$ dado $Y=y$, a la función definida como: $$f_{X\mid Y}(x \mid y)=\ddfrac{f_{X,Y}(x,y)}{f_Y(y)}, \ \ \text{con} \ f_{Y}(y) \neq 0.$$

\begin{remark}
La \textit{función de densidad condicional} se define de la misma forma para $X,Y$ variables aleatorias continuas, tomando $f_{X,Y}$ como la función de densidad conjunta y $f_{Y}$ como la función de densidad marginal.
\end{remark}
\end{definition}\

\begin{comment}
\begin{definition}[Función de distribución condicional]
Sean $X,Y$ variables aleatorias, la \textit{función de distribución condicional} de $X$ dado $Y=y$, viene dada como: 
$$F_{X\mid Y}(x\mid y)=\sum_{u\leq x} f_{X\mid Y}(u \mid y),$$
donde $f_{X\mid Y}$ es la función de probabilidad condicional, o
$$F_{X\mid Y}(x\mid y)=\int_{-\infty}^x f_{X\mid Y}(u\mid y) \ du,$$
si $f_{X\mid Y}$ es la función de densidad condicional.
\end{definition}
\end{comment}

\section{Independencia}

Dos variables aleatorias son \textit{independientes} entre sí cuando la probabilidad de cada variable no está influida por la ocurrencia de la otra. A continuación formalizaremos esta idea.\\

\begin{comment}
\begin{definition}[Sucesos independientes]
Se dice que los \textit{sucesos} $A,B \in \mathcal{A}$ son \textit{independientes} si cumplen: $$P(A, B)=P(A) P(B).$$
\begin{remark}
Aplicando la expresión anterior a la Definición \ref{def:probcond} tenemos que, si los sucesos son independientes, $P(A \mid B)=P(A)$.
\end{remark}
\end{definition}\


\begin{definition}[Sucesos mutuamente independientes]
Sea un conjunto finito de sucesos $\{A_i\}_{i=1}^n \in \mathcal{A}$, diremos que son \textit{mutuamente independientes} si, y solo si, para $\{B_i\}\subset \{A_i\}$, entonces: $$P\left(\bigcap_{i=1}^k B_i \right)=\prod_{i=1}^k P(B_i), \ \text{para todo} \ k \leq n.$$
\end{definition}\
\end{comment}

\begin{definition}[Variables aleatorias independientes]
Sean $X,Y$ variables aleatorias, diremos que son \textit{independientes} si, y solo si, $$P(X=x,Y=y)=P(X=x) P(Y=y).$$ Cuando esto ocurra, lo denotaremos como $X \perp Y$.
\end{definition}\

\begin{definition}[Variables aleatorias independientes condicionalmente]
Sean $X,Y,Z$ variables aleatorias, entonces $X$ e $Y$ son \textit{condicionalmente independientes} dado $Z$ si, y solo si, $$P(X=x,Y=y \mid Z=z)=P(X=x \mid Z=z) P(Y=y \mid Z=z),$$
o equivalentemente, $$P(X=x \mid Y=y, Z=z)=P(X=x \mid Z=z).$$
En este caso, lo denotaremos como $X \perp Y \mid Z$.
\end{definition}\

\begin{comment}
La Definición \ref{def:probcond} también se cumple para variables aleatorias, por lo que podemos utilizando la definición anterior obtenemos la siguiente equivalencia:
\begin{equation*}
    P(X=x,Y=y\mid Z=z)=\ddfrac{P(X=x,Z=z)}{P(Z=z)}\ddfrac{P(Y=y,Z=z)}{P(Z=z)}=\ddfrac{P(X=x,Y=y,Z=z)}{P(Z=z)}.
\end{equation*}
\end{comment}

\begin{definition}[Variables aleatorias independientes e idénticamente distribuidas]
Sea un vector aleatorio $\nm{X}=(X_1,\dots,X_n)$ diremos que sus componentes son \textit{independientes e idénticamente distribuidas} (i.i.d) si, y solo si, son \textit{independientes}: $$F_{\nm{X}}(x)=F_{X_1}(x_1)\cdots F_{X_n}(x_n), \ \text{para todo} \ x_1,\dots,x_n \in \R$$
y son \textit{idénticamente distribuidas}: $$F_{X_1}(x)=F_{X_i}(x), \ \text{para todo} \ i \in \{2,\dots,n\}, \ \text{para todo} \ x \in \R.$$
\end{definition}


\chapter{Distribuciones de probabilidad}

En este capítulo recordaremos algunos ejemplos de distribuciones de probabilidad que usaremos durante el desarrollo del trabajo y definiremos algunos conceptos matemáticos estadísticos previos a la construcción de las distribuciones. Utilizaremos como referencias bibliográficas a \cite{random2004} y \cite{probability2014}.

\section{Esperanza de una variable aleatoria}

Estamos listos para introducir el concepto de \textit{esperanza matemática} de una variable aleatoria $X$. La esperanza representa el valor promedio de los valores que toma la variable.\\


\begin{definition}[Esperanza de una variable aleatoria]
Sea $X$ una variable aleatoria unidimensional no negativa en un espacio de probabilidad $(\Omega, \mathcal{A},P)$. Definimos su \textit{esperanza} como: $$\mathbb{E}[X]=\int_{\Omega} X(\omega) \ dP(\omega).$$

Se denota por $\mu_X$ o $\mu$ dependiendo de si queremos destacar o no cual es la variable aleatoria a la que se refiere.\\

Si $X$ es una variable aleatoria discreta y toma valores en el conjunto $\mathcal{X}$, entonces su esperanza se define como: $$\mathbb{E}[X]=\sum_{x\in \mathcal{X}} x P_X(x).$$ donde $x$ es cada posible resultado del experimento y $P_X(x)$ la probabilidad inducida por $X$ de obtener el resultado $x$.\\

Si $X$ es continua y $f(x)$ es su función de densidad, entonces su esperanza se define como: $$\mathbb{E}[X]=\int_{-\infty}^{+\infty}x f(x) \ dx.$$
\end{definition}\

\begin{proposition}
Dadas $X,Y$ variables aleatorias y $a,b\in \R$. $\mathbb{E}[X]$ es un operador lineal, es decir: $$\mathbb{E}[aX+bY]=a\mathbb{E}[X]+b\mathbb{E}[Y].$$ 
\end{proposition}

\begin{proof}
Consecuencia de la linealidad de la integral de Lebesgue.
\end{proof}\

\begin{proposition}
Sean $X$ una variable aleatoria con función de densidad $f_X$ y $g\colon \R \to \R$ una función integrable de Lebesgue, entonces se cumplen las siguientes propiedades:
\begin{itemize}
    \item $g(X)$ es una variable aleatoria y su esperanza viene dada por: $$\mathbb{E}[g(X)]=\int_\Omega g(x)f_X(x) \ dx.$$ 
    \begin{itemize}
        \item Si $X$ es discreta, $\displaystyle \mathbb{E}[g(X)]=\sum_{x\in \mathcal{X}} g(x)P_X(x).$
        \item Si $X$ es continua, $\displaystyle \mathbb{E}[g(X)]=\int_{-\infty}^{+\infty}g(x) f(x) \ dx.$
    \end{itemize}
    \item Sean $X_1,\dots,X_n$ variables aleatorias independientes, entonces $$\mathbb{E}\left[\prod_{i=1}^n X_i \right]=\prod_{i=1}^n \mathbb{E}[X_i].$$
\end{itemize}
\end{proposition}\

\begin{comment}
Presentaremos una relación entre la esperanza de una variable aleatoria para cualquier función convexa $g \in \mathcal{C}^2(\R)$ cumpliendo $g''(x)\geq 0 \ \text{para todo} \ x \in \R$. \\

\begin{proposition}[Desigualdad de Jensen]
Sea $X$ una variable aleatoria y $g\colon \R \to \R$ una función convexa, entonces: $$g(\mathbb{E}[X]) \leq \mathbb{E}[g(X)].$$
\end{proposition}\
\end{comment}

A continuación, definimos la \textit{esperanza condicional} de una variable aleatoria como el valor esperado de dicha variable respecto a una distribución de probabilidad condicional.\\

\begin{definition}[Esperanza condicional]
Sean $X,Y$ variables aleatorias discretas y $f_{X | Y}$ su función de probabilidad condicional, definimos la \textit{esperanza condicional} de $X$ dado $Y=y$ como: $$\mathbb{E}[X \mid Y=y]=\sum_{x\in \mathcal{X}} x f_{X | Y}(x \mid y),$$
en el caso continuo, sea $f_{X|Y}$ la función de densidad condicional, la esperanza condicional se calcula como: $$\mathbb{E}[X \mid Y=y]=\int_{-\infty}^{+\infty} x f_{X | Y}(x \mid y) \ dy.$$
\end{definition}

\clearpage

\section{Momentos de una variable aleatoria}

A partir de la definición de esperanza de una variable aleatoria, podemos construir el concepto de \textit{momentos} de una variable aleatoria, que nos permiten extraer información relevante de la distribución desconocida.\\

\begin{definition}[Momento no centrado de una variable aleatoria]
Sea $X$ una variable aleatoria y $k\in \N$. Definimos el \textit{momento no centrado de orden $k$} como: $$\mu_k=\mathbb{E}[X^k],$$
siempre que exista dicha esperanza.\\
\begin{remark}
En el caso $k=1$, obtenemos la esperanza matemática de la variable aleatoria $X$, a la cual también denominamos \textit{media} $\mu_X$ o simplemente $\mu$.
\end{remark}
\end{definition}\

\begin{definition}[Momento centrado de una variable aleatoria]
Sea $X$ una variable aleatoria, $k\in \N$ y $c\in \R$ definimos el \textit{momento centrado en $c$ de orden $k$} como: $$\mu_k=\mathbb{E}[(X-c)^k].$$
\end{definition}\

\begin{definition}[Varianza]
La \textit{varianza} de una variable aleatoria se define como: $$\text{Var}(X)=\sigma
^2=\mathbb{E}\left[(X-\mathbb{E}[X])^2\right].$$
A la raíz cuadrada positiva de la varianza la denotaremos como \textit{desviación estándar} $\sigma_X$ o simplemente $\sigma$.
\end{definition}\

\begin{proposition}
Sean $X$ una variable aleatoria y $a,b\in \R$, entonces se cumplen las siguientes propiedades:
\begin{itemize}
    \item $\text{Var}(X)=\mathbb{E}[X]^2-\mathbb{E}[X^2].$
    \item $\text{Var}(b)=0.$
    \item $\text{Var}(aX)=a^2\text{Var}(X).$
    \item Sea $X,Y$ variables aleatorias independientes, entonces: $$\text{Var}(X+Y)=\text{Var}(X)+\text{Var}(Y).$$
\end{itemize}
\end{proposition}\

\begin{proposition}
Dadas $X,Y$ variables aleatorias y $a,b\in \R$. $\text{Var}(X)$ es una operación lineal, es decir: $$\text{Var}(aX+b)=\mathbb{E}[(aX+b)-\mathbb{E}[aX+b]^2]=a^2\mathbb{E}[(X-\mu_X)^2]=a^2\text{Var}(X).$$ 
\end{proposition}

\begin{proof}
Consecuencia de la linealidad de la esperanza.
\end{proof}\

En el mundo real, cuando aplicamos estos conceptos lo haremos sobre múltiples características observables, es decir, sobre vectores aleatorios como escribiremos a continuación.\\

\begin{definition}[Esperanza de un vector aleatorio]
Sea $\nm{X}=(X_1,\dots,X_n)^T$ un vector aleatorio. Se define la \textit{esperanza} de $\bm{\mu_{\nm{X}}}$ como: $$\bm{\mu_{\nm{X}}}=\mathbb{E}[\nm{X}]=\begin{bmatrix}
\mathbb{E}[X_1]\\
\vdots\\
\mathbb{E}[X_n]
\end{bmatrix},$$
siempre que existan las esperanzas unidimensionales.
\end{definition}\

Para generalizar la varianza de una variable aleatoria, construiremos una matriz multidimensional con la que aparece el concepto de covarianza.\\

\begin{definition}[Matriz de covarianzas]
Sea $\nm{X}=(X_1,\dots,X_n)^T$ un vector aleatorio. Se define, la \textit{matriz de covarianzas} de $\nm{X}$ como: $$\bm{\Sigma_{\nm{X}}} = \text{Cov}(\nm{X})=\mathbb{E}[(\nm{X}-\bm{\mu_{\nm{X}}})(\nm{X}-\bm{\mu_{\nm{X}}})^T]=\begin{bmatrix}
\sigma_{11} & \dots & \sigma_{1n}\\
\vdots & \ddots & \vdots\\
\sigma_{n1} & \dots & \sigma_{nn}
\end{bmatrix},$$
donde $\sigma_{ij}=\text{Cov}(X_i,X_j)=\mathbb{E}[(X_i-\mu_i)(X_j-\mu_j)]=\sigma_{ji}$ es la covarianza de las variables aleatorias $X_i,X_j$. Podremos definirla cuando existan todas las covarianzas.
\end{definition}

\section{Ejemplos de distribuciones}

La \textit{distribución de probabilidad} de una variable aleatoria es una función que hace corresponder a cada suceso definido sobre la variable, la probabilidad de que dicho suceso ocurra. Describiremos algunas de estas funciones que nos serán de interés a lo largo del desarrollo del trabajo.\\

\begin{definition}[Moda]
La \textit{moda} de una distribución es el valor donde su función de probabilidad alcanza su máximo. Es el valor que aparece con mayor frecuencia en un conjunto de datos.
\end{definition}

\clearpage

Las distribuciones pueden ser \textit{unimodales, bimodales} o \textit{multimodales} dependiendo de si tienen un solo valor de moda, dos o más, respectivamente.\\

\begin{figure}[h]
    \minipage{0.5\textwidth}
      \includegraphics[width=\linewidth]{images/unimodal.png}
    \endminipage\hfill
    \minipage{0.5\textwidth}%
      \includegraphics[width=\linewidth]{images/bimodal.png}
    \endminipage
     \caption{Ejemplos de distribuciones de probabilidad.}
\end{figure}

\subsection{Distribución Bernoulli}

La \textit{distribución Bernoulli} es una distribución de probabilidad aplicada a una variable aleatoria discreta, la cual solo puede tomar dos resultados mutuamente excluyentes (éxito o fracaso).\\

\begin{definition}[Distribución Bernoulli]
Una variable aleatoria unidimensional $X$ sigue una \textit{distribución Bernoulli} de parámetro $p$ si su función de probabilidad viene dada por: $$P(X=x)=\left \{
\begin{array}{l l}
p, & \mbox{si } x = 1, \\
1-p, & \mbox{si } x =0.
\end{array}
\right.$$
\end{definition}\

Escribiremos la distribución como $X \sim Bernoulli(p)$, donde el parámetro $p\in (0,1)$ indica la probabilidad de éxito y $(1-p)$ la probabilidad de fracaso del experimento.\\

\begin{proposition}[Propiedades]
Si $X \sim Bernoulli(p)$, entonces la variable aleatoria $X$ satisface las siguientes propiedades:
\begin{itemize}
    \item $\mathbb{E}[X]=p$.
    \item $\text{Var}(X)=p(1-p)$.
\end{itemize}
\end{proposition}

\clearpage

\subsection{Distribución de Poisson}

La \textit{distribución de Poisson} es una distribución de probabilidad discreta que modela el número de sucesos raros que ocurren en un determinado periodo de tiempo.\\

\begin{definition}[Distribución de Poisson]
Una variable aleatoria unidimensional $X$ sigue una \textit{distribución de Poisson} de parámetro $\lambda$ si su función de probabilidad viene dada por: $$P(X=x)=\ddfrac{\lambda^xe^{-\lambda}}{x!},$$
donde $x=0,1,\dots$ es el número de ocurrencias del evento o fenómeno.
\end{definition}\

Escribiremos la distribución como $X \sim Poisson(\lambda)$, donde el parámetro $\lambda > 0$ representa el número de veces que se espera que ocurra el fenómeno durante un intervalo dado.\\


\begin{proposition}
Si $X \sim Poisson(\lambda)$, entonces la variable aleatoria $X$ satisface las siguientes propiedades:
\begin{itemize}
    \item[1.] $\mathbb{E}[X]=\lambda$.
    \item[2.] $\text{Var}(X)=\lambda$.
    \item[3.] La moda de $X$ es $\lfloor \lambda \rfloor$ (el mayor entero menor que $\lambda$).
\end{itemize}
\end{proposition}

\begin{proof}
  Demostraremos la propiedad 1 utilizando la definición de $\mathbb{E}[X]$. \\
  \begin{equation*}
  \begin{split}
  \mathbb{E}[X]&=\sum_{x=0}^\infty x \left(\ddfrac{\lambda^x e^{-\lambda}}{x!}\right)\\
  &=\lambda e^{-\lambda} \sum_{x=1}^\infty \ddfrac{\lambda^{x-1}}{(x-1)!}\\
  &\overset{(*)}{=}\lambda e^{-\lambda} \sum_{y=0}^\infty \ddfrac{\lambda^{y}}{y!}\\
  &=\lambda e^{-\lambda} e^{\lambda}\\
  &=\lambda.
  \end{split}
  \end{equation*} donde en $(*)$ hemos cambiado $(x-1)$ por $y$.\\
  
  Para la propiedad 2, sabiendo que
  \begin{equation}\label{eq:varxprop}
    \text{Var}(X)=\mathbb{E}[X^2]-\mathbb{E}[X]^2=\mathbb{E}[X(X-1)]+\mathbb{E}[X]-\mathbb{E}[X]^2.
  \end{equation}
  
  Procederemos con el cálculo de $\mathbb{E}[X(X-1)]$. \begin{equation*}
  \begin{split}
 \mathbb{E}[X(X-1)]&=\sum_{x=0}^\infty x(x-1)\left(\ddfrac{\lambda^x e^{-\lambda}}{x!}\right)\\
 &=\lambda^2 e^{-\lambda} \sum_{x=2}^\infty \ddfrac{\lambda^{x-2}}{(x-2)!}\\ &\overset{(*)}{=}\lambda e^{-\lambda} \sum_{y=0}^\infty \ddfrac{\lambda^{y}}{y!}\\
 &=\lambda^2 e^{-\lambda} e^{\lambda}\\
 &=\lambda^2.
  \end{split}
  \end{equation*} donde en $(*)$ hemos cambiado $(x-2)$ por $y$.\\
  
  Sustituyendo en la Ecuación (\ref{eq:varxprop}) concluimos que, $\text{Var}(X)=\lambda^2+\lambda-\lambda^2=\lambda.$
\end{proof}\

\begin{proposition}
Si $X \sim Poisson(\lambda)$, donde $X$ es una variable aleatoria que representa el número de sucesos raros en una unidad de tiempo e $Y$ es una variable aleatoria que representa el número de dichos sucesos raros en un tiempo $t$, se tiene que $$Y \sim Poisson(t\lambda).$$
\end{proposition}\

\begin{example} \label{ex:ejpoisson}
En un instituto, el número medio de suspensos por clase es de $2.4$. Es decir, si $X$ es el número de suspensos por clase, entonces $$X \sim Poisson(2.4).$$ 

¿Cuál es la probabilidad de que en una clase no haya suspensos? $$P(X=0)=\ddfrac{2.4^0 e^{-2.4}}{0!}=e^{-2.4}=0.09.$$

¿Cuál es la probabilidad de que en 3 clases haya exactamente 6 suspensos?

Sea $Y$ el número de suspensos en 3 clases. Sabemos que: $$Y \sim Poisson(2.4\cdot 3)=Poisson(7.2)$$ $$P(Y=6)=\ddfrac{7.2^6 e^{-7.2}}{6!}=e^{-3.4}=0.14.$$
\end{example}\

\begin{figure}[h]
    \minipage{0.5\textwidth}
      \includegraphics[width=\linewidth]{images/poisson1.png}
    \endminipage\hfill
    \minipage{0.5\textwidth}%
      \includegraphics[width=\linewidth]{images/poisson2.png}
    \endminipage
     \caption{Distribuciones calculadas en el Ejemplo \ref{ex:ejpoisson}.}
\end{figure}

\subsection{Distribución Normal}

La \textit{distribución normal} o  \textit{distribución de Gauss} se utiliza para representar variables aleatorias de valor real cuyas distribuciones son desconocidas.\\

\begin{definition}[Distribución normal]
Una variable aleatoria unidimensional $X$ sigue una \textit{distribución normal} o \textit{gaussiana} de parámetros $\mu, \sigma$, si su función de densidad $f\colon \R \to \R$ viene dada por: $$f(x)=\ddfrac{1}{\sigma \sqrt{2\pi}} \ e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2},$$
donde $\mu,\sigma\in \R$.
\end{definition}\

Donde escribiremos la distribución como $X \sim \mathcal{N}(\mu, \sigma^2)$, donde recordemos el parámetro $\mu$ se refiere a la media y $\sigma$ a la desviación estándar de la variable aleatoria.\\

\begin{proposition} \label{prop:propnormal}
Si $X \sim \mathcal{N}(\mu, \sigma^2)$, entonces la variable aleatoria $X$ satisface las siguientes propiedades :
\begin{itemize}
    \item La distribución es simétrica respecto a $\mu$.
    \item $P(\mu-\sigma < X < \mu+\sigma)\approx 0.683.$
    \item $P(\mu-2\sigma < X < \mu+2\sigma)\approx 0.955.$
    \item $P(\mu-3\sigma < X < \mu+3\sigma)\approx 0.997.$
    \item La moda de $X$ coincide con $\mu$.
\end{itemize}
\end{proposition}\

\begin{comment}
A continuación, mostraremos un resultado que nos da una cota, que depende de la varianza, para la probabilidad de que los valores que toma una variable aleatoria estén en un entorno de su media.\\

\begin{proposition}[Desigualdad de Chebychev]
Sea $X$ una variable aleatoria tal que $X \sim \mathcal{N}(\mu, \sigma^2)$ y dado $k>0$, la probabilidad de obtener un valor que diste de $\mu$ al menos $k\sigma$ es a lo sumo $\frac{1}{k^2}$. El resultado puede resumirse en la siguiente expresión: $$P(\mu-k\sigma< X < \mu + k\sigma)\geq 1-\frac{1}{k^2},$$
donde recordemos $\mu=\mathbb{E}[X], \ \sigma^2=\text{Var}(X).$
\end{proposition}\

Definiremos otras propiedades importantes de la distribución normal que no hemos comentado hasta ahora:\\

\begin{proposition}
Sean $X,Y$ variables aleatorias independientes tal que $X \sim \mathcal{N}(\mu_X, \sigma_X^2)$ y $Y \sim \mathcal{N}(\mu_Y, \sigma_Y^2)$. Entonces, $$X+Y\sim \mathcal{N}(\mu_X+\mu_Y,\sigma_X^2+\sigma_Y^2) \ \ \text{ y } \ \ X-Y\sim \mathcal{N}(\mu_X-\mu_Y,\sigma_X^2-\sigma_Y^2).$$
\end{proposition}\

\begin{proposition}
Sea $X$ una variable aleatoria tal que $X \sim \mathcal{N}(\mu, \sigma^2)$ y sean $a,b\in \R$, la distribución se mantiene mediante trasnformaciones lineales: $$Y=aX+b \sim \mathcal{N}(a\mu_X+b,a^2\sigma_X^2).$$
\end{proposition}\
\end{comment}

Como consecuencia de la primera propiedad de la Proposición \ref{prop:propnormal}, podemos relacionar todas las variables aleatorias normales con la distribución $\mathcal{N}(0,1)$.\\

\begin{proposition}[Estandarización de variables aleatorias normales]
Sea $X$ una variable aleatoria tal que $X \sim \mathcal{N}(\mu, \sigma^2)$, entonces: $$Z=\ddfrac{X-\mu}{\sigma} \sim \mathcal{N}(0,1),$$
donde $\mathcal{N}(0,1)$ es la distribución normal estándar.
\end{proposition}\

\begin{figure}[h]
	\centering
	\includegraphics[width=9.8cm]{distnormal.png}
	\caption{Ejemplo de una distribución normal.}
    \label{fig:distrnormal}
\end{figure}\

\begin{definition}[Media muestral]
Sean $X_1,\dots,X_n$ variables aleatorias obtenidas a partir de $X$ y que siguen su misma distribución, se define la \textit{media muestral} como:
$$\overline{X}_n=\frac{1}{n}\sum_{i=1}^n X_i.$$
\end{definition}\

La importancia de la distribución normal reside principalmente en el Teorema central del límite que trata sobre la distribución de la media muestral para variables aleatorias independientes e idénticamente distribuidas (i.i.d) y garantiza una distribución aproximadamente normal cuando $n$ es lo suficientemente grande.\\

\begin{theorem}[Teorema central del límite] \label{th:thcentral}
Sean $X_1,X_2,\dots,X_n$ variables aleatorias i.i.d con media $\mu$ y desviación estándar $\sigma^2$ (ambas finitas). Con $n$ suficientemente grande, se tiene que $$\frac{\overline{X}-\mu}{\sigma / \sqrt{n}}\sim \mathcal{N}(0,1).$$
\end{theorem}

\clearpage

Finalmente, podemos relacionar las dos distribuciones estudiadas aproximando la distribución de Poisson mediante la distribución normal estándar a partir de la siguiente relación:\\

\begin{proposition}
Sea $X$ una variable aleatoria tal que $X \sim Poisson(\lambda)$ con $\lambda$ suficientemente grande, entonces: $$\ddfrac{X-\lambda}{\sqrt{\lambda}}\sim \mathcal{N}(0,1).$$
\end{proposition}\

Como una extensión del caso unidimensional, aparece el caso de distribución normal multivariante que definiremos a continuación.\\

\begin{definition}[Distribución normal multivariante]
Sea un vector aleatorio $\nm{X}=(X_1,\dots,X_n)^T$ diremos que sigue una \textit{distribución normal multivariante} de parámetros $\bm{\mu},\bm{\Sigma}$ si su función de densidad $f\colon \Rn \to \R$ viene dada por: $$f(x)=\ddfrac{1}{\sqrt{\text{det}(2\pi \bm{\Sigma})}}e^{-\frac{1}{2}(x-\bm{\mu})^T\bm{\Sigma}^{-1}(x-\bm{\mu})},$$
donde $\bm{\mu}\in \Rn$ y $\bm{\Sigma}\in \mathcal{M}(\R)$.
\end{definition}\

Escribiremos la distribución como $\nm{X} \sim \mathcal{N}(\bm{\mu}, \bm{\Sigma})$, donde recordemos el parámetro $\bm{\mu}$ se refiere al vector de las medias de la distribución y $\bm{\Sigma}$ a la matriz de covarianzas.

\subsection{Distribución Uniforme}

\begin{definition}[Distribución uniforme discreta]
Una variable aleatoria discreta $X$ con posibles valores $\{x_1,\dots,x_n\}$ diremos que sigue una \textit{distribución uniforme} si: $$P(X=x_i)=\frac{1}{n}, \ \text{para todo} \ i=1,\dots,n.$$
\end{definition}\

\begin{definition}[Distribución uniforme continua]
Una variable aleatoria continua $X$ sigue una \textit{distribución uniforme} en el intervalo $(a,b)$ si su función de densidad viene dada por: $$f(x)=\left \{
\begin{array}{l l}
\frac{1}{b-a}, & \mbox{si } x \in (a,b), \\
0, & \mbox{si } x \notin (a,b).
\end{array}
\right.$$
\end{definition}\

Escribiremos la distribución como $X \sim U(a,b)$, donde la variable aleatoria queda definida por los extremos del intervalo, es decir, $a$ y $b$ son sus parámetros.\\

\begin{proposition}[Propiedades]
Si $X \sim U(a,b)$, entonces la variable aleatoria $X$ satisface las siguientes propiedades:
\begin{itemize}
    \item $\mathbb{E}[X]=\frac{a+b}{2}$.
    \item $\text{Var}(X)=\frac{(b-a)^2}{12}$.
    \item La moda de $X$ es cualquier valor en $(a,b)$.
\end{itemize}
\end{proposition}


\subsection{Distribución Gamma}

\begin{definition}[Distribución gamma]
Una variable aleatoria continua $X$ sigue una \textit{distribución gamma} de parámetros $\alpha$ y $\lambda$ si su función de densidad viene dada por: $$f(x)=\ddfrac{\lambda^{\alpha}x^{\alpha-1}e^{-\lambda x}}{\Gamma(\alpha)}.$$
donde $\Gamma$ es la \textit{función gamma} definida como $\Gamma(\alpha)=\displaystyle \int_0^{+\infty}x^{\alpha-1}e^{-x} \ dx$.
\end{definition}\

Escribiremos la distribución como $X \sim \Gamma(\alpha,\lambda)$, donde la variable aleatoria queda definida por los parámetros $\alpha,\lambda > 0$.\\

\begin{proposition}[Propiedades]
Si $X \sim \Gamma(\alpha,\lambda)$, entonces la variable aleatoria $X$ satisface las siguientes propiedades:
\begin{itemize}
    \item $\mathbb{E}[X]=\frac{\alpha}{\lambda}$.
    \item $\text{Var}(X)=\frac{\alpha}{\lambda^2}$.
\end{itemize}
\end{proposition}\

\begin{definition}[Distribución gamma inversa]
Una variable aleatoria continua $X$ sigue una \textit{distribución gamma inversa} de parámetros $\alpha$ y $\lambda$ si su función de densidad viene dada por: $$f(x)=\ddfrac{\lambda^\alpha x^{-(\alpha+1)} e^{\frac{-\lambda}{x}}}{\Gamma(\alpha)}.$$
\end{definition}

\chapter{Estadística paramétrica}

La \textit{estadística paramétrica} es una rama de la inferencia estadística que comprende los procedimientos estadísticos y de decisión basados en distribuciones conocidas que vienen determinadas por un número finito de parámetros. En este capítulo definiremos algunos conceptos de este campo de la estadística que utilizaremos a lo largo del trabajo y que es tratado en mayor profundidad en \cite{inferencia2012}.\\

\begin{definition}[Muestra aleatoria simple]
Sea una variable aleatoria $X$ que sigue una distribución de probabilidad determinada. Definimos una \textit{muestra aleatoria simple} de tamaño $n$ como un conjunto de variables aleatorias $(X_1,\dots,X_n)$, independientes e idénticamente distribuidas, obtenidas a partir de la distribución de $X$.
\end{definition}\

\begin{definition}[Estadístico muestral]
Un \textit{estadístico de una muestra} aleatoria simple $(X_1,\dots,X_n)$, es una función medible $T\colon \R^n \to \R$, que se aplica a la muestra, lo denotaremos como $T(X_1,\dots,X_n)$.\\
\begin{remark}
Un \textit{estadístico muestral} también es una variable aleatoria.
\end{remark}
\end{definition}\

\begin{example}
Algunos estadísticos muestrales más comunes son:
\begin{itemize}
    \item \textbf{Media muestral} ($\overline{X}$), $$ T(X_1,\dots,X_n)=\frac{1}{n}\sum_{i=1}^n X_i.$$
    \item \textbf{Varianza muestral} ($\mathcal{S}^2$),  $$ T(X_1,\dots,X_n)=\frac{1}{n}\sum_{i=1}^n (X_i-\overline{X})^2.$$
    \item \textbf{Menor valor muestral}, $$ T(X_1,\dots,X_n)=\min (X_1,\dots,X_n).$$
    \item \textbf{Mayor valor muestral},  $$ T(X_1,\dots,X_n)=\max (X_1,\dots,X_n).$$
\end{itemize}
\end{example}\

Particularizando el uso del término estadístico muestral, aparece el concepto de estimador.\\

\begin{definition}[Estimador]
Un \textit{estimador} es un estadístico cuyos valores se utilizan para obtener información de un parámetro desconocido $\btheta$, lo denotaremos como $\htheta \mst$.
\end{definition}

\begin{comment}
\section{Función de verosimilitud}

Un modelo paramétrico $P_\btheta(x)$, es una familia de funciones de densidad $\{f_{\btheta}(x):\btheta \in \Theta\}$ que puede describirse usando un número finito de parámetros $\btheta$. Ahora estamos en condiciones de definir el concepto de verosimilitud.\\

\begin{definition}[Función de verosimilitud]
La \textit{función de verosimilitud} $\mathcal{L}(\btheta \mid x)$ de un conjunto de parámetros $\btheta$ es una función que mide el grado de verosimilitud de $\btheta$, dado un valor observable $x$ del conjunto de datos $\mathcal{D}$. Se define como el valor de la función de densidad parametrizada por $\btheta$ en $x$. Esto es: $$\mathcal{L}(\btheta \mid x)=P_{\btheta}(x),$$
En un conjunto de datos $\mathcal{D}$ con observaciones independientes, podemos escribir: $$\mathcal{L} (\btheta \mid X)=\prod_{x\in \mathcal{D}} P_{\btheta}(x).$$
\end{definition}\

Como esta función puede ser difícil de calcular computacionalmente hablando, normalmente se suele utilizar el logaritmo de esta función.\\

\begin{definition}[Logaritmo de la función de verosimilitud]
Sea $\mathcal{D}$ un conjunto de datos de observaciones independientes y $\btheta$ un conjunto de parámetros. Definimos el logaritmo de la función de verosimilitud, como: $$\ell(\btheta \mid X)=\sum_{x\in \mathcal{D}} \log P_{\btheta}(x). $$
\end{definition}\

Nuestro objetivo será el valor óptimo de $\htheta$ que maximiza la verosimilitud del conjunto de datos $\mathcal{D}$.\\

\begin{definition}[Estimador de máxima verosimilitud]
Diremos que $\htheta=\htheta(\mathcal{D})$ es un estimador de máxima verosimilitud para $\btheta$ si: $$\htheta \in \underset{\btheta}{\mathrm{argmax}} \ \mathcal{L}(\btheta \mid \mathcal{D}),$$
para cada variable observable de $\mathcal{D}.$
\end{definition}

\end{comment}

\section{Propiedades del estimador}

A continuación definiremos algunas propiedades que nos permitirán comparar diferentes estimadores de un mismo parámetro y nos informarán de la calidad de su estimación.\\

\begin{definition}[Estimador insesgado]
Si $\htheta$ es un estimador del parámetro $\btheta$, la diferencia $$\text{sesgo}(\htheta)=\mathbb{E}[\htheta]-\btheta,$$
se denomina \textit{sesgo} del estimador $\htheta$ como estimador de $\btheta$. Cuando el sesgo es nulo para cualquier valor del parámetro, es decir, si $$\mathbb{E}[\htheta]=\btheta, \ \text{para todo} \ \btheta \in \Theta,$$
diremos que el estimador $\htheta$ es \textit{insesgado} para $\btheta$.
\end{definition}\

\begin{example} \label{ex:sesgonul}
Sea $X_1,\dots,X_n$ una muestra aleatoria de una variable aleatoria $X$. Si el parámetro de interés es la media $\mu=\mathbb{E}[X]$, podemos utilizar la media muestral para estimarlo. Calculamos su sesgo: $$\text{sesgo}(\overline{X})=\mathbb{E}[\overline{X}]-\mu=\frac{1}{n}\sum_{i=1}^n \mathbb{E}[X_i]-\mu=\mu-\mu=0,$$
demostrando que $\overline{X}$ es un estimador insesgado para $\mu$.
\end{example}\

\begin{definition}[Estadístico suficiente]
Sea $T=T\mst$ un estadístico muestral se dice \textit{suficiente} para el parámetro $\btheta$ si la distribución condicional de la muestra $\mst$ dado el valor de $T\mst$ no depende del parámetro a estimar $\btheta$.
\end{definition}\


\begin{definition}[Estimador suficiente]
Un estimador $\htheta$ es \textit{suficiente} si es un estadístico suficiente.
\end{definition}\

A partir de la propiedad de suficiencia aparece el Teorema de factorización de Fisher-Neyman que nos ofrece una caracterización de estadístico suficiente.\\


\begin{theorem}[Teorema de factorización de Fisher-Neyman]
Sea $T=T\mst$ un estadístico muestral si la función de densidad es $f_{\btheta}(x)$, entonces $T$ es suficiente para $\btheta$ si, y solo si, podemos encontrar $g,h\colon \R^n \to \R$ funciones no negativas tal que $$f_{\btheta}(x)=h(x)g_{\btheta}(T(x)).$$
es decir, podemos factorizar la función de densidad como un producto de dos funciones: $h$ que no depende de $\btheta$ y $g$ que depende de $\btheta$ y de $x$ solo a través de $T(x)$.
\end{theorem}\

El tamaño de la muestra es un factor determinante en la estimación, a raíz de esto, surge el término de estimador \textit{consistente}. En la definición de consistencia se usan nociones de \textit{convergencia en probabilidad}, que comentaremos a continuación.\\

\begin{definition}[Convergencia en probabilidad]
Una sucesión de variables aleatorias $\{X_n\}$ \textit{converge en probabilidad} a la variable aleatoria $X$ si para todo $\epsilon>0$ entonces, $$\lim_{n\to +\infty}P([X_n-X]>\epsilon)=0.$$
Lo denotaremos como $X_n \overset{P}{\to} X.$
\end{definition}\

\begin{definition}[Estimador consistente]
Sea $\htheta_n$ un estimador del parámetro $\btheta$ para una muestra de tamaño $n$. Diremos que el estimador $\htheta_n$ es \textit{consistente} si, cuando $n\to +\infty$, se verifica que $$\htheta_n \overset{P}{\to} \btheta.$$
\end{definition}\

\begin{definition}[Estimador asintóticamente insesgado]
Sea $\htheta_n$ un estimador del parámetro $\btheta$ para una muestra de tamaño $n$. Diremos que el estimador $\htheta_n$ es \textit{asintóticamente insesgado} si, cuando $n\to +\infty$, se verifica que $$\text{sesgo}(\htheta_n) \to 0.$$
\end{definition}\

\begin{corollary} \label{cor:lemaestima}
Sea $\htheta_n$ un estimador del parámetro $\btheta$ para una muestra de tamaño $n$. Si $\htheta_n$ es asintóticamente insesgado y además si $n\to +\infty$ se cumple que $$\text{Var}(\htheta_n)\to 0.$$
entonces, el estimador $\htheta_n$ es consistente.
\end{corollary}\

\begin{comment}
\begin{example}
Sea $X$ una variable aleatoria con media $\mu$ y varianza $\sigma^2$. Estudiaremos el comportamiento asintótico de la varianza de $\overline{X}$. $$\text{Var}(\overline{X})=\text{Var}\left(\frac{1}{n} \sum_{i=1}^n X_i\right)=\frac{1}{n^2} \sum_{i=1}^n \text{Var}(X_i)=\frac{\sigma^2}{n}$$
cuando $n\to +\infty$, tenemos que $\text{Var}(\overline{X})\to 0$.
Como en el Ejemplo \ref{ex:sesgonul} hemos visto que para $\overline{X}$ el sesgo es nulo para cualquier valor de $n$. Aplicando el Corolario \ref{cor:lemaestima}, concluimos que $\overline{X}$ es un estimador consistente.
\end{example}

Para completar el estudio sobre el comportamiento de la media muestral, recordaremos la ley débil de los grandes números que prueba la convergencia en probabilidad de la media muestral a la media teórica. A partir de este teorema, podemos concluir que $\overline{X}$ es un estimador insesgado, consistente y converge en probabilidad a la media teórica de una variable aleatoria $X$.\\

\begin{theorem}[Ley débil de los grandes números]\label{th:leygrande}
Sean $X_1,\dots,X_n$ variables aleatorias i.i.d. con un valor de media finito, entonces la media muestral converge en probabilidad a $\mu$, es decir, $$\overline{X}_n \overset{P}{\to} \mu.$$
\end{theorem}\
\end{comment}

Finalmente enunciaremos la propiedad de \textit{eficiencia} de un estimador.\\

\begin{definition}[Estimador eficiente] \label{def:efvar}
Sean $\htheta_1$ y $\htheta_2$ dos estimadores del parámetro $\btheta$. Se dice que $\htheta_1$ es más \textit{eficiente} que $\htheta_2$ si verifica que $$\text{Var}(\htheta_1)<\text{Var}(\htheta_2).$$
\end{definition}\

\begin{comment}
\begin{definition}[Eficiencia relativa]
Sean $\htheta_1$ y $\htheta_2$ dos estimadores del parámetro $\btheta$. Se define la \textit{eficiencia relativa} de $\htheta_1$ respecto a $\htheta_2$ si como: $$\text{ER}(\htheta_1,\htheta_2)=\ddfrac{\text{Var}(\htheta_2)}{\text{Var}(\htheta_1)}.$$
\end{definition}\
\end{comment}

\begin{example} \label{ex:eficienciaex}
Sea $X \sim U(0,b)$.
Como $X$ sigue una distribución uniforme, entonces: $$\mathbb{E}[X]=\frac{b}{2} \quad \text{ y } \quad \text{Var}(X)=\frac{b^2}{12}.$$
Sean dos estimadores insesgados:
$$\bm{\hat{b}_1}(X_1,\dots,X_{5})=\overline{X}.$$
$$\bm{\hat{b}_2}(X_1,\dots,X_{5})=\max \{X_1,\dots,X_5\}.$$\\
A continuación veremos cual de los dos estimadores es más eficiente, procederemos calculando la varianza de cada uno de ellos:\\

\begin{itemize}
    \item Cálculo de la varianza de $\bm{\hat{b}_1}$: $$\text{Var}(\bm{\hat{b}_1})=\text{Var}(\overline{X})=\frac{\text{Var}(X)}{n}=\frac{b^2/12}{5}=\frac{b^2}{60}.$$
    \item Cálculo de la varianza de $\bm{\hat{b}_2}$: $$\mathbb{E}\left[\bm{\hat{b}_2}^2\right]=\int_{0}^b x^2 f_{\bm{\hat{b}_2}^2}(x) \ dx=\int_{0}^b x^2 \ \frac{5x^4}{b^5} \ dx=\frac{5}{b^5}\int_{0}^b x^6 \ dx=\frac{5}{b^5}\left[\frac{x^7}{7}\right]_{0}^b=\frac{5}{7}b^2.$$
    Como $\bm{\hat{b}_2}$ es un estimador insesgado, entonces $\mathbb{E}[\bm{\hat{b}_2}]=b$.
    $$\text{Var}\left(\bm{\hat{b}_2}\right)=\mathbb{E}\left[\bm{\hat{b}_2}^2\right]-\mathbb{E}\left[\bm{\hat{b}_2}\right]^2=\frac{5}{7}b^2-b^2=-\frac{2}{7}b^2.$$
\end{itemize}\

Concluimos que $\bm{\hat{b}_2}$ es más eficiente que $\bm{\hat{b}_1}$ ya que: $$\text{Var}(\bm{\hat{b}_2})= -\frac{2b^2}{7} < \frac{b^2}{60}=\text{Var}(\bm{\hat{b}_1}).$$
\end{example}

\clearpage

\section{Criterios de evaluación}

\begin{definition}[Error cuadrático medio]
El \textit{error cuadrático medio} (ECM) de un estimador $\htheta$ con respecto al parámetro desconocido $\btheta$  se define como: $$\text{ECM}(\htheta)=\mathbb{E}[(\htheta-\btheta)^2].$$
\end{definition}\

\begin{proposition}
Sea $\htheta$ un estimador de $\btheta$. Se cumple que:
$$\text{ECM}(\htheta)=\text{Var}(\htheta)+\text{sesgo}(\htheta)^2.$$
\end{proposition}

\begin{proof}
 $$\text{ECM}(\htheta)=\mathbb{E}[(\htheta-\btheta)^2]=\mathbb{E}[\htheta^2+\btheta^2-2\htheta \btheta]=\mathbb{E}[\htheta^2]+\btheta^2-2\btheta\mathbb{E}[\htheta].$$\\
Sumando y restando $\mathbb{E}[\htheta]^2$ en la expresión anterior, obtenemos:\\
  \begin{equation*}
  \begin{split}
 \text{ECM}(\htheta)&=\mathbb{E}[\htheta^2]+\btheta^2-2\btheta\mathbb{E}[\htheta]\textcolor{blue}{+\mathbb{E}[\htheta]^2-\mathbb{E}[\htheta]^2}\\
 &=(\mathbb{E}[\htheta^2]-\mathbb{E}[\htheta]^2)+(\btheta^2+\mathbb{E}[\htheta]^2-2\btheta\mathbb{E}[\htheta])\\
 &=(\mathbb{E}[\htheta^2]-\mathbb{E}[\htheta]^2)+(\btheta+\mathbb{E}[\htheta])^2\\
 &=\text{Var}(\htheta)+\text{sesgo}(\htheta)^2.
  \end{split}
  \end{equation*} 
\end{proof}

El ECM involucra las propiedades de insesgadez y
eficiencia, ya que cuanto más cerca esté la esperanza de un estimador del parámetro y cuanto más pequeña sea su varianza, menor será su error cuadrático medio.\\

\begin{proposition} \label{prop:varecm}
Si $\htheta$ es un estimador insesgado, entonces: $$\text{ECM}(\htheta)=\text{Var}(\htheta).$$ 
\end{proposition}\

A partir del concepto de ECM se define el concepto de \textit{raíz del error cuadrático medio} que es una medida de uso frecuente de las diferencias entre los valores predichos por un estimador y los valores observados y que utilizaremos en la práctica para evaluar modelos de predicción.\\

\begin{definition}[Raíz del error cuadrático medio]
La \textit{raíz del error cuadrático medio} (RMSE) de un estimador $\htheta$ con respecto al parámetro desconocido $\btheta$ se define como: $$\text{RSME}(\htheta)=\sqrt{\text{ECM}(\htheta)}=\sqrt{\mathbb{E}[(\htheta-\btheta)^2]}.$$
\end{definition}\

\begin{remark}
Si $\htheta$ es insesgado, por la Proposición \ref{prop:varecm}: $$\text{RMSE}(\htheta)=\sqrt{\text{Var}(\htheta)}=\sigma.$$
\end{remark}

Estas medidas de evaluación pueden particularizarse para modelos de predicción a partir del siguiente resultado:\\

\begin{proposition}
Sea $\hat{\nm{y}}$ un vector de $n$ predicciones e $\nm{y}$ el vector con las etiquetas reales de las mismas, entonces podemos calcular las medidas de de evaluación anteriores como: $$\text{ECM}=\frac{1}{n}\sum_{i=1}^n (\hat{y}_i-y_i)^2.$$ $$\text{RMSE}=\sqrt{\text{ECM}}=\sqrt{\frac{1}{n}\sum_{i=1}^n (\hat{y}_i-y_i)^2}.$$
\end{proposition}

\chapter{Teoría de grafos}

En esta capítulo incluiremos algunos conceptos fundamentales que nos serán útiles en la construcción de los modelos causales que funcionan como base del concepto de equidad contrafactual que trataremos a lo largo del trabajo. La referencia básica de este capítulo es el libro de \cite{graphtheory2001}.

\section{Grafos, nodos y aristas}

\begin{definition}[Par ordenado]
Un \textit{par ordenado} es una pareja de elementos, en la que los elementos vienen distinguidos por su orden. El par ordenado donde el primer elemento es $a$ y el segundo $b$ se denota como $(a,b)$. Si el orden no es relevante, lo llamaremos \textit{par no ordenado} y lo denotaremos $\{a,b\}$. En este caso, es claro que $\{a,b\}=\{b,a\}$.
\end{definition}\

\begin{definition}[Grafo]
Un \textit{grafo} $G = (V,E)$ es un conjunto no vacío de vértices o nodos $V$ y aristas $E \subset V\times V$ entre ellos.
Si $E$ consta de pares ordenados de vértices lo llamaremos \textit{grafo dirigido}, si en otro caso $E$ consta de pares no ordenados, lo llamaremos \textit{grafo no dirigido}.
\end{definition}\

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (a) {A};
\node[mynode,below =of a] (b) {B};
\node[mynode, right=of a] (d) {D};
\node[mynode,below =of d] (c) {C};

\node[mynode, right=of d] (e) {A};
\node[mynode,below =of e] (f) {B};
\node[mynode, right=of e] (g) {D};
\node[mynode,below =of g] (h) {C};

\path (a) edge[-latex] (d)
(a) edge[-latex] (b)
(c) edge[latex-] (b)
(c) edge[latex-] (d);

\draw (e) -- (f) -- (h) -- (g) -- (e);

\end{tikzpicture}
\caption{Ejemplo de un grafo dirigido y un grafo no dirigido, respectivamente.}
\label{fig:dirungraphs}
\end{figure}\

\begin{definition}[Camino]
En un grafo dirigido $G=(V,E)$, un \textit{camino} $A \to B$ es una secuencia de vértices $\{A = v_0,v_1,\dots,v_{n-1}, v_n = B\}$ donde $(v_i, v_{i+1}) \in E \ \text{para todo} \ i \in
\{0,\dots ,n-1\}$. Si $G$ es un grafo no dirigido, $A\to B$ es un \textit{camino} si $\{v_i,v_{i+1}\} \in E \ \text{para todo} \ i \in \{0,\dots,n-1\}$.
\end{definition}\

\begin{definition}[Grafo acíclico dirigido]
Un \textit{grafo acíclico dirigido} es un grafo dirigido que no tiene ciclos, es decir, que para cada nodo $v\in V$, no existe ningún camino que empiece y termine en $v$. Si un grafo no es acíclico lo llamaremos \textit{ciclo}.
\end{definition}\

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (a) {A};
\node[mynode,below =of a] (b) {B};
\node[mynode, right=of a] (d) {D};
\node[mynode,below =of d] (c) {C};

\node[mynode, right=of d] (e) {A};
\node[mynode,below =of e] (f) {B};
\node[mynode, right=of e] (g) {D};
\node[mynode,below =of g] (h) {C};

\path (a) edge[-latex] (d)
(a) edge[-latex] (b)
(c) edge[latex-] (b)
(c) edge[latex-] (d);

\path (e) edge[-latex] (f)
(g) edge[-latex] (e)
(h) edge[latex-] (f)
(g) edge[latex-] (h);


\end{tikzpicture}
\caption{Ejemplo de un grafo acíclico dirigido y un ciclo dirigido, respectivamente.}
\label{fig:acigraphs}
\end{figure}

\begin{example}
Podemos ver que el grafo de la izquierda de la Figura \ref{fig:acigraphs} es acíclico puesto que sea $v\in \{A,B,C,D\}$ no existe ningún camino que revisite $v$. Por otro lado, el grafo de la derecha es un ciclo, ya que el camino $A\to B \to C \to D \to A$ es directo y empieza y termina en $A$.
\end{example}\

\begin{definition}[Bucle]
Sea $G=(V,E)$ un grafo y $v\in V$ un vértice del mismo. Diremos que $v$ tiene un \textit{bucle} si $(v,v)\in E$.
\end{definition}\

\begin{definition}[Grafo simple]
Diremos que $G$ \textit{grafo simple} si no posee bucles ni ninguna de sus aristas relacionan al mismo par de vértices. Llamaremos a $G$ \textit{multigrafo} si, y solo si, no es un grafo simple.
\end{definition}

\section{Estructura de un grafo}

Ahora definiremos algunas relaciones entre nodos en un grafo acíclico dirigido.\\

\begin{definition}[Ancestros y descendientes de un nodo] \label{def:ancestro}
Sean $A,B$ dos vértices de un grafo dirigido $G$. Si $A \to B$ es un camino dirigido y $B \not \to A$ (no existe un camino dirigido de $B$ a $A$), entonces diremos que $A$ es el \textit{ancestro} de $B$ y $B$ es \textit{descendiente} de $A$.
\end{definition}\

\begin{example}
En el grafo de la derecha de la Figura \ref{fig:acigraphs}, el nodo $A$ es un ancestro de  $B$, $D$ y $C$. Por otro lado, el nodo $C$ es un descendiente de $A$, $B$ y $D$.
\end{example}

\clearpage

\begin{definition}[Padres e hijos de un nodo]
Particularizando la Definición \ref{def:ancestro}, definimos a los \textit{padres} de un nodo $A$ como el conjunto de nodos $pa(A)$ tal que existe una arista dirigida para cada nodo de $pa(A)$ hacia $A$, la noción inversa de padres define el concepto de \textit{hijos} a los que denotaremos como $hi(A)$.
\end{definition}\

\begin{definition}[Vecinos de un nodo]
Sea un grafo $G$, definimos los \textit{vecinos} de un nodo como todos los nodos directamente conectados a él. Denotaremos al conjunto de vecinos de un nodo $A$ como $ve(A)$.
\end{definition}\

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (a) {A};
\node[mynode,below right=of a] (b) {B};
\node[mynode,above right=of b] (c) {C};
\node[mynode,below right=of b] (d) {D};
\node[mynode,below left=of b] (e) {E};
\node[mynode,right=of c] (f) {F};

\path (c) edge[-latex] (a)
(a) edge[-latex] (b)
(b) edge[latex-] (c)
(b) edge[-latex] (e)
(c) edge[-latex] (f)
(b) edge[-latex] (d)
(f) edge[-latex] (d)
;

\end{tikzpicture}
\caption{Ejemplo de grafo acíclico dirigido.}
\label{fig:relations}
\end{figure}

\begin{example}
Para comprender las definiciones anteriores, definiremos los conjuntos de padres, hijos y vecinos del nodo $B$ para la Figura \ref{fig:relations}: $$pa(B)=\{A,C\}, \ \ hi(B)=\{E,D\}, \ \ ve(B)=\{A,C,E,D\}.$$
\end{example}

\ctparttext{\color{black}\begin{center}
Conceptos básicos del aprendizaje automático, formalización de las medidas de equidad y definición de los principales algoritmos de mitigación del sesgo.
\end{center}}
\part{Justicia en Aprendizaje Automático}


\chapter{Conceptos básicos del Aprendizaje Automático}

En este capítulo presentaremos algunos conceptos básicos del aprendizaje automático. Definiremos el concepto de aprendizaje supervisado, discutiremos el proceso de creación de modelos predictivos a partir de un conjunto de datos dado y comentaremos las medidas básicas de evaluación de un modelo de clasificación.

\section{¿Qué es el aprendizaje automático?}

\label{sec:queaprendizaje}

El \textit{aprendizaje automático} o \textit{machine learning} se encarga de extraer patrones significativos de un conjunto de datos con el objetivo de inferir en la distribución estadística subyacente (\cite{pattern2006}). Para ello, durante la fase de entrenamiento, aprendemos un modelo a partir de un conjunto de datos de interés seleccionado previamente. Una vez entrenado el modelo, podremos predecir o tomar decisiones sin que el modelo haya sido específicamente programado para esa tarea. Los patrones generales obtenidos a partir del entrenamiento del modelo podrán aplicarse posteriormente a datos no vistos y seguir obteniendo resultados de utilidad. 

A grandes rasgos, consideramos tres tipos de algoritmos de aprendizaje automático: los de aprendizaje supervisado que actúan sobre datos etiquetados, los de aprendizaje no supervisado que actúan sobre datos no etiquetados y los de aprendizaje por refuerzo que actúan en un entorno de ensayo-error. Nos centraremos en el entorno de \textit{aprendizaje supervisado}, ya que nuestro trabajo utilizará algoritmos de este tipo. 

\subsection{Aprendizaje supervisado}

\label{subsec:ejemplobanco}

Definiremos los componentes del aprendizaje a partir de un ejemplo real: la aprobación de un crédito bancario. En principio, el banco no conoce ninguna fórmula ideal que pueda decirle cuando debe aprobar un crédito. El banco utilizará los registros de los clientes anteriores para aprender sobre ellos y encontrar una buena fórmula para la aprobación de las nuevas peticiones de créditos. Cada registro de clientes tiene información relativa al mismo, como pueden ser salario anual, años de residencia, préstamos pendientes, etc. También registra si la aprobación del crédito para ese cliente fue una buena idea, es decir, si le proporcionó o no beneficios a la entidad financiera. Estos datos serán los que guíen la construcción de una fórmula de éxito para la aprobación del crédito que podrá utilizarse con futuros solicitantes.

A continuación, formalizaremos los principales componentes de este problema de aprendizaje. Tenemos el vector de entrada $\nm{x}$ que contiene la información del cliente que se utilizará para tomar la decisión del crédito, la \textit{función objetivo desconocida} $f\colon \mathcal{X}\to \mathcal{Y}$ (fórmula ideal para la aprobación del crédito), donde $\mathcal{X}$ es el conjunto de todas las posibles características e $\mathcal{Y}$ el conjunto de todos los posibles resultados (en este caso, una decisión binaria si/no). Tenemos un \textit{conjunto de datos} consistente en pares de entrada-salida $\mathcal{D}=\{(\nm{x}_1,y_1),\dots,(\nm{x}_n,y_n)\}$ donde cada  $y_i$ viene dado por una función desconocida $y_i=f(\nm{x}_i)$ con $i=1,\dots,n$ (los valores de entrada corresponden a los antiguos clientes y el resultado será la decisión de crédito correcta para ellos en retrospectiva).

Finalmente, contamos con el \textit{algoritmo de aprendizaje} $\mathcal{A}$ que utiliza el conjunto de datos $\mathcal{D}$ para elegir una fórmula $g\colon \mathcal{X}\to \mathcal{Y}$ que mejor aproxime a la función ideal $f$. El algoritmo elegirá $g$ de entre un conjunto de fórmulas candidatas consideradas, al que denominamos \textit{conjunto de hipótesis} $\mathcal{H}$. Por ejemplo, $\mathcal{H}$ podría ser el conjunto de todas las fórmulas lineales de las que el algoritmo elegiría la que mejor ajuste linealmente a los datos.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=12.6cm]{setup_learning.png}
	\caption{Configuración básica del problema de aprendizaje. (\cite{learning2012})}
    \label{fig:learningesq}
\end{figure}

\clearpage

Cuando un nuevo cliente solicite un crédito, el banco basará su decisión en $g$ (la hipótesis que produjo el algoritmo de aprendizaje), no en $f$ (la
función objetivo ideal que sigue siendo desconocida). La decisión será buena solo en la medida en que $g$ replique $f$. Para ello, el algoritmo elige $g$ que mejor se ajuste a $f$ en los ejemplos de entrenamiento de clientes anteriores, con la esperanza de que siga coincidiendo con $f$ en los nuevos clientes.

\section{Propiedades del modelo de aprendizaje}

Definiremos algunos conceptos importantes que surgen junto a algunas cuestiones planteadas una vez que hemos creado nuestro modelo de aprendizaje (\cite{fairnesslearning2019}). Suponemos un conjunto de datos etiquetado $\mathcal{D}=\{(\nm{x}_1,y_1),\dots,(\nm{x}_n,y_n)\}$. Normalmente los datos $(\nm{x}_1, y_1),\dots, (\nm{x}_n, y_n)$ se extraen de forma independiente e idénticamente distribuida de una población $(\mathcal{X},\mathcal{Y})$.\\

\begin{definition}[Clasificador arbitrario]
Un \textit{clasificador arbitrario} se define como una aplicación $g\colon \mathcal{X} \to \mathcal{Y}$ del conjunto de características $\mathcal{X}$ al conjunto de resultados $\mathcal{Y}$, de forma que $ g(\nm{x})$ es el resultado predicho para el individuo $\nm{x}$.
\end{definition}\

\begin{definition}[Función de pérdida]
Una \textit{función de pérdida} es una función definida como $\ell \colon \mathcal{Y} \times \mathcal{Y} \to \R$ que asigna un valor real no-negativo $\ell(y',y)$ que denota el coste del valor de la predicción $y'$ cuando la etiqueta real es $y$.
\end{definition}\

\begin{definition}[Riesgo empírico]
El \textit{riesgo empírico} de un clasificador arbitrario $g$ con respecto a un conjunto de datos $\mathcal{D}$ dada una función de pérdida $\ell$ se define como $$R_{\mathcal{D}}(g)=\frac{1}{n}\sum_{i=1}^n \ell(g(\nm{x}_i),y_i).$$
\end{definition}\

\begin{definition}[Minimización del riesgo empírico] \label{def:minrisk}
La \textit{minimización del riesgo empírico} es el problema de optimización que busca encontrar un clasificador $g$ en una familia de funciones $\mathcal{H}$ tal que minimice el riesgo empírico, $$\underset{g\in \mathcal{H}}{\text{arg min}} \  R_{\mathcal{D}}(g).$$
\end{definition}\

La introducción del concepto de minimización del riesgo empírico genera diversas dudas que intentaremos abordar a lo largo de este capítulo y que pueden resumirse en las siguientes tres preguntas:

\begin{itemize}
    \item[1.] ¿Cuál es la clase de funciones $\mathcal{H}$ que deberíamos escoger?
    \item[2.] ¿Cómo podemos resolver de manera eficiente el problema de optimización resultante?
    \item[3.] ¿El clasificador encontrado tendrá el mismo rendimiento sobre los ejemplos del conjunto de entrenamiento que sobre el conjunto de datos de prueba?
\end{itemize}

Estas cuestiones están relacionadas entre sí y dan lugar a los conceptos de \textit{representación, optimización} y \textit{generalización} respectivamente.

\subsection*{Representación}

Normalmente las aplicaciones que utilizan datos con un número relativamente pequeño de características suelen usar \textit{modelos de predicción lineales}. Por otro lado, si los datos de entrenamiento del modelo incluyen imágenes o audio, se suelen aplicar \textit{modelos no lineales}. Las redes neuronales por ejemplo, aplican una secuencia de transformaciones de cada tipo para obtener mejores resultados sobre el conjunto de datos de entrada.

Lo más relevante de la representación para este trabajo es conocer que la mecánica de entrenamiento de un modelo sigue siendo la misma, y los detalles del tipo de modelo utilizado rara vez importa para las cuestiones relativas a la equidad.


\subsection*{Optimización}

Si nuestro objetivo es minimizar la exactitud de un clasificador, sería evidente pensar en resolver directamente el problema de minimización del riesgo empírico con respecto a la siguiente función de pérdida $$\ell(y',y)=\left \{
\begin{array}{l l}
1, & \mbox{si } y\neq y', \\
0, & \mbox{si } y=y'.
\end{array}
\right.$$

El problema de usar esta función es que es difícil de optimizar. Los gradientes de la pérdida 0-1 toman el valor cero en todo su dominio, por lo que no podemos esperar que los métodos basados en el gradiente optimicen directamente la pérdida 0-1.

A continuación, veremos una serie de métodos de optimización diferentes que, en determinadas circunstancias, encuentran un mínimo global o local del objetivo de riesgo empírico y aproximan en cierta medida la pérdida 0-1.

\begin{itemize}
    \item \textbf{Pérdida al cuadrado} (\textit{squared loss}) dada por $\frac{1}{2}(y-y')^2$. La minimización empírica del riesgo con esta función de pérdida equivale a la regresión lineal por mínimos cuadrados. 
    \item \textbf{Pérdida de bisagra} (\textit{hinge loss}) se expresa como $\max\{1-yy',0\}$. Los algoritmos SVM se refieren a la minimización empírica del riesgo con esta función junto con la regularización $\ell_2$.
    \clearpage
    \item \textbf{Pérdida logística} (\textit{logistic loss}) definida como $$\left \{
    \begin{array}{l l}
    -\log(\sigma(y')), & \mbox{si } y=1, \\
    -\log(1-\sigma(y')), & \mbox{si } y=-1.
    \end{array}
    \right.$$
    Donde $\sigma(y')=1/(1+e^{-y'})$ es la función logística. La minimización empírica del riesgo con esta función de pérdida equivale a la regresión logística. 
\end{itemize}

La elección de la función de pérdida se realizará comparando los rendimientos de las diferentes aproximaciones mediante prueba y error, eligiendo la que mejor funcione en cada caso.

\subsection*{Generalización}

La generalización en el aprendizaje automático hace referencia a cómo de bueno es un modelo predictivo que etiqueta correctamente datos con los que ha entrenado previamente realizando la misma tarea sobre un nuevo conjunto de datos que sigue la misma distribución de la que se extrajeron los datos de entrenamiento. %Este tipo de generalización se puede considerar una interpolación. El modelo es capaz de suavizar los huecos entre los datos de entrenamiento y de actuar bien para la distribución de la que proceden los datos. 

No obstante, incluso los modelos más avanzados suelen funcionar peor cuando los datos de prueba se extraen de una distribución que difiere ligeramente de la seguida por los datos de entrenamiento. Un ejemplo de ello fue el caso de la creación de un nuevo conjunto de pruebas para la base de datos ImageNet (\cite{generalize2019}).

\section{Creación de modelos de aprendizaje}

Dado el esquema de configuración del problema de aprendizaje supervisado de la Figura \ref{fig:learningesq} discutiremos la creación de un modelo simple de aprendizaje (\cite{learning2012}). Sea $\mathcal{X}=\R^d$ el espacio de entrada, donde $\R^d$ es el espacio euclídeo $d$-dimensional y sea $\mathcal{Y}=\{-1,1\}$ el espacio de salida denotando una decisión binaria (si/no). En el ejemplo de concesión de crédito, las diferentes coordenadas del vector $\nm{x}\in \mathcal{X}$ corresponden a los datos relativos del individuo que solicita el crédito. La salida binaria $y$ corresponde a la aprobación o denegación del préstamo. Especificamos el conjunto de hipótesis $\mathcal{H}$ mediante una forma funcional común a todas las hipótesis $h\in \mathcal{H}$. La forma funcional $h(\nm{x})$ elegida, asigna pesos diferentes a cada coordenada del vector $\nm{x}$, reflejando su importancia en la decisión del problema. Las coordenadas ponderadas se combinan para formar una puntuación y el resultado se compara con un valor umbral previamente establecido. Si el solicitante supera el umbral, el crédito es aprobado, si no, es denegado: $$\text{Aprobar crédito si } \sum_{i=1}^d w_ix_i > \text{ umbral},$$ $$\text{Denegar crédito si } \sum_{i=1}^d w_ix_i < \text{ umbral}.$$

Esta fórmula se puede escribir de forma más compacta como 
\begin{equation} \label{eq:sign-6}
h(\nm{x})=\text{sign}\left(\left( \sum_{i=1}^d w_ix_i\right)+b \right),
\end{equation}


donde $x_1,\dots,x_d$ son los componentes del vector $\nm{x}$; $h(\nm{x})=1$ significa la aprobación del crédito y $h(\nm{x})=-1$ significa la denegación del crédito; $\text{sign}(s)=1$ si $s>0$ y $\text{sign}(s)=-1$ si $s<0$. Los pesos $w_1,\dots,w_d$ y el umbral viene determinado en términos del sesgo $b$, el crédito se aprueba si $\displaystyle \sum_{i=1}^d w_ix_i > -b.$ 

Uno de los ejemplos más comunes en la literatura del aprendizaje automático es el del \textit{perceptrón} introducido por el neurocientífico e informático teórico \cite{perceptron1957}. Este algoritmo de aprendizaje intenta encontrar $\mathcal{H}$ buscando los pesos y el sesgo que funcionen bien en el conjunto de datos. Algunos de los pesos $w_1,\dots,w_d$ podrían acabar siendo negativos, teniendo un efecto adverso en la aprobación del crédito. Por ejemplo, el peso del campo relativo a deudas pendientes debería ser negativo, ya que una mayor deuda no es buena señal para la aprobación de un crédito. El valor del sesgo $b$ podría terminar siendo muy grande o muy pequeño, reflejando lo indulgente o estricto que debe ser el banco a la hora de conceder créditos. La elección óptima de los pesos y el sesgo define la hipótesis final $g \in \mathcal{H}$ que produce el algoritmo.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=12.4cm]{gbclassified.png}
	\caption{Clasificación de datos linealmente separables en un espacio bidimensional.}
    \label{fig:linsep}
\end{figure}

En la Figura \ref{fig:linsep} ilustra dos ejemplos de clasificación de un perceptrón en un espacio de entrada bidimensional ($d=2$). En el dibujo de la izquierda los ejemplos están perfectamente clasificados mientras que en el de la derecha hay algunos ejemplos mal clasificados. Diferentes valores para los parámetros $b,w_1,w_2$ dan lugar a diferentes rectas $w_1x_1+w_2x_2+b=0$. Si el conjunto de datos es linealmente separable existe una elección de parámetros que clasifica todos los ejemplos correctamente.

\subsection{Ejemplo: Perceptrón} \label{subsec:perceptron}

Introduciremos el algoritmo de del perceptrón como un ejemplo simple de un modelo de aprendizaje. Para simplificar la notación de la fórmula del perceptrón, trataremos el sesgo $b$ como un peso $w_0=b$ y lo añadiremos como una coordenada más al vector de pesos $\nm{w}=(w_0,w_1,\dots,w_d)^T$. Además añadimos una coordenada $x_0=1$ al vector $\nm{x}=(x_0,x_1,\dots,x_d)^T$. Observar que tratamos a $\nm{x}$ y $\nm{w}$ como vectores columna. Formalmente denotaremos el espacio de entrada como $$\mathcal{X}=\{1\}\times \R^d=\{\nm{x}=(x_0,x_1,\dots,x_d)^T \ : \ x_0=1, \ x_1,\dots,x_d \in \R\}.$$

Denotando $\nm{w}^T\nm{x}=\displaystyle \sum_{i=0}^d w_ix_i$, podemos reescribir la Ecuación \ref{eq:sign-6} como $$h(\nm{x})=\text{sign}(\nm{w}^T\nm{x}).$$

El algoritmo determinará el valor de $\nm{w}$ basado en los datos. Supondremos que el conjunto de datos es linealmente separable, lo que significa que podemos encontrar un vector $\nm{w}$ tal que $h(\nm{x})$ consigue una decisión correcta $h(\nm{x}_n)=y_n$ para todos los ejemplos del conjunto de datos de entrenamiento.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=7.8cm]{movclassified.png}
	\caption{Esquema de actualización del algoritmo del perceptrón.}
    \label{fig:movsep}
\end{figure}

Nuestro algoritmo de aprendizaje encontrará este $w$ usando un simple método iterativo, que funciona de la siguiente forma:

En cada paso $t=0,1\dots$, hay un valor actual del vector de pesos $\nm{w}_t$, selecionamos aleatoriamente un índice $i\in \{1,\dots,n\}$ correspondiente a un ejemplo actualmente mal clasificado lo denota como $(\nm{x}_t,y_t)$ y lo usa para actualizar el valor de $\nm{w}_t$. Como el ejemplo está mal clasificado, tenemos que $y_t\neq \text{sign}(\nm{w}^T_t\nm{x}_t)$. La regla de actualización es $$\nm{w}_{t+1}=\nm{w}_t+y_t\nm{x}_t.$$

Esta regla mueve la frontera con el objetivo de cambiar la dirección en la clasificación correcta de $\nm{x}_t$, como se puede ver en la Figura \ref{fig:movsep}. El algoritmo continua con las sucesivas iteraciones hasta que no haya ejemplos mal clasificados en el conjunto de datos.

Aunque la regla de actualización solo considera un único ejemplo del conjunto de entrenamiento y podría desordenar la clasificación para otros ejemplos no implicados en la iteración actual, el algoritmo garantiza una solución óptima (véase Teorema 1 en~\cite{perceptronproof2012}). El resultado se mantiene de forma independiente al ejemplo que elijamos y a la inicialización del vector de pesos al comienzo del algoritmo. Por simplicidad, elegiremos un ejemplo mal clasificado aleatorio e inicializaremos $w_0$ a un vector de ceros.

\subsection*{Otra definición del algoritmo}

Podemos definir el algoritmo de perceptrón como una instancia de la minimización del riesgo empírico (\cite{fairnesslearning2019}). Por la descripción del algoritmo, buscamos un separador lineal y por tanto, nuestra clase de funciones corresponde con el conjunto de funciones lineales $$\mathcal H = \{ f(\nm{x}) = \langle \nm{w},\nm{x} \rangle \ : \ \nm{w} \in \mathbb R^d \}.$$

Utilizaremos el método de gradiente estocástico como base del algoritmo, ya que es un método de optimización que elige un ejemplo aleatorio en cada paso y realiza una actualización de los parámetros del modelo. Se define con la regla siguiente: $$\nm{w}_{t+1}=\nm{w}_t-\eta \nabla_{\nm{w}_t} \ell (f(\nm{x}_t),y_t).$$

Donde $\nabla \ell (f(\nm{x}_t),y_t)$ es el gradiente de la función de pérdida con respecto a los parámetros del modelo $\nm{w}_t$ para un ejemplo $(\nm{x}_t,y_t)$. El escalar $\eta > 0$, es un parámetro denominado \textit{tamaño de paso}, pensaremos en él como una constante pequeña.

Consideramos ahora la función de pérdida $$\ell(y,\langle \nm{w},\nm{x} \rangle)=\max (1-y\langle \nm{w},\nm{x} \rangle,0),$$

donde su gradiente se define como: $$\nabla \ell (y,\langle \nm{w},\nm{x} \rangle)=\left \{
\begin{array}{l l}
-y\nm{x}, & \mbox{si } y\langle \nm{w},\nm{x} \rangle < 1, \\
0, & \mbox{si } y\langle \nm{w},\nm{x} \rangle > 1.
\end{array}
\right.$$

La expresión anterior define una parte de la regla de actualización del perceptrón, la otra parte la deduciremos al añadir la penalización $\frac{\upalpha}{2}\norm{\nm{w}}^2$ (donde $\norm{\cdot}$ denota la norma euclídea) a la función de pérdida para impedir a los pesos tomar valores por encima de un umbral establecido. Esta penalización se denomina \textit{regularización $\ell_2$} o \textit{regularización de Tíjonov} y su propósito es fomentar la generalización.

Sumando las dos funciones de pérdida, obtenemos una expresión del riesgo empírico regularizado por $\ell_2$ para la función de pérdida de bisagra: $$R_{\mathcal{D}}(\nm{w})=\frac{1}{n}\sum_{i=1}^n \max (1-y_i\langle \nm{w},\nm{x}_i \rangle,0)+\frac{\upalpha}{2}\norm{\nm{w}}^2.$$

Definiremos el algoritmo perceptrón como la resolución a este problema de minimización del riesgo empírico utilizando el método del gradiente estocástico.

\subsection{Regresión lineal}

Volviendo al ejemplo del apartado \ref{subsec:ejemplobanco}, recordemos que el banco tiene un registro de clientes con variables que pueden ser usadas para aprender un clasificador lineal de decisión para la aprobación del crédito. En este caso, en lugar de limitarnos a tomar una decisión binaria (aprobar o no el crédito), en el caso de aprobación, podríamos querer establecer un umbral en el valor del crédito concedido. Esta tarea, puede ser automatizada haciendo uso del aprendizaje por \textit{regresión} (\cite{learning2012}). 

El banco parte de un conjunto de datos $\mathcal{D}=\{(\nm{x}_1,y_1),\dots,(\nm{x}_n,y_n)\}$, donde $\nm{x}_n$ es la información del cliente e $y_n$ es el límite de crédito establecido por uno de los expertos del banco. Ahora $y_n$ será un número real en lugar de un valor binario. El banco querrá usar un modelo de aprendizaje para encontrar una hipótesis $g$ que replique la actuación humana en los límites de crédito. En este caso, no buscaremos una función determinista $y = f(x)$ y en su lugar, asumiremos que la etiqueta $y_n$ proviene de una distribución $P(y \mid \nm{x})$. No obstante, la naturaleza del problema sigue siendo la misma. Tenemos una distribución desconocida $P(\nm{x}, y)$ que genera cada $(\nm{x}_n, y_n)$ y queremos encontrar una hipótesis $g$ que minimice el error entre $g(\nm{x})$ e $y$ con respecto a esa distribución.

\subsection*{Algoritmo de regresión lineal}

El algoritmo se basa en la minimización del riesgo empírico para la pérdida al cuadrado entre $h(\nm{x})$ e $y$. El problema es equivalente a minimizar la siguiente función:
\begin{equation*}
    R_{\mathcal{D}}(h) = \frac{1}{n} \sum_{i=1}^n (h(\nm{x}_i)-y_i)^2.
\end{equation*}

En regresión lineal, $h$ se expresa como combinación lineal de las componentes de $\nm{x}$: 
\begin{equation*}
    h(\nm{x})=\sum_{i=0}^d w_i x_i = \nm{w}^T \nm{x},
\end{equation*}

donde $x_0=1$ y $\nm{x} \in \{1\} \times \R^d$ y $\nm{w} \in \R^{d+1}$. Para el caso lineal se suele definir una matriz de representación para $R_{\mathcal{D}}(h)$. La matriz de datos $X\in \R^{N \times (d+1)}$ tiene como filas los vectores $\nm{x}_n$ e $\nm{y}\in \R^n$ como vector columna cuyas componentes son los valores objetivo $y_n$. Podemos expresar el error en función de $\nm{w}, X$ e $\nm{y}$ como:
\begin{equation}
\begin{split}
    R_{\mathcal{D}}(\nm{w})&=\frac{1}{n}\sum_{i=1}^n (\nm{w}^T\nm{x}_i-y_i)^2\\
    &=\frac{1}{n} \norm{X\nm{w}-\nm{y}}^2\\
    &=\frac{1}{n} (\nm{w}^TX^TX\nm{w} -2\nm{w}^TX^T\nm{y}+\nm{y}^T\nm{y}).
\end{split}
\label{eq:derivable}
\end{equation}

Nuestro problema se reduce a minimizar la función dada por: 
\begin{equation}
    \underset{\nm{w}\in \R^{d+1}}{\text{arg min}} \ R_{\mathcal{D}}(\nm{w}).
    \label{eq:soloptima}
\end{equation}\

\begin{figure}[h]
	\centering
	\includegraphics[width=13.3cm]{linear_regresion.png}
	\caption{Problema de regresión lineal en una y dos dimensiones respectivamente.}
    \label{fig:reglin}
\end{figure}

La Figura \ref{fig:reglin} ilustra la solución para los casos unidimensional y bidimensional respectivamente. La Ecuación \ref{eq:derivable} implica que $R_{\mathcal{D}}(\nm{w})$ es diferenciable, por lo que podemos encontrar el mínimo de esta función igualando su gradiente a cero.
\begin{equation*}
    \nabla R_{\mathcal{D}}(\nm{w}) = \frac{2}{n} (X^TX\nm{w}-X^T\nm{y}) = 0.
\end{equation*}

Para encontrar la solución se debe cumplir que $X^TX\nm{w}=X^T\nm{y}$. Si $X^TX$ es regular, $\nm{w}=(X^TX)^{-1}X^T \nm{y}$ es la única solución óptima para la Ecuación \ref{eq:soloptima}. En otro caso, existirá una solución óptima, pero no será única.

\section{Evaluación en aprendizaje automático}

\label{sec:evalaa}

Un modelo en aprendizaje automático funciona bien, cuando puede predecir resultados correctos a partir de un conjunto de datos de entrada no conocido previamente. A este proceso lo conocemos como generalización. La manera de medir este fenómeno, no es única, por lo que existen una gran cantidad de \textit{métricas de rendimiento} en el ámbito del \textit{machine learning}. La elección de la métrica dependerá del problema específico, su dominio y de las restricciones del mismo al mundo real. Sin embargo, la mayoría de las métricas vienen descritas como funciones de una \textit{matriz de confusión}. 

Una matriz de confusión define un modelo de predicción a partir de las dimensiones de los \textit{valores reales de las etiquetas} y de sus posibles \textit{predicciones}, resumiendo el número de predicciones correctas e incorrectas por clase.\\

\begin{table}[h]
\centering
\resizebox{9.4cm}{!} {
\begin{tabular}{cc|c|c|}
\cline{3-4}
\multicolumn{1}{l}{}                                                                                      & \multicolumn{1}{l|}{} & \multicolumn{2}{c|}{\textbf{Etiqueta real}}                                                                                          \\ \cline{3-4} 
                                                                                                          &                       & \textbf{$y=1$}                                                     & \textbf{$y=-1$}                                                    \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Predicción \end{tabular}}}} & \textbf{$\hat{y}=1$}        & \begin{tabular}[c]{@{}c@{}}Verdadero \\ Positivo (TP)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Falso\\ Positivo (FP) \\ \textcolor{blue}{(Error tipo I)}\end{tabular}     \\ \cline{2-4} 
\multicolumn{1}{|c|}{}                                                                                    & \textbf{$\hat{y}=-1$}        & \begin{tabular}[c]{@{}c@{}}Falso \\ Negativo (FN) \\ \textcolor{blue}{(Error tipo II)}\end{tabular}     & \begin{tabular}[c]{@{}c@{}}Verdadero\\ Negativo (TN)\end{tabular} \\ \hline
\end{tabular}
}
\caption{Matriz de confusión, ilustra la relación entre la etiqueta real y la predicción.}
\label{tab:table2}
\end{table}

En el caso de una matriz de confusión binaria como la del Cuadro \ref{tab:table2}, tenemos una matriz $2 \times 2$ que nos ofrece información sobre el número de \textit{verdaderos positivos} (TP) ($y=1 \ \wedge \ \hat{y}=1$), \textit{falsos positivos} (FP) ($y=-1 \ \wedge \ \hat{y}=1$), \textit{falsos negativos} (FN) ($y=1 \ \wedge \ \hat{y}=-1$) y \textit{verdaderos negativos} (TN) ($y=-1 \ \wedge \ \hat{y}=-1$). A partir de ellos, podemos obtener el número total de positivos predichos (TP+FP), el número total de negativos predichos (TN+FN), el número total de etiquetados positivos (TP+FN), y
el número total de etiquetados negativos (TN+FP). 

Además, podemos definir las siguientes métricas de clasificación avanzadas construidas a partir de combinaciones lineales de las básicas:\\

\begin{itemize}
    \item \textbf{Tasa de verdaderos positivos} (\textit{Recall}), $$\text{TPR}=\ddfrac{\text{TP}}{\text{TP+FN}}.$$
    \item \textbf{Tasa de falsos negativos}, $$\text{FNR}=1-\text{TPR}=\ddfrac{\text{FN}}{\text{TP+FN}}.$$
\end{itemize}

\clearpage

\begin{itemize}
    \item \textbf{Tasa de verdaderos negativos} (\textit{Specificity}), $$\text{TNR}=\ddfrac{\text{TN}}{\text{TN+FP}}.$$
    \item \textbf{Tasa de falsos positivos}, $$\text{FPR}=1-\text{TNR}=\ddfrac{\text{FP}}{\text{TN+FP}}.$$
    \item \textbf{Valor positivo predictivo} (\textit{Precision}),  $$\text{PPV}=\ddfrac{\text{TP}}{\text{TP+FP}}.$$
    \item \textbf{Tasa de falso descubrimiento}, $$\text{FDR}=1-\text{PPV}=\ddfrac{\text{FP}}{\text{TP+FP}}.$$
    \item \textbf{Valor negativo predictivo}, $$\text{NPV}=\ddfrac{\text{TN}}{\text{TN+FN}}.$$
    \item \textbf{Tasa de falsa omisión}, $$\text{FOR}=1-\text{NPV}=\ddfrac{\text{FN}}{\text{TN+FN}}.$$
    \item \textbf{Exactitud} (\textit{Accuracy}), $$\text{Accuracy}=\ddfrac{\text{TP+TN}}{\text{TP+TN+FP+FN}}.$$
\end{itemize}\

\begin{table}[h]
\centering
\resizebox{14.1cm}{!} {
\begin{tabular}{cc|c|c|cc}
\cline{3-4}
                                                           &              & \multicolumn{2}{c|}{\textbf{Etiqueta real}}                                                                                                                                 &                                                                                                             &                                                                                                                    \\ \cline{3-4}
                                                           &              & $y=1$                                                                                   & $y=-1$                                                                                  &                                                                                                             &                                                                                                                    \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Predicción}}} & $\hat{y}=1$  & \begin{tabular}[c]{@{}c@{}}Verdadero \\ Positivo (TP)\end{tabular}                      & \begin{tabular}[c]{@{}c@{}}Falso\\ Positivo (FP)\end{tabular}                           & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Precision}\\ $\ddfrac{TP}{TP+FP}$\end{tabular}}               & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Tasa de Falso}\\ \textbf{Descubrimiento}\\ $\ddfrac{FP}{TP+FP}$\end{tabular}} \\ \cline{2-6} 
\multicolumn{1}{|c|}{}                                     & $\hat{y}=-1$ & \begin{tabular}[c]{@{}c@{}}Falso \\ Negativo (FN)\end{tabular}                          & \begin{tabular}[c]{@{}c@{}}Verdadero\\ Negativo (TN)\end{tabular}                       & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Tasa de Falsa}\\ \textbf{Omisión}\\ $\ddfrac{FN}{TN+FN}$\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Valor Negativo}\\ \textbf{Predictivo}\\ $\ddfrac{TN}{TN+FN}$\end{tabular}}    \\ \hline
                                                           &              & \begin{tabular}[c]{@{}c@{}}\textbf{Recall}\\ $\ddfrac{TP}{TP+FN}$\end{tabular}                   & \begin{tabular}[c]{@{}c@{}}\textbf{Tasa de Falsos}\\ \textbf{Positivos}\\ $\ddfrac{FP}{TN+FP}$\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Accuracy}\\ $\ddfrac{TP+TN}{TP+TN+FP+FN}$\end{tabular}}       &                                                                                                                    \\ \cline{3-5}
                                                           &              & \begin{tabular}[c]{@{}c@{}}\textbf{Tasa de Falsos}\\ \textbf{Negativos}\\ $\ddfrac{FN}{TP+FN}$\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Specificity}\\ $\ddfrac{TN}{TN+FP}$\end{tabular}              &                                                                                                             &                                                                                                                    \\ \cline{3-4}
\end{tabular}
}
\caption{Matriz de confusión con métricas de evaluación avanzadas.}
\label{tab:tableadv}
\end{table}

\clearpage

Normalmente mediremos el rendimiento de un modelo con una de las métricas mencionadas o con una combinación de ambas. Una de las medidas de rendimiento más usadas es el valor positivo predictivo o precisión, que mide el porcentaje de predicciones correctas realizas. No obstante, existen problemas derivados de utilizar la precisión a la hora de medir el rendimiento cuando abordamos problemas que contienen desequilibrio entre las clases del conjunto de datos. Por ejemplo, si solo $5$ de cada $100$ muestras es positiva, el modelo trivial que siempre prediga la clase negativa tendrá una precisión del $95\%$.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=7.8cm]{ROC_curve.png}
	\caption{Ejemplo de un gráfico de curva ROC.}
    \label{fig:curvaROC}
\end{figure}

Otras métricas comúnmente utilizadas son la \textit{F$_1$-score} la cual podemos calcularla como F$_1$-score$=2 \cdot \ddfrac{\text{PPV}\cdot \text{TPR}}{\text{PPV}+\text{TPR}}$ y el \textit{área bajo la curva ROC} (AUC). El espacio ROC se representa en
un gráfico bidimensional, en el que la tasa de verdaderos positivos (TPR) se representa en el eje vertical y la tasa de falsos positivos (FPR) en
el eje horizontal. Como podemos ver en la Figura \ref{fig:curvaROC}, diferentes umbrales de clasificación corresponden a diferentes puntos en el espacio ROC.  

El área bajo la curva ROC (AUC) se interpreta como la probabilidad de que el modelo clasifique un ejemplo positivo aleatorio más alto que un ejemplo negativo aleatorio. Al ser una probablididad, el AUC oscilará entre los valores $0$ y $1$. Un modelo cuyas predicciones sean un 100\% incorrectas tendrá un AUC de $0$, mientras que otro cuyas predicciones sean un 100\% correctas tendrá un AUC de $1$. También es importante saber que el AUC es invariable con respecto a la escala y con respecto al umbral de clasificación.



\chapter{Formalización de las Medidas de equidad}

\label{ch:formalmedeq}

En este capítulo presentaremos el concepto de equidad, realizaremos un análisis de las distintas nociones de equidad conocidas y formalizaremos sus definiciones.

\section{¿Qué es la equidad?}

Con el aumento de los métodos de toma de decisiones automatizadas en la actualidad, la necesidad de satisfacer \textit{equidad} en los modelos de \textit{machine learning} ha cobrado importancia. Por ello, cabe hacerse las siguientes preguntas: ¿Qué es la equidad? ¿Cómo podemos medirla? ¿Y, cómo podemos fomentarla en nuestros algoritmos? En esta sección se intentará dar respuesta a estas preguntas.

En la Sección \ref{sec:queaprendizaje}, formalizamos el proceso de aprendizaje automático sobre un ejemplo de aprobación de créditos bancarios. Hemos visto que existe un proceso de apredizaje sobre registros históricos de datos que nos aportan información a la hora de predecir nuevos ejemplos, pero: ¿Existirán atributos que discriminen a un grupo determinado de la población?, ¿Dos clientes con características similares recibirán la misma predicción? Estas y otras preguntas son las motivaron el concepto de equidad.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=11.05cm]{exponencial.png}
	\caption{Incremento de las publicaciones sobre equidad entre 2011 y 2017.}
\end{figure}

\clearpage

Podemos dividir los trabajos sobre equidad en \textit{machine learning} en, detectar el sesgo y discriminación en los modelos (\cite{detect2012}) y mitigar el sesgo algorítmico (\cite{mitigate2017}). Para estas tareas, al igual que en cualquier ciencia experimental, debemos ser capaces de medir el concepto partiendo de una definición teórica. La equidad es un concepto inherentemente subjetivo y que depende en gran medida del ámbito en el que lo apliquemos. Por lo tanto, a partir de conceptos de la literatura de la ciencias sociales, se han ido proponiendo diferentes medidas y formalizando estos conceptos para que puedan ser aplicadas al aprendizaje automático.

La primera idea es buscar apoyo legal y comprobar si existe alguna definición que pueda utilizarse para formular la equidad matemáticamente. Las leyes antidiscriminatorias de muchos países prohíben el trato desigual entre personas en función de atributos sensibles, tales como el sexo o la raza (\cite{ley1964}). Estas leyes suelen evaluar la imparcialidad de un proceso de toma de decisiones utilizando dos nociones distintas (\cite{bigdata2016}): el tratamiento dispar y el impacto dispar. Un proceso de toma de decisiones sufrirá un trato dispar si basamos su juicio en el atributo sensible del sujeto, y tendrá un impacto dispar si sus resultados perjudican (o benefician) de forma desigual a personas con valores de atributos sensibles diferentes (por ejemplo, mujeres o afroamericanos).

\subsection{Principales familias de las medidas de equidad}

Los conceptos anteriores son demasiado abstractos como para tener una formulación cuantitativa directa por lo que, poco a poco, se han ido añadiendo numerosas definiciones de equidad a la literatura del aprendizaje automático. Sin embargo, el Cuadro \ref{tab:table1} recoge los criterios más importantes, los cuales han sido previamente recopilados en trabajos como \cite{formalizing2018} o \cite{definitions2018}.  

En este capítulo, nos centraremos en las medidas de equidad relativas a \textit{impacto y tratamiento dispar}, estableciendo a su vez una división más específica en algunas de ellas. Los conceptos que trataremos a lo largo de este capítulo son:

\begin{itemize}
    \item \textbf{Equidad por desconocimiento}.
    \item \textbf{Equidad individual}.
    \item \textbf{Equidad de grupo}: formalizaremos la paridad demográfica, el criterio de probabilidades igualadas y la tasa de paridad predictiva.
    \item \textbf{Medidas causales}: analizaremos su base matemática y desarrollaremos un ejemplo práctico de su aplicación en la Parte \ref{part:analisis_exp} de este trabajo.
\end{itemize}

La equidad basada en \textit{preferencias} está fuera del marco de este trabajo, por lo que no será explicado en profundidad. Para el lector interesado en este concepto, puede consultar el artículo \cite{preferences2017}.

\subsection{Medición de la parcialidad y la equidad}

Consideramos una tarea de la clasificación estándar en la que el objetivo es predecir una variable de resultado binaria $y \in \mathcal{Y}$ utilizando un vector de variables de entrada $\nm{x} \in \mathcal{X}$ que sigue una distribución de probabilidad $P_{\nm{x}}$.

Sea un clasificador arbitrario $g:\mathcal{X} \rightarrow \mathcal{Y}$, donde $\mathcal{X}=\R^d$ y donde $\mathcal{Y}=[0,1]$ si produce una probabilidad predicha (por ejemplo, regresión logística) y $\mathcal{Y}=\{-1,1\}$ si el clasificador produce un resultado predicho (por ejemplo, SVM). A lo largo del proyecto, normalmente asumiremos que el espacio de salida trabaja sobre decisiones binarias (si/no), es decir, $\mathcal{Y}=\{-1,1\}$.

Muchos de los problemas en los que aparece el concepto de equidad pueden formularse como problemas estadísticos de evaluación de riesgos en los que asignamos una puntuación de valor real $s \in [0,1]$ a cada individuo del conjunto de datos y se toma una decisión $\hat{y}$
basada en la puntuación, normalmente seleccionando un número predefinido ($k$) de
entidades que deben clasificarse como positivas.

Las principales definiciones que usaremos en este capítulo son las siguientes:

\begin{itemize}
    \item \textbf{Vector de características -} $\nm{x}\in \mathcal{X}$ es un vector de características reales que identifican a un individuo.
    \item \textbf{Puntuación -} $s \in [0,1]$ es una puntuación de valor real asignada a cada entidad por el clasificador.
    \item \textbf{Predicción -} $\hat{y} \in \mathcal{Y}$ es una predicción binaria asignada a un individuo determinado, basada en el umbral de la puntuación (por ejemplo, el máximo $k$).
    \item \textbf{Etiqueta real -} $y \in \{-1,1\}$ es la etiqueta binaria que representa el valor real de un individuo en concreto.
\end{itemize}

\section{Equidad por desconocimiento}

\label{sec:eqdesconocimiento}

La \textit{equidad por desconocimiento} se basa en eliminar los atributos sensibles para todos los individuos en el proceso de predicción. Algunos clasificadores propuestos en la literatura de aprendizaje automático satisfacen esta medida (\cite{detect2012}) debido a que es intuitiva y muy fácil de aplicar.

\begin{notation}
Sea $\Delta=\{ \pi(\nm{x})\colon \R^d \to \R^m  \ : \ \nm{x} \in \mathcal{X}, \ 1 \leq m \leq d\}$ el conjunto formado por las proyecciones del conjunto de individuos a todas las posibles dimensiones menores o iguales que la dimension de $\mathcal{X}$. Sea $\mathcal{A} \subset \Delta$ un subconjunto que contiene todos los individuos de $\mathcal{X}$ a los que se ha aplicado una proyección $\pi$ sobre ciertas características. Notaremos como $\mathcal{A}_i$ al elemento de $\mathcal{A}$ que contiene las características del individuo $\nm{x}_i\in \mathcal{X}$ que cierta proyección $\pi$ ha seleccionado.
\end{notation}\

\begin{definition}[Equidad por desconocimiento]
Sea $\mathcal{A} \subset \Delta$ donde $\mathcal{A}_i$ contiene las proyecciones sobre los atributos sensibles para el individuo $\nm{x}_i$ y sean $g\colon \mathcal{X} \to \mathcal{Y}$ y $h\colon \mathcal{X}\backslash \mathcal{A} \to \mathcal{Y}$ dos clasificadores arbitrarios. Diremos que $g$ logra \textit{equidad por desconocimiento} si, y solo si, 

\begin{equation*} 
g(\nm{x}_i)=h(\nm{x}_i \backslash \mathcal{A}_i), \ \text{para todo} \ \nm{x}_i \in \mathcal{X}.
\end{equation*}
\end{definition}\

Uno de los principales problemas de la equidad por desconocimiento es que no da una condición suficiente para evitar la discriminación ya que puede haber muchas características altamente correlacionadas (por ejemplo, la zona de residencia) que funcionen como sustitutos del atributo sensible (por ejemplo, la raza). Por lo tanto, no bastaría con eliminar el atributo sensible para eliminar las disparidades. Además, se han documentado diversos ejemplos de equidad por desconocimiento para la raza en ámbitos como educación, concesión de préstamos o justicia
penal y se ha demostrado que, a largo plazo, el enfoque ciego de la raza es menos eficaz que el enfoque consciente de la misma (\cite{ceguera2008}).

Las críticas anteriores cuestionan la idoneidad de la
equidad por desconocimiento en los dominios en
los que, los atributos sensibles pueden deducirse a partir de los atributos no sensibles disponibles y tenemos conocimiento de la existencia de barreras estructurales, que obstaculizan a los grupos desfavorecidos, a partir de encuestas verosímiles sobre los grupos demográficos.

\begin{example} \label{ex:redlining}
Supongamos un modelo utilizado para aprobar o denegar créditos bancarios. Por sesgos históricos, sabemos que uno de los atributos sensibles en la concesión de préstamos, es la raza. Procedemos entonces a eliminar esta información de todos los individuos en el modelo de predicción. El problema surge cuando notamos que el código postal, otro atributo presente en el vector de características, está altamente correlacionado con la raza y por tanto, las decisiones basadas en este serán racialmente discriminatorias. En consecuencia, el criterio de equidad por desconocimiento en este caso concreto, sería insuficiente. 

Esta práctica se conoce como \textit{redlining}, cuyo término fue acuñado en la década de 1960 debido a la práctica de negar bienes y servicios a las minorías mediante la \textit{redlining} de barrios específicos en un mapa (\cite{redlining2012}).
\end{example}


\section{Equidad individual}

La \textit{equidad individual} se basa en métricas de similitud sobre los atributos y  establece que individuos similares deben recibir predicciones similares independientemente del atributo sensible (\cite{detect2012}). Además, la equidad individual es más precisa que la equidad de grupo, ya que impone restricciones en el tratamiento de cada par de individuos.\\ 

\begin{definition}[Equidad individual] \label{def:indvfair}
Sea $g\colon \mathcal{X} \to \mathcal{Y}$ un clasificador arbitrario, $D\colon \mathcal{Y} \times \mathcal{Y} \to \R$ una medida de distancia sobre el espacio de clasificación resultante y $d\colon \mathcal{X} \times \mathcal{X} \to \R$ una medida de distancia sobre los individuos, se dice que $g$ cumple con la \textit{equidad individual} si, y solo si,

\begin{equation*} 
D(g(\nm{x}_i),g(\nm{x}_j))\leq d(\nm{x}_i,\nm{x}_j), \ \text{para todo} \ \nm{x}_i,\nm{x}_j \in \mathcal{X}.
\end{equation*}
\end{definition}\

\begin{figure}[h]
	\centering
	\includegraphics[width=10.5cm]{individual_fair.jpg}
	\caption{Ilustración de la noción de equidad individual.}
    \label{fig:individual_fairness}
\end{figure}

La definición de equidad individual, también es conocida como \textit{propiedad $(D,d)$-Lipschitz}. Cualquier clasificador que satisfaga esta propiedad también verificará la paridad demográfica con un cierto sesgo (véase Lema 3.1 en~\cite{detect2012}).

En la literatura de las ciencias sociales, esta formalización
equivale al individualismo igualitario, conocido por ser el principio formal de justicia. Esta noción responsabiliza a la \textit{métrica de la distancia} de garantizar la justicia del clasificador. Si la métrica de la distancia utiliza los atributos sensibles directa o indirectamente para calcular la distancia entre dos individuos, un clasificador que satisfaga la Definición \ref{def:indvfair} podría seguir causando impacto dispar. Por tanto, la equidad individual, no podría considerarse adecuada para dominios en los que no se dispone de una métrica de distancia fiable y no discriminatoria.

\begin{example}
Imaginemos tres candidatos a un puesto de trabajo, $A$, $B$ y $C$. $A$ tiene únicamente el título de graduado y un año de experiencia laboral relacionada. $B$ tiene un máster y un año de experiencia laboral relacionada. $C$ tiene un doble grado pero no tiene experiencia laboral. En principio, no podemos disponer del rendimiento de los tres individuos ya que no podemos contratar a todos. Entonces: ¿Está $A$ más cerca de $B$ que de $C$? Si es así, ¿por cuánto? La cosa se complica aún más cuando entran en juego los atributos sensibles. ¿Cómo deberíamos cuantificar la diferencia de pertenencia a un grupo en nuestra función métrica?

En este ejemplo podemos observar los problemas de la equidad individual comentados anteriormente y cómo dependen directamente de la construcción de la métrica de distancia entre individuos.
\end{example}

\section{Equidad de grupo}

La \textit{equidad de grupo} mide el impacto dispar entre grupos desfavorecidos y privilegiados, como podrían ser grupos de diferentes razas, edad o género.

Supongamos que en $\mathcal{X}$ se definen todas los posibles valores para las características \{raza, género, salario, trabajo, edad\}, denotaremos por $\mathcal{A}$ el conjunto de atributos sensibles sobre $\mathcal{X}$ considerando por ejemplo, en este caso, $$\mathcal{A}\text{=\{raza, género, edad\}}.$$

Consideremos un \textit{atributo multivaluado} $a = \{a_1,\dots,a_n\} \in \mathcal{A}$, por ejemplo, $$\text{raza=\{hispánico, caucásico, afroamericano, otro\}}.$$ Definimos un \textit{grupo} $G(a_i)$
como un conjunto de entidades que tienen en común un valor específico
del atributo $a$, por ejemplo raza=hispánico corresponde a todos los individuos de raza hispánica del
conjunto de datos.

Teniendo en cuenta todos los grupos definidos por el atributo $a$, las predicciones $\hat{y}$ y la etiqueta real $y$ para cada entidad de cada grupo, podemos hablar
ahora de las métricas de grupo. Las principales definiciones sobre grupos para
evaluar el sesgo y la equidad son las siguientes:

\begin{itemize}
    \item \textbf{Atributo -} $a = \{a_1,\dots,a_n\}$ es un atributo multivaluado, por ejemplo, $$\text{raza=\{hispánico, caucásico, afroamericano, otro\}}.$$
    \item \textbf{Grupo -} $G(a_i)$ es un grupo de todas las entidades que comparten el mismo valor de atributo $a=a_i$, por ejemplo, raza=hispánico.
    \item \textbf{Grupo de objetivo -} $G(a_o)$ es un grupo utilizado como objetivo del cálculo de las medidas de sesgo.
    \item \textbf{Grupo de referencia -} $G(a_r)$ es el grupo que se utiliza como referencia para calcular las medidas de sesgo. Suele fijarse siguiendo un criterio determinado.
    \item \textbf{Etiquetado positivo} - LP$_G$ número de entidades etiquetadas como positivas dentro de un grupo.
    \item \textbf{Etiquetado negativo -} LN$_G$ número de entidades etiquetadas como negativas dentro de un grupo.
\end{itemize}

\begin{notation}
De aquí en adelante, utilizaremos la siguiente notación:

\begin{itemize}
    \item $Y$ es una variable aleatoria binaria que representa la etiqueta real de un individuo de $\mathcal{X}.$
    \item $\hat{Y}$ es una variable aleatoria binaria que representa el resultado de la predicción de un clasificador $g\colon \mathcal{X} \to \mathcal{Y}$ para un individuo de $\mathcal{X}.$
    \item $A$ es una variable aleatoria binaria que representa si un individuo de $\mathcal{X}$ pertenece al grupo objetivo ($a_o$) o de referencia ($a_r$).
\end{itemize}
\end{notation}

\subsection*{Métricas de grupos de distribución}

Definiremos \textit{métricas de decisión} a nivel de grupo centradas en la distribución de los individuos entre los grupos del conjunto seleccionado para la intervención (máximo $k$) y por
tanto, no requieren del valor de la etiqueta $Y$. Definimos las métricas de distribución de los grupos como sigue:

\begin{itemize}
    \item \textbf{Positivos predichos -} PP$_G$ número de entidades dentro de un grupo donde la decisión es positiva, es decir, $\hat{Y} = 1$.
    \item \textbf{Negativos predichos -} PN$_G$ número de entidades dentro de un grupo cuya decisión es negativa, es decir, $\hat{Y} = -1$.
    \item \textbf{Total de predicciones positivas -} número total de entidades predichas como positivas en los grupos definidos por $a$, $$\text{K} = \sum_{i=1}^{n} \  \text{PP}_{G(a_i)}.$$
    \item \textbf{Prevalencia predicha -} fracción de entidades dentro de un grupo que se predijo como positiva, $$\text{PPrev}_G = \frac{\text{PP}_G}{\abs{G}} = P(\hat{Y}=1 \mid A=a_i).$$
    \item \textbf{Tasa de positivos predichos -} fracción de las entidades predichas como positivas que pertenecen a un determinado grupo, $$\text{PPR}_G = \frac{\text{PP}_G}{\text{K}} = P(A=a_i \mid \hat{Y}=1).$$
\end{itemize}

\subsection*{Métricas de grupo basadas en la etiqueta real}

\label{subsec:groupmetrics}

A continuación, discutiremos diferentes métricas que surgen dependiendo de la coincidencia o no entre los valores de predicción y, en este caso, de la etiqueta real $Y$. La mayoría de ellas, ya fueron presentadas en la Sección \ref{sec:evalaa}. Las métricas de grupo basadas en los errores y aciertos son las siguientes:

\begin{itemize} 
    \item \textbf{Falso positivo -} FP$_G$ es el número de entidades del grupo con $$\hat{Y} = 1 \ \wedge \ Y = -1.$$
    \item \textbf{Falso negativo -} FN$_G$ es el número de entidades del grupo con $$\hat{Y} = -1 \ \wedge \ Y = 1.$$
    \item \textbf{Verdadero positivo -} TP$_G$ es el número de entidades del grupo con $$\hat{Y} = 1 \ \wedge \ Y = 1.$$
    \item \textbf{Verdadero negativo -} TN$_G$ es el número de entidades del grupo con $$\hat{Y} = -1 \ \wedge \ Y = -1.$$
    \item \textbf{Prevalencia -}  fracción de entidades dentro de un grupo cuyo resultado verdadero fue positivo, $$\text{Prev}_G= \frac{\text{LP}_G}{\abs{G}}=P(Y=1 \mid A=a_i).$$
    \item \textbf{Tasa de falso descubrimiento -} fracción de falsos positivos de un grupo dentro de los positivos predichos del grupo, $$\text{FDR}_G = \frac{\text{FP}_G}{\text{PP}_G} = P(Y=-1 \mid A=a_i,\hat{Y}=1).$$
    \item \textbf{Tasa de falsa omisión -} fracción de falsos negativos de un grupo dentro de los negativos predichos del grupo, $$\text{FOR}_G = \frac{\text{FN}_G}{\text{PN}_G} = P(Y=1 \mid A=a_i, \hat{Y}=-1).$$
    \item \textbf{Tasa de falsos positivos -} fracción de falsos positivos de un grupo dentro de los negativos etiquetados del grupo, $$\text{FPR}_G = \frac{\text{FP}_G}{\text{LN}_G} = P(\hat{Y}=1 \mid A=a_i,Y=-1).$$
    \item \textbf{Tasa de falsos negativos -} fracción de falsos negativos de un grupo dentro de los positivos etiquetados del grupo, $$\text{FNR}_G = \frac{\text{FN}_G}{\text{LP}_G} = P(\hat{Y}=-1 \mid A=a_i,Y=1).$$
    \item \textbf{Valor negativo predictivo -} fracción de verdaderos negativos de un grupo dentro de los negativos predichos del grupo, $$\text{NPV}_G = \frac{\text{TN}_G}{\text{PN}_G} = P(Y=-1 \mid A=a_i,\hat{Y}=-1).$$
    \item \textbf{Valor positivo predictivo (Precision) -} fracción de verdaderos positivos de un grupo dentro de los positivos predichos del grupo, $$\text{PPV}_G = \frac{\text{TP}_G}{\text{PP}_G} = P(Y=1 \mid A=a_i,\hat{Y}=1).$$
    \item \textbf{Tasa de verdaderos positivos (Recall) -} fracción de verdaderos positivos de un grupo dentro de los positivos etiquetados del grupo, $$\text{TPR}_G = \frac{\text{TP}_G}{\text{LP}_G} = P(\hat{Y}=1 \mid A=a_i,Y=1).$$
    \item \textbf{Tasa de verdaderos negativos (Specificity) -} fracción de verdaderos negativos de un grupo dentro de los negativos etiquetados del grupo, $$\text{TNR}_G = \frac{\text{TN}_G}{\text{LN}_G} = P(\hat{Y}=-1 \mid A=a_i,Y=-1).$$
    \item \textbf{Exactitud (Accuracy) -} fracción de resultados verdaderos de un grupo dentro del total de casos examinados del grupo, $$\text{Accuracy}_G = \frac{\text{TP}_G+\text{TN}_G}{\text{LP}_G+\text{LN}_G} = P(\hat{Y}=Y \mid A=a_i).$$
    \item \textbf{Tasa global de clasificación errónea. -} fracción de resultados falsos de un grupo dentro del total de casos examinados del grupo, $$\text{OMR}_G = \frac{\text{FP}_G+\text{FN}_G}{\text{LP}_G+\text{LN}_G} = P(\hat{Y}\neq Y \mid A=a_i).$$
\end{itemize}\

En los apartados siguientes, formalizaremos algunas de las nociones populares de equidad de grupo que podemos encontrar en la literatura. Sea un atributo sensible multivaluado $a\in \mathcal{A}$, las métricas relativas a equidad de grupo se definen como una igualdad entre las probabilidades de un \textit{grupo objetivo} ($a_o$) en comparación con un \textit{grupo de referencia} ($a_r$).

El grupo de referencia se suele seleccionar en base a diferentes criterios. Por ejemplo, se podría utilizar el grupo mayoritario entre los grupos definidos por $A$, o el enfoque
tradicional de fijar un grupo históricamente favorecido, por ejemplo, en el caso de la raza, los individuos de raza caucásica.


\subsection{Paridad demográfica}

La \textit{paridad demográfica}, también conocida como \textit{paridad estadística} o \textit{independencia}, es uno de los criterios de equidad de grupo más conocidos. Esta noción de equidad afirma que la probabilidad de ser clasificado con el resultado positivo (o negativo) debe ser independiente de que el individuo pertenezca al grupo protegido, es decir, que los datos demográficos de los individuos clasificados positivamente son idénticos a los de la población en su conjunto (\cite{detect2012}).\\

\begin{definition}[Independencia en clasificación binaria]
Sean $C,A$ variables aleatorias. La \textit{independencia} entre $C$ y $A$ equivale a que se cumpla la siguiente restricción:
$$P(C=c \mid A=a_r)=P(C=c \mid A=a_o).$$
Lo denotaremos como $C \perp A.$
\end{definition}\

\begin{definition}[Paridad demográfica] \label{def:pardemo}
Sea $A \in \mathcal{A}$ un atributo sensible multivaluado y $g\colon \mathcal{X} \to \mathcal{Y}$ un clasificador arbitrario. Se dice que $g$ cumple con la \textit{paridad demográfica} si, y solo si, $\hat{Y} \perp A.$
\end{definition}

\subsection*{Relajaciones y aproximaciones}

Podemos \textit{relajar} el concepto de paridad demográfica suponiendo que $\hat{Y}=1$ y aproximando su definición con una acotación en valor absoluto de las probabilidades a partir una constante fijada $\epsilon \in (0,1]$, donde $\epsilon=\frac{p}{100}$ para que se satisfaga la ''regla $p$'' (\cite{constraints2017}). De esta manera, aproximamos este criterio de equidad como: 

\begin{equation*}
\abs{P(\hat{Y}=1 \mid A=a_r)-P(\hat{Y}=1 \mid A=a_o)} \leq \epsilon.
\end{equation*}

Tomando un $\epsilon \in (0,1]$ también podemos aproximar el concepto de paridad demográfica de la siguiente manera:
\begin{equation}\label{eq:p}
   1-\epsilon \leq \ddfrac{P(\hat{Y}=1 \mid A=a_o)}{P(\hat{Y}=1 \mid A=a_r)}. 
\end{equation}

En algunos trabajos se suele obviar la acotación por una constante y simplemente se define la paridad demográfica utilizando la Definición \ref{def:pardemo} y asumiendo que $\hat{Y}=1$, lo que sería equivalente a igualar las métricas PPrev entre los subgrupos.

\begin{equation*}
\text{PPrev}_{a_r}=\text{PPrev}_{a_o} \Rightarrow P(\hat{Y}=1 \mid A=a_r)=P(\hat{Y}=1 \mid A=a_o).
\end{equation*}

\begin{example}
Consideremos un sistema de detección de delitos y dos grupos de igual tamaño, $A$ y $B$. Suponemos que los miembros del grupo $B$ tienen el doble probabilidades reales de cometer un delito que los individuos del grupo $A$. Al igualar la probabilidad de un resultado positivo, el mismo número de predicciones positivas se distribuiría entre un grupo mucho mayor de delincuentes para $B$ que para $A$. Así, un delincuente del grupo $B$ tendría menos probabilidades de serlo que un delincuente del grupo $A$ ($\text{FNR}_A < \text{FNR}_B$). De hecho, para la misma precisión, la tasa de verdaderos positivos del grupo $B$ sería la mitad de la del grupo $A$, $\frac{1}{2} \text{TPR}_A = \text{TPR}_B$, cumpliendo la paridad demográfica.
\end{example}

\subsection{Probabilidades igualadas}

Uno de los problemas de la paridad demográfica es que ignora una posible correlación entre $Y$ y $A$. El criterio de las \textit{probabilidades igualadas}, también conocido como \textit{ratio de paridad positiva} o \textit{separación}, tiene en cuenta la etiqueta real de cada grupo y su condicionamiento al resto de variables. Además, proporciona un incentivo para reducir los errores de manera uniforme en todos los grupos sin descartar el clasificador perfecto (que obtenga $\hat{Y}=Y$) a diferencia de la paridad estadística.\\

\begin{definition}[Independencia condicional en clasificación binaria]
Sean $C,Y,A$ variables aleatorias. La \textit{independencia condicional} entre $C$ y $A$ dado $Y$ equivale a que se cumpla la siguiente restricción:
$$P(C=c \mid A=a_r, Y=y)=P(C=c \mid A=a_o,Y=y).$$
Lo denotaremos como $C \perp A \mid Y.$
\end{definition}\

El criterio de las probabilidades igualadas establece que $\hat{Y}$ debe ser condicionalmente independiente de $A$ dado $Y$, permitiendo que el clasificador dependa de $A$ a través de la variable objetivo (\cite{eodd2016}). Para utilizar este criterio, es necesario conocer las etiquetas reales de cada individuo, por lo que esta medida restringe su uso para determinadas tareas en las que no conozcamos previamente el resultado de la acción sobre el individuo.\\

\begin{definition}[Probabilidades igualadas] \label{def:probigual}
Sea $A \in \mathcal{A}$ un atributo sensible multivaluado y $g\colon \mathcal{X} \to \mathcal{Y}$ un clasificador arbitrario. Se dice que $g$ cumple con el criterio de las \textit{probabilidades igualadas} si, y solo si, $\hat{Y} \perp A \mid Y$.
\end{definition}

\subsection*{Relajaciones y aproximaciones}

El concepto previo depende de varias variables por lo que normalmente en la práctica se tiende a relajar el criterio fijando algunos valores en la definición. A partir de estas relajaciones surgen otros criterios de equidad que también son ampliamente utilizados y conocidos en la literatura.

La relajación más común surge al suponer que $\hat{Y}=1$, en este caso, el criterio de las probabilidades igualadas equivale a igualar las métricas FPR y TPR entre los subgrupos. Esta aproximación beneficia al individuo, y equilibra la probabilidad de tener un resultado beneficioso, en todos los subgrupos de los individuos etiquetados tanto positiva como negativamente.
\begin{equation*}
\begin{split}
\text{FPR}_{a_r}=\text{FPR}_{a_o} &\Rightarrow P(\hat{Y}=1 \mid A=a_r,Y=-1)=P(\hat{Y}=1 \mid A=a_o, Y=-1).\\
\text{TPR}_{a_r}=\text{TPR}_{a_o} &\Rightarrow P(\hat{Y}=1 \mid A=a_r, Y=1)=P(\hat{Y}=1 \mid A=a_o, Y=1).
\end{split}
\end{equation*}

\begin{comment}
Otra de las relajaciones consiste en suponer que $\hat{Y}=Y$, en este caso, la medida de las probabilidades igualadas equivale a igualar la métrica de exactitud entre los subgrupos.

\begin{equation*}
\text{Accuracy}_{a_r}=\text{Accuracy}_{a_o} \Rightarrow P(\hat{Y}= Y \mid A=a_r)=P(\hat{Y}= Y \mid A=a_o).
\end{equation*}

A partir de esta idea surge el concepto de igualdad de exactitud que dicta que la exactitud de la predicción debe ser igual en todos los grupos protegidos (\cite{igualexac2017}).\\

\begin{definition}[Igualdad de exactitud]
Sea $A \in \mathcal{A}$ un atributo sensible multivaluado y $g\colon \mathcal{X} \to \mathcal{Y}$ un clasificador arbitrario. Se dice que $g$ cumple con la \textit{igualdad de exactitud} si, y solo si, $$P(\hat{Y}=Y \mid A=a_r)=P(\hat{Y}=Y \mid A=a_o).$$
\end{definition}\

La igualdad de exactitud asume el mismo valor en la identificación de verdaderos positivos y verdaderos negativos, lo que no es cierto en determinados dominios de aplicación (por ejemplo, cuando se identifican enfermedades raras), por lo que no es aplicable en todos los ámbitos.
\end{comment}

Definiremos el concepto de igualdad de oportunidades como la igualdad de las tasas de verdaderos positivos entre los subgrupos (\cite{eodd2016}). En algunos trabajos, también se define este criterio de forma equivalente para las tasas de verdaderos negativos.
\begin{equation*}
\text{TPR}_{a_r}=\text{TPR}_{a_o} \Rightarrow P(\hat{Y}= 1 \mid A=a_r, Y=1)=P(\hat{Y}= 1 \mid A=a_o, Y=1).
\end{equation*}\

\begin{definition}[Igualdad de oportunidades]
Sea $A \in \mathcal{A}$ un atributo sensible multivaluado y $g\colon \mathcal{X} \to \mathcal{Y}$ un clasificador arbitrario. Se dice que $g$ cumple con la \textit{igualdad de oportunidades} si, y solo si, $$P(\hat{Y}=1 \mid A=a_r,Y=1)=P(\hat{Y}=1 \mid A=a_o, Y=1).$$
\end{definition}\

La igualdad de oportunidades es naturalmente más débil que la Definición \ref{def:probigual}, ya que asumimos los valores de $\hat{Y}=1$ e $Y=1$. Este concepto, iguala la probabilidad de que los individuos etiquetados positivamente
sean correctamente clasificados con el resultado positivo (beneficioso). Por ejemplo, dos individuos, un
hombre y una mujer, que están cualificados para un trabajo ($Y=1$), deberían tener la misma probabilidad de conseguir el trabajo ($\hat{Y}=1$).

\begin{example}
Consideremos un sistema de contratación y dos grupos de igual tamaño, $A$ y $B$. Imaginemos que en el grupo $A$ de los 100 aspirantes al cargo, 58 están cualificados, mientras que el grupo $B$ solo 2 de ellos son aptos para el cargo. Si la empresa decide aceptar a 30 solicitantes y satisfacer la igualdad de oportunidades, se concederán 29 ofertas al grupo $A$ mientras que solo se concederá 1 oferta al grupo $B$. Si el trabajo es bien remunerado, el grupo $A$ mejorará sus condiciones de vida y a la larga, podrá permitir una mejor educación para sus hijos, y en consecuencia una mejor cualificación de los mismos en el futuro. 

En este ejemplo podemos observar que la igualdad de oportunidades no ayuda a cerrar la brecha entre los dos grupos, es más la brecha entre el grupo $A$ y el grupo $B$, tenderá a ampliarse con el tiempo. 
\end{example}

\subsection{Tasa de paridad predictiva}

\label{subsec:suficiencia}

La \textit{tasa de paridad predictiva}, también denominada \textit{suficiencia} surge de una motivación equivalente a la del criterio de las probabilidades igualadas. El concepto se define de igual manera haciendo uso de la independencia condicional, pero cambiando los papeles de $\hat{Y}$ e $Y$. \\

\begin{definition}[Tasa de paridad predictiva]
Sea $A \in \mathcal{A}$ un atributo sensible multivaluado y $g\colon \mathcal{X} \to \mathcal{Y}$ un clasificador arbitrario. Se dice que $g$ cumple con la \textit{tasa de paridad predictiva} si, y solo si, $Y \perp A \mid \hat{Y}$.
\end{definition}

\subsection*{Relajaciones y aproximaciones}

La relajación más común surge al suponer que $\hat{Y}=Y$, en este caso, el criterio de las probabilidades igualadas equivale a igualar las métricas PPV y NPV entre los subgrupos. 
\begin{equation*}
\begin{split}
\text{PPV}_{a_r}=\text{PPV}_{a_o} &\Rightarrow P(Y=1 \mid A=a_r, \hat{Y}=1)=P(Y=1 \mid A=a_o, \hat{Y}=1).\\
\text{NPV}_{a_r}=\text{NPV}_{a_o} &\Rightarrow P(Y=-1 \mid A=a_r,\hat{Y}=-1)=P(Y=-1 \mid A=a_o,\hat{Y}=-1).
\end{split}
\end{equation*}

Las limitaciones de este concepto de equidad también son similares a las de las probabilidades igualadas, pudiendo acrecentar las diferencias entre los grupos privilegiado y desfavorecido.

\subsection{Medidas basadas en la puntuación}

A diferencia de las definiciones anteriores, que se basan en los índices de clasificación binaria, algunas nociones de equidad se basan en la \textit{puntuación} de la probabilidad predicha $S$ y la etiqueta real $Y$ (\cite{catalogue2018}).

El \textit{balance para la clase positiva} (o \textit{negativa}) se cumple cuando la puntuación esperada para un individuo clasificado positivamente (o negativamente) es igual en todos los grupos.\\

\begin{definition}[Balance para la clase positiva]
Sea $A \in \mathcal{A}$ un atributo sensible multivaluado y $S\in [0,1]$ la puntuación asignada por el clasificador arbitrario $g\colon \mathcal{X} \to \mathcal{Y}$. Se dice que $g$ cumple con el \textit{balance para la clase positiva} si, y solo si, $$\mathbb{E}[S \mid A=a_r,Y=1]=\mathbb{E}[S \mid A=a_o,Y=1].$$
\end{definition}\

\begin{definition}[Balance para la clase negativa]
Sea $A \in \mathcal{A}$ un atributo sensible multivaluado y $S\in [0,1]$ la puntuación asignada por el clasificador arbitrario $g\colon \mathcal{X} \to \mathcal{Y}$. Se dice que $g$ cumple con el \textit{balance para la clase negativa} si, y solo si, $$\mathbb{E}[S \mid A=a_r, Y=-1]=\mathbb{E}[S \mid A=a_o, Y=-1].$$
\end{definition}\

Sin embargo, hay que tener en cuenta que
en los problemas del mundo real es casi imposible cumplir el equilibrio simultáneo para la clase negativa y el equilibrio para la clase positiva simultáneamente.

\clearpage

\subsection{Igualdad de las métricas de predicción}

Como hemos podido observar, en general, la mayoría de los criterios de equidad definidos surgen como una igualación de las métricas de grupo presentadas en el Apartado \ref{subsec:groupmetrics}. De esta forma podemos definir, un nuevo criterio para cada métrica de la siguiente forma.\\

\begin{definition}[Paridad métrica]\label{def:parmetr}
Sea $A \in \mathcal{A}$ un atributo sensible multivaluado y $g\colon \mathcal{X} \to \mathcal{Y}$ un clasificador arbitrario. Se dice que $g$ cumple con la \textit{paridad métrica} si, y solo si, $$\text{Métrica}_{a_r}=\text{Métrica}_{a_o}.$$
Donde Métrica podrá ser cualquier métrica de grupo definida previamente. 
\end{definition}\

Aunque esta definición nos permite crear una gran variedad de criterios de equidad, la formulación a partir de una igualdad sigue siendo difícil a la hora de aplicarla en la práctica. 

\subsection{Impacto desigual}

Para facilitar la implementación práctica de las medidas anteriores, aparece el término \textit{impacto desigual} (no confundir con el término impacto dispar definido en el Cuadro \ref{tab:table1}) como una aproximación más de la paridad demográfica. Esta noción está directamente relacionada con la ''regla $p$'', antes mencionada, y que podemos encontrar en la literatura jurídica (\cite{prule2018}), según la cual una decisión es discriminatoria si el coeficiente del impacto desigual es inferior a un valor $p$.\\

\begin{definition}(Impacto desigual) \label{def:mdi}
Sea $A \in \mathcal{A}$ un atributo sensible multivaluado, $g\colon \mathcal{X} \to \mathcal{Y}$ un clasificador arbitrario y $p\in (0,1]$. Se dice que $g$ satisface el \textit{impacto desigual} si, y solo si,
\begin{equation} \label{eq:1p}
    \ddfrac{P(\hat{Y}=1 \mid A=a_o)}{P(\hat{Y}=1 \mid A=a_r)}\leq p.
\end{equation}
\end{definition}\

En el mundo real, si un clasificador aplicado a una tarea en una empresa satisface el impacto desigual, debe justificarse que su existencia es esencial para el funcionamiento seguro y eficiente del negocio, y no existen procedimientos alternativos que sean sustancialmente igual de válidos y tengan un impacto menos adverso. La Comisión para la Igualdad de Oportunidades en el Empleo (EEOC) de Estados Unidos adopta la regla del $80\%$ ($p = 0.80$) para considerar si una decisión tiene impacto desigual (\cite{adverse2009}).

\subsection*{Construcción de otras medidas en la práctica}

Combinando las acotaciones de las Ecuaciones (\ref{eq:1p}) y (\ref{eq:p}), obtenemos un criterio de equidad popularmente utilizado, en concreto por el software de Aequitas (\cite{aequitas2019}). Además esta formalización también puede extenderse a cualquier métrica de grupo de las explicadas en el Apartado \ref{subsec:groupmetrics}.

Podemos, por ejemplo, definir la tasa de falsas omisiones (FOR) como:

\begin{equation*} 
\text{Métrica FOR}_G =\frac{\text{FOR}{a_o}}{\text{FOR}{a_r}}=\frac{P(Y=1 \mid A=a_o, \hat{Y}=-1)}{P(Y=1 \mid A=a_r, \hat{Y}=-1)}.
\end{equation*}

Aequitas utiliza la tasa de positivos predichos (PPR) para aproximar el concepto de paridad demográfica. Además, usando la prevalencia predicha (PPrev), observamos que la métrica generada es equivalente a la Definición \ref{def:mdi} y por tanto al concepto de paridad de impacto.

\begin{equation*} 
\text{Métrica PPrev}_G=\frac{\text{PPrev}{a_o}}{\text{PPrev}{a_r}}=\frac{P(\hat{Y}=1 \mid A=a_o)}{P(\hat{Y}=1 \mid A=a_r)}.
\end{equation*}

La formulación aportada por Aequitas se basa en calcular la fracción de la métrica de grupo elegida y una vez calculada, comprobar si se encuentra dentro de un rango definido. Tomando $p=\frac{1}{1-\epsilon}$, obtenemos la acotación que define el rango. La pertenencia o no al rango es una aproximación a la igualación de las métricas en la Definición \ref{def:parmetr}.

\begin{equation}
1-\epsilon \leq \text{Métrica de disparidad}_{G} \leq \frac{1}{1-\epsilon}.
\end{equation}

Usamos $\epsilon \in
(0,1]$ para controlar el rango de valores de disparidad que pueden considerarse justos. Para aplicar la regla del 80\%, simplemente bastaría con tomar $1-\epsilon=0,8$. Diremos que un clasificador será tan justo como lo permita el valor máximo del sesgo entre los grupos definidos por $A$. 

Usaremos unas métricas u otras en función del impacto y el objetivo que quiera intervenir el usuario. Si las intervenciones pueden perjudicar a los individuos (punitivas), entonces querremos minimizar los falsos positivos (centrándonos en la Tasa de Falsos Descubrimientos (FDR) o la Tasa de Falsos Positivos (FPR)). Si por otro lado, tienen como objetivo beneficiar a los individuos (asistenciales), deberíamos preocuparnos más por los falsos negativos (centrándonos en la Tasa de Falsa Omisión (FOR) o la Tasa de Falsos Negativos (FNR)).

\chapter{Algoritmos de mitigación de sesgo}

En este capítulo discutiremos los diferentes algoritmos existentes para la mitigación del sesgo, daremos algunos ejemplos específicos de los aportados en la bibliografía y comentaremos sus características más relevantes.

\section{Modelos de aprendizaje justos}

En la literatura, podemos encontrar una gran variedad de métodos y algoritmos que nos pueden ayudar a mejorar la equidad en un modelo de aprendizaje. Los enfoques de \textit{mitigación del sesgo} pueden subdividirse en tres categorías: algoritmos de preprocesamiento, que intentan aprender representaciones justas de los datos; algoritmos de optimización durante el entrenamiento, que ajustan el proceso de aprendizaje para cumplir los criterios de justicia; y algoritmos de posprocesamiento, que adaptan las predicciones del modelo en función de sus resultados. Estas categorías no tienen por qué ser mutuamente excluyentes y a veces pueden tener varios métodos de actuación sobre los datos.

A lo largo de este capítulo, presentaremos las características comunes entre los algoritmos de cada uno de los tres grupos presentados y discutiremos algunos ejemplos que podemos encontrar en la literatura, profundizando en algún caso específico de cada tipo. Además, contextualizaremos en su categoría correspondiente el algoritmo de optimización de equidad contrafactual presentado por \cite{counterfactual2018} sobre el que basaremos la parte práctica de nuestro trabajo.

\subsection{Selección de los datos del modelo}

En la Sección \ref{sec:queaprendizaje}, observamos que para construir un modelo de aprendizaje es necesario tener un conjunto de datos $\mathcal{D}=\{(\nm{x}_1,y_1),\dots,(\nm{x}_n,y_n)\}$ a partir del cual poder entrenar el modelo y extraer la información que se quiere aprender. Normalmente para entrenar los modelos, no utilizaremos el conjunto de datos al completo, sino que tomaremos un conjunto de entrenamiento $X$, que contiene $m$  individuos extraídos aleatoriamente del conjunto de datos total, donde $m < n$. Cada individuo $\nm{x} \in X$ es un vector de longitud $d$ donde cada componente del vector describe una característica de la persona. Además, cada individuo $\nm{x}$ tiene asociado un atributo sensible $a\in \{0,1\}$, donde el valor $0$ denota la pertenencia al grupo de referencia $(a_r)$ y $1$ al grupo objetivo $(a_o)$. Denotaremos por $Y$ al conjunto con las etiquetas reales de los individuos que están en $X$. De esta forma, tenemos que $X \times Y \subset \mathcal{D}$. 

En la práctica, se suele utilizar el conjunto de los individuos restantes contenidos en $\mathcal{D}$ como conjunto de prueba del modelo con el objetivo de poder comprobar su rendimiento sobre un conjunto de datos ''nuevo'' con el que no ha sido entrenado. 

\subsection{Equilibrio entre equidad y métricas de evaluación}

Algunas medidas de evaluación como la precisión o exactitud dependen directamente del conjunto de datos, la definición de equidad utilizada y los algoritmos empleados. La equidad en la práctica, perjudica a métricas como la exactitud. Si queremos mitigar el sesgo entre grupos, debemos hacer una compensación entre la equidad y la exactitud, sacrificando en parte esta última como se puede observar en la Figura \ref{fig:tradeoff}.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=7.6cm]{accuracyeq.png}
	\caption{Exactitud vs. Independencia en un problema de clasificación (\cite{constraints2017}).}
    \label{fig:tradeoff}
\end{figure}

\section{Algoritmos de preprocesamiento}

Los \textit{algoritmos de preprocesamiento} buscan mejorar la equidad antes de entrenar el modelo, modificando los datos de entrenamiento de forma que no presenten sesgos antes de ser procesados. El propósito de estos algoritmos es aprender una nueva representación $Z$ que elimine la información correlacionada con el atributo sensible $A$ y preserve, en la medida de lo posible, la información del conjunto de individuos $X$ sin necesidad de conocer el valor de sus etiquetas $Y$. La tarea posterior (por ejemplo, regresión o clasificación) desempeñada por $g$, será independiente del método usado y podrá producir resultados que preserven diversos criterios de equidad.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=12.1cm]{preprocesado.png}
	\caption{Etapas del proceso de preprocesamiento.}
    \label{fig:preprocessing}
\end{figure}

\subsection{Ejemplo: Aprendizaje de la representación justa}

\cite{fairrepresentation2013} presenta  este problema como el aprendizaje de un función que lleva muestras individuales a representaciones intermedias. Este trabajo tiene dos objetivos: minimizar la pérdida de información (los datos originales deberían preservar la misma cantidad de información) y maximizar la equidad (la pertenencia a un grupo protegido no debería afectar negativamente a los individuos del mismo). 

Este ejemplo, podría confundirse como una aplicación de equidad por desconocimiento, pero no es así, ya que la pertenencia a grupos protegidos no es simplemente ignorada, sino que se trata activamente (junto con la información redundante de estos atributos). El modelo propuesto, tiene como objetivo mantener la equidad de grupo e individual, al mismo tiempo que maximiza la exactitud.

\begin{notation}
Introducimos la siguiente notación que utilizaremos en este ejemplo:

\begin{itemize}
    %\item $S$ es una variable aleatoria binaria que representa si un individuo determinado es o no miembro del conjunto protegido. 
    %\item $Y$ es una variable aleatoria binaria que representa la decisión de clasificación de un individuo y $f\colon X \to Y$ es la función de clasificiación.
    \item $X^+=\{\nm{x}_n \in X \ : \ A=1\}\subset X$ es el conjunto de datos de entrenamiento cuyos miembros pertenecen al grupo protegido, $X^-=\{\nm{x}_n \in X \ : \ A=0\}$ denota al conjunto cuyos miembros pertenecen al grupo no protegido.
    \item $Z$ es una variable aleatoria multivariante, donde cada uno de sus $r$ valores representa un ''prototipo''. Asociado a cada prototipo existe un vector $\nm{v}_k$ en el mismo espacio que los individuos $\nm{x}_n$.
\end{itemize}
\end{notation}

La idea es representar cada individuo $\nm{x}_n \in X$ como una combinación lineal ponderada de $r$ prototipos para satisfacer la paridad demográfica, minimizando la pérdida de información original y maximizando la exactitud en la medida de lo posible.

Se define la probabilidad \textit{softmax} de que un elemento sea un prototipo concreto como:
\begin{equation*}
M_{n,k}:= P(Z=k \mid \nm{x}_n)=\ddfrac{\text{exp}(-d(\nm{x}_n,\nm{v}_k))}{\sum_{j=1}^r \text{exp}(-d(\nm{x}_n,\nm{v}_j))} \ \text{para todo} \ n,k.
\end{equation*}

donde $d$ es una función de medida de distancia (por ejemplo, la distancia $\ell_2$). Los autores definen la regularización $\ell_2$ como una función de distancia ponderada que viene dada por
\begin{equation*}
d(\nm{x}_n,\nm{v}_k,\upalpha)=\sum_{i=1}^d \upalpha_i (x_{ni}-v_{ki})^2.
\end{equation*}

Se define $\hat{y}_n$ como la predicción para $y_n$ calculada partiendo de la marginalización sobre el valor de $Y$ para la predicción de cada prototipo
\begin{equation*}
\hat{y}_n=\sum_{k=1}^r M_{n,k} w_k.
\end{equation*}

El modelo de aprendizaje, minimiza la siguiente función de pérdida:
\begin{equation*}
L=A_zL_z+A_xL_x+A_yL_y.   
\end{equation*}

Donde $L_z$ regulariza la paridad demográfica, $L_x$ es el error de reconstrucción de la distribución y $L_y$ cuantifica la pérdida de predicción. Los factores $A_z, A_x, A_y$ se definen como hiperparámetros para equilibrar estas pérdidas.

\begin{equation*}
L_z=\sum_{k=1}^r \abs{M_k^+-M_k^-}.
\end{equation*}
expresando $\displaystyle M_k^+=\frac{1}{\abs{X^+}} \sum_{n\in X^+} M_{n,k}$ y $M_k^-$ se formula de forma similar a partir de $X^-$.

\begin{equation*}
L_x=\sum_{n=1}^m (\nm{x}_n-\hat{\nm{x}}_n)^2.
\end{equation*}
donde $\hat{\nm{x}}_n$ son los nuevos valores de $\nm{x}_n$ para $Z$:
\begin{equation*}
\hat{\nm{x}}_n=\sum_{k=1}^r M_{n,k}\nm{v}_k.
\end{equation*}

\begin{equation*}
L_y=\sum_{n=1}^m -y_n \log \hat{y}_n -(1-y_n) \log (1-\hat{y}_n).
\end{equation*}

En la fase de entrenamiento, los valores de $\nm{v}, \nm{w}, \upalpha$ se optimizan conjuntamente a través del método L-BFGS (\cite{lbfgs1997}) para minimizar la función objetivo $L$. Los valores de $A_x,A_y,A_z$ se seleccionan a través de la afinación de los hiperparámetros, optando por los que producen mejores resultados (búsqueda en malla). Tenemos que tener en cuenta que la función objetivo no es convexa y, por tanto, no garantiza la optimización.

\subsection*{Ventajas e inconvenientes}

Algunas de las principales ventajas de los algoritmos de preprocesamiento ya han sido mencionadas previamente. En general, estos métodos, son muy útiles cuando el clasificador que utilizaremos para la fase de entrenamiento es un modelo de caja negra y no conocemos su actuación sobre los datos. En estos casos, al devolvernos una nueva distribución que no contiene correlación con los atributos sensibles, los datos podrán ser usados de forma independiente para cualquier tarea posterior sin necesidad de preocuparnos por la existencia de sesgo entre grupos.

Por otro lado, solo podemos utilizar los métodos de preprocesamiento para optimizar los criterios de equidad que no requieran información sobre el valor de las etiquetas $Y$ (por ejemplo, la paridad demográfica o la equidad individual). Como desconocemos el uso que se le va a dar a los nuevos datos, en algunos casos, podría no garantizarse la equidad en el modelo final aprendido. Además, este grupo de algoritmos suele ser inferior respecto los otros dos en términos de rendimiento entre exactitud y equidad.


\subsection*{Otros ejemplos en la literatura}

El concepto de preprocesamiento optimizado es introducido por \cite{optimizeddata2017}. En este se plantea la reducción de la discriminación como una tarea de optimización convexa con el objetivo de minimizar la pérdida de exactitud (preservando la utilidad) mientras se limita por ciertas medidas de equidad de grupo e individual.

\cite{repairing2019} ofrece métodos que usan distribuciones contrafactuales para resolver el trato dispar de un clasificador de caja negra en una población de despliegue sin la necesidad de reentrenar el modelo. El método propuesto se basa en la construcción de una nueva distribución para los individuos del grupo objetivo de manera que mejoren sus resultados en promedio.

\section{Algoritmos de optimización durante el entrenamiento}

Los \textit{algoritmos de optimización} alteran el entrenamiento del propio modelo. En este contexto, la mitigación del sesgo se plantea como un aprendizaje al que se le añade una restricción o un término de regularización al objetivo de optimización existente. Es decir, un modelo aprende a optimizar una función de pérdida en los datos de entrenamiento, sujeta a restricciones de equidad (por ejemplo, la distancia máxima a la paridad demográfica).

Otro enfoque diferente, sería el de la optimización de métricas de rendimiento complejas que incluyan alguna noción de equidad. Por ejemplo, introduciendo una penalización relacionada con la equidad en la función objetivo.

\subsection{Ejemplo: Aprendizaje en clasificación sin impacto dispar.}

Uno de los enfoques más populares es el de la optimización con restricciones, donde el objetivo es encontrar un conjunto de parámetros, $\btheta \in \Theta$ que minimicen una función objetivo $l_0(\btheta)$, sujeta a $m$ restricciones funcionales, $l_i(\btheta) \ \text{para todo} \ i \in \{1,\dots,m\}$: $$\btheta^\ast = \underset{\btheta \in \Theta}{\text{arg min}}  \ l_0(\btheta) \ \ \text{donde} \ l_i(\btheta) \leq 0 \ \ \text{para todo} \ i \in \{1,\dots,m\}.$$

\cite{constraints2017} enmarca la tarea de clasificación justa imponiendo restricciones lineales a la covarianza entre los atributos sensibles y las predicciones (o la distancia con signo entre el vector de características del individuo y el límite de decisión del clasificador). Este método es adecuado para múltiples atributos sensibles y para cualquier clasificador basado en contornos convexos (por ejemplo, SVM o regresión logística). Además, se propone otra formulación similar, destinada a satisfacer necesidades del mundo real, maximizando la equidad sujeta a restricciones de exactitud. 

En el trabajo anterior, se asume que el conjunto de datos original presenta un sesgo histórico, por lo que en \cite{disparate2017} se amplía este enfoque a los casos en los que tenemos acceso a la verdad histórica no sesgada en la fase de entrenamiento y podemos saber si una decisión histórica fue correcta o incorrecta.

\begin{notation}
Introducimos la siguiente notación para el ejemplo propuesto:

\begin{itemize}
    \item $\mathcal{D}'$ es el conjunto de datos de entrenamiento que se define como $$\mathcal{D}'=\{(\nm{x}_i,y_i) \in X \times Y \ : \ i=1,\dots,m\}.$$
    \item $\btheta$ son los parámetros que debemos aprender.
    \item $L(\btheta)$ es la función de pérdida convexa original.
    \item $d_{\btheta}(\nm{x})$ es la función de distancia con signo del vector de características $\nm{x}$ al límite de decisión del clasificador.
    \item $f_{\btheta}(\nm{x})$ es la función de clasificación, definida por $$f_{\btheta}(\nm{x})=\left \{
    \begin{array}{l l}
    1, & \mbox{si } d_{\btheta}(\nm{x}) \geq 0, \\
    -1, & \mbox{en otro caso}.
    \end{array}
    \right.$$
\end{itemize}
\end{notation}

Podemos añadir como restricciones al problema de optimización original, la \textit{paridad OMR} (definida como una relajación para $\hat{Y}\neq Y$ del criterio de probabilidades igualadas) o la \textit{paridad FNR}. Aunque, para el ejemplo, utilizaremos la \textit{paridad FPR} construida a partir de la Definición \ref{def:parmetr} y que se define como: $$P(\hat{Y}=1 \mid A=0, Y=-1)=P(\hat{Y}=1 \mid A=1, Y=-1).$$

\clearpage

Cabe señalar que la paridad FNR y FPR implican la paridad TPR y TNR respectivamente, y por tanto, la igualdad de oportunidades. En este ejemplo, usaremos como restricción la paridad FPR a partir de la cual surge la siguiente formulación de optimización:
\begin{equation}\label{eq:restricciones}
\begin{split}
\text{minimizar:} &\quad L(\btheta)\\
\text{sujeto a:} &\quad P(\hat{Y}=1 \mid A=0, Y=-1)-P(\hat{Y}=1 \mid A=1, Y=-1) \leq \epsilon,\\
&\quad  P(\hat{Y}=1 \mid A=0, Y=-1)-P(\hat{Y}=1 \mid A=1, Y=-1) \geq -\epsilon.
\end{split}
\end{equation}

La complejidad de las restricciones, hacen que el problema que plantea minimizar la función $L(\btheta)$ (a priori no convexa) sea intratable a nivel computacional al no poder utilizar los algoritmos tradicionales como el de descenso de gradiente estocástico u otros resolutores para encontrar la solución óptima al problema que se plantea. 

Para subsanar esta cuestión, se presentan algunas relajaciones de las restricciones que utilizan la covarianza entre los atributos sensibles de los individuos y $d_{\btheta}(\nm{x})$ para detectar la relación entre el atributo sensible y las predicciones (condicionales) a nivel de grupo:
\begin{equation*}
\begin{split}
\text{Cov}(a,g_{\btheta}(y,\nm{x}))&=\mathbb{E}[(a-\bar{a})(g_{\btheta}(y,\nm{x})-\bar{g}_{\btheta}(y,\nm{x}))]\\
&\approx \frac{1}{m} \sum_{(\nm{x},y,a)\in \mathcal{D}'}(a-\bar{a})g_{\btheta}(y,\nm{x}),
\end{split}
\end{equation*}

donde el término $\mathbb{E}[(z-\bar{z})]\bar{g}_{\btheta}(\nm{x})$ se anula, ya que $\mathbb{E}[(z-\bar{z})]=0$ y la función $g_{\btheta}(y,\nm{x})$ se puede definir como:
\begin{equation*}
\begin{split}
g_{\btheta}(y,\nm{x})&=\min (0,\frac{1-y}{2} y d_{\btheta} (\nm{x})).
\end{split}
\end{equation*}

Tras la relajación, podemos reescribir (\ref{eq:restricciones}) como:
\begin{equation}\label{eq:restricciones2}
\begin{split}
\text{minimizar:} &\quad L(\btheta)\\
\text{sujeto a:} &\quad \frac{1}{m} \sum_{(\nm{x},y,a)\in \mathcal D'} (a-\bar{a})g_{\btheta}(y,\nm{x}) \leq c,\\
&\quad \frac{1}{m} \sum_{(\nm{x},y,a)\in \mathcal D'} (a-\bar{a})g_{\btheta}(y,\nm{x})  \geq -c,
\end{split}
\end{equation}

donde el umbral de covarianza $c \in \R^+$ controla el grado de desempeño del criterio de igualdad de oportunidades.

Esta formulación sigue siendo no convexa, por lo que a continuación convertiremos estas restricciones en un \textit{programa convexo-cóncavo disciplinado} (DCCP), que puede resolverse de manera eficiente aprovechando los recientes avances en la programación convexa-cóncava (\cite{convex2016}).

En primer lugar, consideramos la restricción descrita en (\ref{eq:restricciones2}), es decir:
\begin{equation*}
\sum_{(\nm{x},y,a)\in \mathcal{D}'} (a-\bar{a})g_{\btheta} (y,\nm{x}) \sim c,
\end{equation*}

donde $\sim$, podría denotar '$\leq$' o '$\geq$'. Además, dejamos de lado la constante $\frac{1}{n}$ para simplificar. Como $a \in \{0,1\}$, dividimos la suma en la expresión anterior en dos términos:
\begin{equation} \label{eq:restricciones3}
\sum_{(\nm{x},y)\in \mathcal{D}_0'} (0-\bar{a})g_{\btheta} (y,\nm{x})+\sum_{(\nm{x},y)\in \mathcal{D}_1'} (1-\bar{a})g_{\btheta} (y,\nm{x}) \sim c,
\end{equation}

donde $\mathcal{D}_0'$ y $\mathcal{D}_1'$ son subconjuntos del conjunto de datos $\mathcal{D'}$ que toman valores $a=0$ y $a=1$, respectivamente. Definimos $m_0=\abs{\mathcal{D}_0'}$ y $m_1=\abs{\mathcal{D}_1'}$, entonces $\bar{z}=\ddfrac{m_1}{m}$ y podemos reescribir (\ref{eq:restricciones3}) como:
\begin{equation*}
-\ddfrac{m_1}{m} \sum_{(\nm{x},y)\in \mathcal{D}_0'} g_{\btheta} (y,\bm{x})+\ddfrac{m_0}{m} \sum_{(\nm{x},y)\in \mathcal{D}_1'} g_{\btheta} (y,\nm{x}) \sim c,
\end{equation*}

que, dado que $g_{\btheta}$ es convexa en en $\btheta$ (por suposición), resulta en una función convexa-cóncava. 

Por lo tanto, podemos reescribir el problema definido por (\ref{eq:restricciones2}) como:
\begin{equation*}
\begin{split}
\text{minimizar:} &\quad L(\btheta)\\
\text{sujeto a:} &\quad
-\frac{m_1}{m} \sum_{(\nm{x},y)\in \mathcal D_0'} g_{\btheta}(y,\nm{x}) +\frac{m_0}{m} \sum_{(\nm{x},y)\in \mathcal D_1'} g_{\btheta}(y,\bm{x}) \leq c,\\
&\quad -\frac{m_1}{m} \sum_{(\nm{x},y)\in \mathcal D_0'} g_{\btheta}(y,\nm{x})+\frac{m_0}{m} \sum_{(\nm{x},y)\in \mathcal D_1'} g_{\btheta}(y,\bm{x}) \geq -c,
\end{split}
\end{equation*}

que es un DCCP para cualquier función de pérdida convexa $L(\btheta)$, y puede ser resuelto eficientemente usando heurísticas bien conocidas como la propuestas en \cite{convex2016}. 

\subsection*{Ventajas e inconvenientes}

Entre las principales ventajas que tiene este grupo de algoritmos, se encuentra que al poder modificar el modelo y optimizar las métricas utilizadas, consiguen un mejor rendimiento en términos de las medidas de equidad y exactitud, variando en función del algoritmo utilizado.

En cambio, como estos métodos modifican directamente el proceso de aprendizaje, a menudo son difíciles de generalizar a diferentes modelos o métricas. Además, en el ámbito de los problemas de \textit{machine learning} en el mundo real, no siempre tendremos acceso al modelo de clasificación, por lo que esto supone un gran inconveniente en la optimización del mismo.

\clearpage

\subsection*{Otros ejemplos en la literatura}

La equidad contrafactual presentada por \cite{counterfactual2018}, se basa en la noción de que un resultado debería ser el mismo independientemente del grupo demográfico del individuo. Definiendo una decisión como justa si se mantiene igual cuando se cambia el valor del atributo protegido. En el artículo, se plantea como objetivo la mitigación del sesgo como un problema de optimización de equidad usando como base la inferencia causal. Este enfoque funciona bien para capturar los sesgos sociales e identificar el equilibrio entre equidad y utilidad. 

\cite{worlds2017} presenta un artículo donde extiende el trabajo de \cite{counterfactual2018} aportando un método para ofrecer predicciones justas con respecto a varios modelos causales posibles simultáneamente.

\section{Algoritmos de posprocesamiento}

Los \textit{algoritmos de posprocesamiento} tienen como objetivo ajustar un clasificador ya entrenado para que cumpla con unas restricciones de equidad específicas. Esto se suele hacer mediante la calibración de umbrales, cuya idea principal es encontrar un umbral adecuado utilizando una función de puntuación para cada grupo.

\subsection{Ejemplo: Aprendizaje en igualdad de oportunidades}

\cite{eodd2016} desarrolla un marco para eliminar la discriminación de forma óptima para cualquier modelo de clasificación aprendido. Los autores definen la equidad como una restricción del concepto de probabilidades igualadas. Esta técnica, conocida como \textit{predictor derivado}, utiliza la calibración del umbral para navegar por la curva ROC hasta que se cumplan los criterios de equidad establecidos. 

Se utilizan diferentes valores de umbral para los distintos subgrupos, y se buscan soluciones factibles a lo largo de la intersección de cada intervalo convexo de la curva ROC correspondiente. Al definir el rendimiento del modelo como el rendimiento mínimo en cualquier subgrupo, el objetivo de equidad coincidirá con el objetivo del modelo. Dado un clasificador y las correspondientes curvas ROC para ambos grupos. Podemos encontrar el umbral basado en las curvas, como podemos observar en la Figura \ref{fig:posprocessing}.

\clearpage

\begin{figure}[h]
	\centering
	\includegraphics[width=15.0cm]{posexample.png}
	\caption{Búsqueda del clasificador óptimo para los criterios de probabilidades igualadas e igualdad de oportunidad, respectivamente. (\cite{eodd2016})}
    \label{fig:posprocessing}
\end{figure}

El criterio de probabilidades igualadas se satisface únicamente cuando las curvas ROC de los dos grupos se cruzan, como se muestra en la imagen izquierda de la Figura \ref{fig:posprocessing}; la igualdad de oportunidades, como relajación de la noción anterior, puede satisfacerse tomando un umbral tal que las tasas de verdaderos positivos de los dos grupos sean iguales, como se puede ver en la imagen de la derecha.

\subsection*{Ventajas e inconvenientes}

Los algoritmos de posprocesamiento comparten algunas ventajas con los de preprocesamiento, pudiendose aplicar de forma independiente al modelo de clasificación sin necesidad de modificar su método de actuación. Además, consigue un rendimiento relativamente bueno en la optimización de la mayoría de definiciones de equidad (excepto la  equidad contrafactual).

Sin embargo, se puede argumentar que al actuar sobre el modelo después de haberlo aprendido, este proceso es intrínsecamente subóptimo. Siendo equivalente a aprender a sabiendas un modelo sesgado y luego corregirlo, en lugar de aprender un modelo insesgado desde el principio.

\subsection*{Otros ejemplos en la literatura}

En el artículo presentado por \cite{woodworth2017} se amplía el trabajo de \cite{eodd2016} demostrando que su método de posprocesamiento podría ser subóptimo en algunos casos. Se ofrece una demostración de que el problema es intratable y se presenta un método nuevo que aproxima el criterio de equidad proporcionando un resultado estadísticamente cercano al óptimo.

\ctparttext{\color{black}\begin{center}
Discusión sobre la inferencia causal, el cálculo de contrafactuales y el teorema de incompatibilidad como fundamentos matemáticos de la equidad contrafactual.
\end{center}}
\part{Fundamentos de la equidad contrafactual}

\chapter{Inferencia causal}

En este capítulo formalizaremos algunos conceptos básicos para desarrollar la teoría relativa a la causalidad en el ámbito de la equidad. Además, introduciremos los grafos como herramienta para describir los modelos causales explicados, así como los efectos que tienen estos modelos en las poblaciones donde se aplican.

\section{Modelos causales}

Elegiremos los \textit{modelos causales estructurales} aprovechando que pueden ofrecernos una base sólida para las diferentes nociones causales utilizadas en este trabajo. La forma más sencilla de conceptualizar un modelo causal estructural es como un programa que genera una distribución a partir de \textit{variables de ruido} independientes mediante una secuencia de instrucciones formales. 

Imaginemos que en lugar de muestras de una distribución, tenemos un programa informático que genera muestras a partir de una semilla aleatoria. El código de este programa, partiría de una semilla aleatoria simple e iría construyendo muestras cada vez más complejas. Esta idea es la misma que utiliza un modelo causal estructural cambiando la sintaxis de programación por lenguaje matemático.

\subsection{Ejemplo: Construcción de un modelo causal}

\label{subsec:sobrepeso}

Supongamos una población en la que un individuo hace ejercicio regularmente con una probabilidad de $\frac{1}{2}$. Con una probabilidad de $\frac{1}{3}$, el individuo tiene predisposición a desarrollar sobrepeso en ausencia de ejercicio regular. Del mismo modo, en ausencia de ejercicio, la aparición de una enfermedad cardíaca puede aparecer con una probabilidad de $\frac{1}{3}$. Denotaremos por $X$ el indicador de ejercicio regular, por $Y$ el de exceso de peso, y por $Z$ el indicador de la enfermedad cardíaca. A continuación, construiremos un modelo causal estructural para generar muestras de esta población hipotética (\cite{fairnesslearning2019}).

\clearpage

\begin{algorithm}[h]
\caption{Programa distribución causal 1.}
    Muestras de variables aleatorias independientes de Bernoulli:
    
    $U_1 \sim  Bernoulli\left(\frac{1}{2}\right), \ U_2,U_3 \sim  Bernoulli\left(\frac{1}{3}\right)$\;
    $X:=U_1$\;
    $Y:=$ \textbf{if} $X=1$ \textbf{then} $0$ \textbf{else} $U_2$\;
    $Z:=$ \textbf{if} $X=1$ \textbf{then} $0$ \textbf{else} $U_3$\;
    \label{alg:programa2}
\end{algorithm}

A partir de la descripción anterior, observamos que en nuestra población el ejercicio evita tanto el sobrepeso como las enfermedades cardíacas, pero en ausencia de ejercicio, ambos son independientes. Nuestro programa genera una distribución conjunta sobre las variables aleatorias $(X, Y, Z)$. Podemos calcular las probabilidades bajo esta distribución. Por ejemplo, la probabilidad de sufrir una enfermedad cardíaca bajo la distribución especificada por nuestro modelo es de $\frac{1}{2}\cdot \frac{1}{3}=\frac{1}{6}$. También podemos calcular la probabilidad condicional de padecer enfermedades cardíacas dado el sobrepeso. Dado el suceso $Y = 1$ podemos inferir que el individuo no hace ejercicio $X=0$, por lo que la probabilidad de sufrir una enfermedad cardíaca debido al sobrepeso es $\frac{1}{3}$.

Formalmente, tener un programa que genere una distribución es más potente que el simple acceso al muestreo. Una de las razones es que podemos manipular el programa de la manera que queramos, mientras resulte en un programa funcional. Podríamos, por ejemplo, establecer $Y := 1$, dando lugar a una nueva distribución. El programa resultante tiene el siguiente aspecto:

\begin{algorithm}[h]
\caption{Programa distribución causal 2.}
    Muestras de variables aleatorias independientes de Bernoulli:
    
    $U_1 \sim  Bernoulli\left(\frac{1}{2}\right), \ U_2,U_3 \sim  Bernoulli\left(\frac{1}{3}\right)$\;
    $X:=U_1$\;
    $Y:=1$\;
    $Z:=$ \textbf{if} $X=1$ \textbf{then} $0$ \textbf{else} $U_3$\;
\end{algorithm}

Calculando de nuevo la probabilidad de sufrir una enfermedad cardíaca sobre la nueva distribución, de nuevo obtenemos $\frac{1}{6}$. Este cálculo revela una idea importante, la sustitución $Y := 1$ no corresponde a un condicionamiento de $Y = 1$. Una se trata de una acción y la otra es una observación de la que podemos extraer conclusiones. En este ejemplo, si observamos que un individuo tiene sobrepeso, podemos inferir que tiene un mayor riesgo de enfermedad cardíaca. Sin embargo, esto no significa que la reducción del peso corporal evite las enfermedades cardíacas. En cambio, la intervención $Y := 1$ crea un nuevo modelo en el que todos los individuos de la población tienen sobrepeso con todo lo que ello conlleva.

A continuación, profundizaremos un poco más en este punto considerando otra población hipotética, especificada por el siguiente programa:

\begin{algorithm}[h]
\caption{Programa distribución causal 3.}
    Muestras de variables aleatorias independientes de Bernoulli:
     
    $U_1 \sim  Bernoulli\left(\frac{1}{2}\right), \ U_2,U_3 \sim  Bernoulli\left(\frac{1}{3}\right)$\;
    $Y:=U_2$\;
    $X:=$ \textbf{if} $Y=0$ \textbf{then} $0$ \textbf{else} $U_1$\;
    $Z:=$ \textbf{if} $X=1$ \textbf{then} $0$ \textbf{else} $U_3$\;
    \label{alg:programa3}
\end{algorithm}

En esta población, la única razón por la que los individuos eligen hacer ejercicio con cierta probabilidad es el sobrepeso. Por otro lado, las enfermedades del corazón se desarrollan en ausencia de ejercicio. La sustitución $Y := 1$ en este modelo conduce a un aumento de la probabilidad de hacer ejercicio y por tanto, a una disminución de la probabilidad de sufrir una enfermedad cardíaca. El condicionamiento de $Y = 1$ también tiene el mismo efecto y en ambos casos, la probabilidad de sufrir un problema cardíaco es de  $\frac{1}{6}$.

\subsection{Formalización de los modelos causales estructurales}

Los modelos causales estructurales nos proporcionan un cálculo preciso para razonar sobre el efecto de las acciones hipotéticas. Formalmente, un modelo causal estructural es una secuencia de asignaciones que generan una distribución conjunta a partir de variables de ruido independientes. A continuación ofreceremos la definición de modelo causal estructural presentada por \cite{causality2000}.\\

\begin{definition}[Modelo causal estructural] \label{def:modcausalest}
Un \textit{modelo causal estructural} $M$ se define como una tupla $(U,V,F)$ de conjuntos tales que:
\begin{itemize}
    \item $U$ es un conjunto de variables aleatorias de ruido, las cuales deben ser conjuntamente independientes. Corresponden a factores no causados por ninguna variable del conjunto $V$ de variables observadas.
    \item $F$ es un conjunto de funciones $\{f_1,\dots,f_n\}$, una para cada $V_i\in V$, tal que, $$V_i=f_i(pa_i,U_i), \ \text{para todo} \ i=1,\dots,n.$$
    donde $pa_i \subseteq V \setminus \{V_i\}$ y $U_i \subseteq U$. Estas ecuaciones también son conocidas como ecuaciones estructurales.
\end{itemize}
\end{definition}\

El modelo es causal en el sentido de que, dada una distribución de probabilidad $P(U)$ sobre las variables de ruido $U$, podemos derivar la distribución de un subconjunto $W \subseteq V$ tras una intervención en $V \setminus W$. Cuando $M$ denota un modelo causal estructural, escribiremos la probabilidad de un evento $E$ bajo la distribución conjunta vinculada como $P_M(E)$. 

Para familiarizarnos con la notación, supongamos que $M$ denota el modelo causal estructural del Apartado \ref{subsec:sobrepeso}, entonces la probabilidad de sufrir una enfermedad cardíaca en este modelo será $P_M(Z) = \frac{1}{6}$.

\section{Grafos causales}

La notación $pa_i$ utilizada en la Definición \ref{def:modcausalest} se refiere al subconjunto de variables $pa(V_i)$ que contiene los padres del nodo $V_i$. Esta notación viene motivada por la suposición de que el modelo se factoriza como un grafo dirigido, el cual en este trabajo, restringiremos al caso acíclico (DAG). A este grafo lo llamaremos: el \textit{grafo o diagrama causal} correspondiente al modelo causal estructural especificado.\\

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (z) {$X$};
\node[mynode,below left=of z] (x) {$Y$};
\node[mynode, below right=of z] (y) {$Z$};

\node[mynode,right=of y] (a) {$Y$};
\node[mynode,above right=of a] (b) {$X$};
\node[mynode,below right=of b] (c) {$Z$};

\path (z) edge[-latex] (x)
(z) edge[-latex] (y);

\path (a) edge[-latex] (b)
(c) edge[latex-] (b);

\end{tikzpicture}
\caption{Grafos causales de los modelos descritos por los Programas \ref{alg:programa2} y \ref{alg:programa3}, respectivamente.}
\end{figure}

Los diagramas causales se utilizan cuando las asignaciones exactas en un modelo causal estructural son secundarias y lo que es realmente relevante son los caminos presentes y ausentes entre nodos. 

Los grafos también nos permiten aprovechar el lenguaje establecido de la teoría de grafos para discutir nociones causales. En particular, los grafos causales nos permiten distinguir la causa y el efecto (de tipo directo o indirecto) en función de si un nodo es ancestro o descendiente de otro.

\subsection{Forks}

\begin{definition} [\textit{Fork}]
Sea $G$ un grafo acíclico dirigido, $U$ el camino entre dos nodos y $A\in U$. Llamaremos \textit{fork} al nodo $A$ si $(A,B)\in E$, para todo $B\in ve(A) \cap U$.
\end{definition}\

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (z) {A};
\node[mynode,below left=of z] (x) {B};
\node[mynode, below right=of z] (y) {C};

\path (z) edge[-latex] (x)
(z) edge[-latex] (y);

\end{tikzpicture}
\caption{Ejemplo de \textit{fork}.}
\label{fig:fork}
\end{figure}

\clearpage

En la Figura \ref{fig:fork}, el nodo $A$ es un ejemplo de \textit{fork} o dicho de otro modo, el nodo $A$ es una causa común de los nodos $B$ y $C$. En el grafo causal resultante de la distribución del Programa $\ref{alg:programa2}$, el nodo $X$ también es un ejemplo de \textit{fork} ($Y \leftarrow X \to Z$). En ese caso, la variable indicadora de ejercicio regular $X$ influía tanto en el aumento de peso $Y$ como en el riesgo de enfermedad $Z$, pero como ya discutimos en el Apartado \ref{subsec:sobrepeso}, las variables $Y$ y $Z$ no están correlacionadas positivamente. Llegamos a la conclusión de que el nodo \textit{fork}, tiene un efecto de confusión que conduce a un desacuerdo entre el cálculo de las probabilidades condicionales y las intervenciones. 

\begin{example}
En un conocido estudio médico, un presunto efecto beneficioso de la terapia de sustitución hormonal para reducir las enfermedades cardiovasculares desapareció tras identificar el estatus socioeconómico como variable de confusión (\cite{ejconfusion2002}). Los ejemplos de confusión suponen una amenaza para la validez de las conclusiones extraídas de los datos en problemas del mundo real.
\end{example}

\subsection{Colliders}

\begin{definition} [\textit{Collider}]
Sea $G$ un grafo acíclico dirigido, $U$ el camino entre dos nodos y $A\in U$. Llamaremos \textit{collider} al nodo a $A$ si $(B,A)\in E$, para todo $B\in ve(A) \cap U$.
\end{definition}

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode,right=of y] (a) {B};
\node[mynode,above right=of a] (b) {A};
\node[mynode,below right=of b] (c) {C};

\path (a) edge[-latex] (b)
(b) edge[latex-] (c);

\end{tikzpicture}
\caption{Ejemplo de \textit{collider}.}
\label{fig:collider}
\end{figure}\

En la Figura \ref{fig:collider}, el nodo $A$ es un ejemplo de \textit{collider}. Cabe destacar que los \textit{colliders} no dan lugar a situaciones en las que se pueda dar confusión. De hecho, en la figura anterior, la relación entre $B$ y $C$ no es confusa, lo que significa que podemos sustituir las intervenciones por probabilidades condicionales. Sin embargo, condicionar un \textit{collider}, podría crear una correlación entre $B$ y $C$, un fenómeno al que denominaremos sesgo de \textit{collider}. 

\begin{example}
En el ámbito sanitario, dos enfermedades independientes pueden correlacionarse negativamente cuando se analizan pacientes hospitalizados. La razón es que cuando cualquiera de las dos enfermedades ($B$ o $C$) es suficiente para el ingreso en el hospital (indicado por la variable $A$), observar que un paciente tiene una enfermedad hace que la otra sea estadísticamente menos probable. A esto es lo que se le conoce como paradoja de Berkson (\cite{berksonparadox2014}).
\end{example}

\subsection{Mediador}

En la definición de \textit{fork}, no tenemos una relación directa entre los nodos $B$ y $C$. Si queremos un efecto total de $B$ sobre $C$ estableceremos esta relación causal a través de $A$. En este caso, $A$ no será un factor confusión y recibirá el nombre de \textit{mediador}.\\

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (z) {A};
\node[mynode,below left=of z] (x) {B};
\node[mynode, below right=of z] (y) {C};

\path (x) edge[-latex] (z)
(z) edge[-latex] (y);

\end{tikzpicture}
\caption{Ejemplo de mediador.}
\label{fig:mediator}
\end{figure}

En el grafo causal asociado a la distribución generada por el Programa \ref{alg:programa3}, el nodo $X$ es un ejemplo de mediador ($Y \to X \to Z$). La noción de mediador es especialmente relevante para el tema del análisis de la discriminación ya que establece una relación directa entre las diferentes variables que definen a un individuo. Esta relación causal nos servirá como herramienta para extraer conclusiones sobre las causas de segregación entre grupos.

\section{Intervención y confusión}

Los modelos causales estructurales nos proporcionan una herramienta de formalización del efecto de acciones e intervenciones sobre la población donde se aplican. Como hemos visto previamente, para modelar estos efectos simplemente necesitamos la capacidad de realizar sustituciones.

\subsection{Operadores para realizar actuaciones en el modelo}

A partir de los ejemplos propuestos en el Apartado \ref{subsec:sobrepeso}, hemos observado que fijar una variable por sustitución puede corresponder o no a una probabilidad condicional. Esto refuerza nuestra intuición de que una observación no es una acción. En cambio, una sustitución sí es una acción, ya que al sustituir un valor estamos rompiendo el curso natural de la acción captada por nuestro modelo.\\

\begin{definition}[Intervención]
Dado un modelo causal estructural $M$, se define una \textit{intervención} sobre una variable observada $X$ como la sustitución de la ecuación $X:=f(pa,U)$ por la ecuación $X:=x$ para un valor $x$ constante.
\end{definition}

\clearpage

Denotaremos el modelo resultante por $M' = M[X := x]$ para indicar la modificación que realizamos sobre el modelo original $M$. Bajo esta asignación mantenemos $X$ constante eliminando la influencia de sus nodos padres y por tanto, de cualquier otra variable del modelo. Por otra parte, los nodos hijos de $X$ recibirán un valor constante $x$ cuando consulten el valor de su padre.\\

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (z) {$X$};
\node[mynode,above=of z] (u) {$P_2$};
\node[mynode,above left=of z] (v) {$P_1$};
\node[mynode,above right=of z] (w) {$P_3$};
\node[mynode,below=of z] (s) {$H_2$};
\node[mynode,below left=of z] (x) {$H_1$};
\node[mynode, below right=of z] (y) {$H_3$};

\node[mynode,right=of y] (a) {$H_1$};
\node[mynode,above right=of a] (b) {$X$};
\node[mynode,above=of b] (c) {$P_2$};
\node[mynode,above right=of b] (d) {$P_3$};
\node[mynode,above left=of b] (e) {$P_1$};
\node[mynode,below=of b] (f) {$H_2$};
\node[mynode,below right=of b] (g) {$H_3$};

\path (z) edge[-latex] (x)
(z) edge[-latex] (y)
(z) edge[-latex] (s)
(v) edge[-latex] (z)
(u) edge[-latex] (z)
(w) edge[-latex] (z);

\path (b) edge[-latex] (a)
(b) edge[-latex] (f)
(b) edge[-latex] (g);

\end{tikzpicture}
\caption{Grafo causal antes y después de la sustitución.}
\end{figure}

El operador de asignación también se denomina \textit{operador do} para destacar que corresponde a la realización de una acción o intervención. La notación que usaremos para calcular las probabilidades para un evento cualquiera $E$ después de aplicar el operador do, será $P_{M[X:=x]}(E)$. También podemos utilizar otra notación que aproxima el concepto de probabilidad condicional y es equivalente al anterior y que se define como: 
\begin{equation*}
P(E \mid \text{do}(X:=x))=P_{M[X:=x]}(E).
\end{equation*}

\subsection{Confusión entre dos variables}

Las cuestiones importantes en inferencia causal están relacionadas con cuándo podemos reescribir una operación do en términos de probabilidades condicionales. Cuando esto sea posible, podremos estimar el efecto de la operación do a partir de las probabilidades condicionales estimadas a partir de los datos. 

Sea una variable $Y$ sobre la que actúa una variable $X$, nos interesará que exista una equivalencia entre el efecto causal de la acción y la probabilidad condicional correspondiente, es decir, que se cumpla la siguiente igualdad: 
\begin{equation*}
P(Y=y \mid \text{do}(X:=x))=P(Y=y \mid X=x).
\end{equation*}

En general, esto no es cierto. Al fin y al cabo, la diferencia entre la observación (probabilidad condicional) y la acción (intervención) es la principal motivación de la inferencia causal.\\

\begin{definition}[Confusión]
Sean $Y$ una variable aleatoria sobre la que actúa otra variable $X$, diremos que son \textit{confusas} si, y solo si, $$P(Y=y \mid \text{do}(X:=x)) \neq P(Y=y \mid X=x).$$
\end{definition}\

Cuando tenemos dos variables aleatorias confusas, podemos estimar el efecto de una intervención en términos de probabilidades condicionales a partir de la denominada fórmula de ajuste.\\

\begin{proposition} \label{prop:forajuste}
Sean $X,Y$ dos variables confusas, podemos aproximar el efecto causal de una intervención dada a partir de probabilidades condicionales como: $$P(Y=y \mid \text{do}(X:=x))=\sum_{z}P(Y=y \mid X=x,PA=z)P(PA=z),$$
donde $PA$ indica el conjunto $pa(X)$.
\end{proposition}\

Dependiendo de la estructura del grafo podremos eliminar o no la confusión entre dos variables utilizando la fórmula de ajuste sobre un nodo u otro. Si el grafo tiene una estructura de \textit{fork} (por ejemplo, $B \leftarrow A \to C$), eliminaremos la confusión entre los nodos $B$ y $C$, condicionando $A$. En cualquier otro caso (mediador o \textit{collider}), ajustar una variable tendría consecuencias opuestas a las que buscamos.

\subsection*{Criterio de \textit{backdoor}}

El tratamiento de la confusión a partir de la fórmula de ajuste, puede ser una tarea complicada cuando la cantidad de nodos en el grafo aumente considerablemente. Para detectar las variables sobre las que deberemos condicionar, aparece el \textit{criterio de backdoor} (\cite{causality2000}). Este método parte de la idea de seleccionar un conjunto de variables que ''bloqueen'' todos los \textit{caminos de backdoor} entre los dos nodos sobre los que queremos eliminar la confusión.\\

\begin{definition}[Camino de \textit{backdoor}]
Un \textit{camino de backdoor} entre dos nodos $A$ y $B$ es cualquier camino que empiece con una arista de la forma ''$\leftarrow$'' hacia $A$.
\end{definition}\

\begin{definition}[Conjunto de \textit{backdoor}]
Un \textit{conjunto de backdoor} es una secuencia de variables o nodos contenida en un camino de \textit{backdoor}.
\end{definition}\

Para aplicar el criterio de \textit{backdoor}, primero seleccionaremos un conjunto de \textit{backdoor} de nodos del grafo. Si el conjunto está formado por una secuencia de nodos relacionados únicamente por aristas de tipo ''$\to$'', podremos eliminar la confusión entre las variables aplicando la fórmula de ajuste sobre un nodo central de la cadena. Por otro lado, si el camino contiene un \textit{collider} o un descendiente de este, la confusión es inevitable, ya que ''bloqueando'' el camino podríamos impedir que la información fluyera a través de los nodos.

\begin{example}
Sea un grafo causal dado por la secuencia $A \leftarrow C \to D \to E \to B$, nuestro objetivo será eliminar la confusión entre las variables $A$ y $B$. Es evidente que la cadena $A \leftarrow C \to D \to E \to B$ es un camino de \textit{backdoor}. A continuación, seleccionamos un conjunto de \textit{backdoor} entre ambos nodos, por ejemplo $C \to D \to E$. En vista de la forma de la secuencia anterior, podemos eliminar la confusión entre $A$ y $B$ aplicando la fórmula de ajuste sobre el nodo $D$.
\end{example}

\subsection*{Confusión no observada}

La fórmula de ajuste presentada en la Proposición \ref{prop:forajuste}, podría sugerir que siempre podemos eliminar el sesgo de confusión condicionando a los nodos padres. Sin embargo, esto no se cumple cuando aparecen \textit{factores de confusión no observados}. En la práctica, a menudo hay variables que son difíciles de medir o que no fueron resgistradas. Podemos incluir estos nodos no observados en un grafo indicando su influencia con líneas discontinuas.\\

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (z) {C};
\node[mynode,below left=of z] (x) {A};
\node[mynode, below right=of z] (y) {B};

\node[mynode,right=of y] (a) {A};
\node[mynode,above right=of a] (b) {C};
\node[mynode,right=of b] (d) {D};
\node[mynode,below right=of d] (c) {B};

\path (z) edge[dashed, -latex] (x)
(z) edge[dashed, -latex] (y);

\path (b) edge[dashed, -latex] (d)
(a) edge[dashed, latex-] (b)
(d) edge[-latex] (c);

\end{tikzpicture}
\caption{Ejemplos de confusión no observada.}
\label{fig:unobserved}
\end{figure}

La Figura \ref{fig:unobserved} muestra dos casos de confusión no observada. En el primer ejemplo, el efecto causal de $A$ sobre $B$ no es identificable. En el segundo caso, podemos eliminar la confusión entre $A$ y $B$ a partir del criterio de \textit{backdoor}. Sea $C \to D \to B$ un conjunto de \textit{backdoor}, podemos eliminar la confusión entre las variables $A$ y $B$ ajustando la fórmula sobre la variable $D$ aunque $C$ no se observe.

Cabe destacar que podemos combatir la confusión no observada aumentando el número de variables consideradas, pero esto aumentaría progresivamente la complejidad de nuestro modelo causal. En la práctica, es habitual controlar el mayor número posible de variables con el objetivo de eliminar el sesgo de confusión. Sin embargo, como hemos visto, el control de mediadores y \textit{colliders} podría ser problemático en la resolución de nuestro problema.

\chapter{Teorema de imposibilidad de la equidad}

\label{ch:teoremaimposibilidad}

En este capítulo se propondrá una demostración del teorema de imposibilidad de equidad. Este enunciado surge como una formalización de la \textit{incompatibilidad} entre los criterios de paridad demográfica, probabilidades igualadas y tasa de paridad predictiva.

\section{Caracterización del teorema}

La mayoría de los criterios de equidad definidos en el Capítulo \ref{ch:formalmedeq} se construyen a partir de restricciones no triviales de la distribución de probabilidad conjunta. Por ello, es lógico pensar que la imposición de varios de ellos de forma simultánea, restringirían el espacio de búsqueda hasta el punto de que solo obtendríamos soluciones degeneradas.

El Teorema de la Imposibilidad, cuya primera aproximación fue ofrecida por \cite{fairth2016}, establece que no se puede satisfacer más de una medida de equidad al mismo tiempo para un clasificador bien entrenado y un atributo sensible que sea capaz de introducir un sesgo en el modelo. En nuestro caso, presentaremos una versión del Teorema de la Imposibilidad para tres de los criterios de equidad de grupo estudiados: paridad demográfica, tasa de paridad predictiva y probabilidades igualadas. Los enunciados de los lemas demostrados a lo largo de este capítulo, han sido definidos en base al trabajo de \cite{fairnesslearning2019}.


\subsection{Paridad demográfica versus Tasa de paridad predictiva}

Comenzamos con un lema que muestra cómo, en general, la paridad demográfica y la paridad predictiva se excluyen mutuamente. La única suposición necesaria es que el atributo sensible $A$ y la variable $Y$ no son independientes, es decir, dependen una de la otra. Esto es una forma diferente de decir que un grupo tiene mayor tasa de resultados positivos que otro, lo que es cierto en la mayoría de casos.

\clearpage

\begin{lemma} 
Supongamos que $A$ e $Y$ son variables dependientes. Entonces la paridad demográfica ($\hat{Y} \perp A$) y la tasa de paridad predictiva ($Y \perp A \mid \hat{Y}$) no pueden verificarse simultáneamente.
\label{lem:demopred}
\end{lemma}

\begin{proof}
El enunciado del lema es análogo a la siguiente expresión,
\begin{equation*}
Y \not \perp A \quad \Longrightarrow \quad \neg(\hat{Y} \perp A \ \wedge \ Y \perp A \mid \hat{Y}).
\end{equation*}

Procederemos por contrarrecíproco, lo que equivale a demostrar que,
\begin{equation*}
\hat{Y} \perp A \ \wedge \ Y \perp A \mid \hat{Y} \quad \Longrightarrow \quad Y \perp A.
\end{equation*}

Si se da la independencia entre las variables $A$ e $\hat{Y}$, entonces se cumple,
\begin{equation}\label{eq:lema1eq1}
P(A=a,\hat{Y}=\hat{y})=P(A=a)P(\hat{Y}=\hat{y}).
\end{equation}

Por otro lado, sabemos que la independencia condicional dada por $Y \perp A \mid \hat{Y}$ satisface que,
\begin{equation}\label{eq:lema1eq2}
P(Y=y, A=a \mid \hat{Y}=\hat{y})=P(Y=y\mid \hat{Y}=\hat{y})P(A=a\mid \hat{Y}=\hat{y}).
\end{equation}

Finalmente, aplicando las hipótesis del enunciado y usando el teorema de probabilidad total (T.P.T) sobre $P(Y=y,A=a)$, llegamos a la siguiente expresión:
\begin{equation*}
\begin{split}
P(Y=y,A=a)&\overset{\text{T.P.T}}{=}\sum_{\hat{y}} P(\hat{Y}=\hat{y})P(Y=y,A=a \mid \hat{Y}=\hat{y})\\
&\overset{(\ref{eq:lema1eq2})}{=} \sum_{\hat{y}} P(\hat{Y}=\hat{y})P(Y=y\mid \hat{Y}=\hat{y})P(A=a \mid \hat{Y}=\hat{y})\\
&= \sum_{\hat{y}} P(\hat{Y}=\hat{y})P(Y=y\mid \hat{Y}=\hat{y})\ddfrac{P(A=a,\hat{Y}=\hat{y})}{P(\hat{Y}=\hat{y})}\\
&\overset{(\ref{eq:lema1eq1})}{=} \sum_{\hat{y}} P(\hat{Y}=\hat{y})P(Y=y\mid \hat{Y}=\hat{y})\ddfrac{P(A=a)P(\hat{Y}=\hat{y})}{P(\hat{Y}=\hat{y})}\\
&=\sum_{\hat{y}} P(\hat{Y}=\hat{y})P(Y=y\mid \hat{Y}=\hat{y})P(A=a)\\
&=P(A=a)\sum_{\hat{y}} P(\hat{Y}=\hat{y})P(Y=y\mid \hat{Y}=\hat{y})\\
&\overset{\text{T.P.T}}{=}P(A=a)P(Y=y).
\end{split}
\end{equation*}

La última igualdad nos da que las variables $A$ e $Y$ son independientes y por tanto tenemos que $Y\perp A$.
\end{proof}

\subsection{Paridad demográfica versus Probabilidades igualadas}

Un resultado análogo de exclusión mutua es válido para la paridad demográfica y el criterio de probabilidades igualadas. El enunciado, en este caso, es un poco más rebuscado y requiere la suposición adicional de que la variable $Y$ sea binaria. También necesitamos que la variable $\hat{Y}$ dependa de $Y$. Esta suposición es una relajación bastante suave, ya que cualquier función de clasificación útil tiene correlación con la variable $Y$.\\

\begin{lemma} 
Supongamos que $Y$ es una variable binaria, $A$ e $Y$ son dependientes y además, $Y$  también depende de $\hat{Y}$. Entonces la paridad demográfica ($\hat{Y} \perp A$) y el criterio de las probabilidades igualadas ($\hat{Y} \perp A \mid Y$) no pueden verificarse simultáneamente.
\label{lem:demoigu}
\end{lemma}

\begin{proof}
El enunciado del lema es equivalente a la siguiente expresión,
\begin{equation*}
Y \not \perp A \ \wedge \ Y \not \perp \hat{Y} \quad \Longrightarrow \quad \neg(\hat{Y} \perp A \ \wedge \ \hat{Y} \perp A \mid Y).
\end{equation*}

Por el contrarrecíproco, deberemos demostrar que,
\begin{equation*}
\hat{Y} \perp A \ \wedge \ \hat{Y} \perp A \mid Y
 \quad \Longrightarrow \quad Y  \perp A \ \vee \ Y  \perp \hat{Y}.
\end{equation*}

Si se da la independencia entre las variables $A$ e $\hat{Y}$, entonces se cumple,
\begin{equation}\label{eq:lema2eq1}
P(\hat{Y}=\hat{y},A=a)=P(\hat{Y}=\hat{y})P(A=a).
\end{equation}

Sabemos que la independencia condicional dada por $\hat{Y} \perp A \mid Y$ satisface que,
\begin{equation}\label{eq:lema2eq2}
P(\hat{Y}=\hat{y} \mid A=a, Y=y)=P(\hat{Y}=\hat{y} \mid Y=y).
\end{equation}

Aplicando el teorema de la probabilidad total (T.P.T) y la hipótesis de independencia condicional sobre $P(\hat{Y}=\hat{y} \mid A=a)$, obtenemos la siguiente expresión:
\begin{equation}\label{eq:lema2eq3}
\begin{split}
P(\hat{Y}=\hat{y} \mid A=a) &\overset{\text{T.P.T}}{=} \sum_{y} P(Y=y \mid A=a) P(\hat{Y}=\hat{y} \mid A=a, Y=y)\\
&\overset{(\ref{eq:lema2eq2})}{=} \sum_{y} P(Y=y \mid A=a)P(\hat{Y}=\hat{y} \mid Y=y).
\end{split}
\end{equation}

Usando la hipótesis de independencia entre las variables $A$ e $\hat{Y}$,
\begin{equation}\label{eq:lema2eq4}
\begin{split}
P(\hat{Y}=\hat{y} \mid A=a) &= \ddfrac{P(\hat{Y}=\hat{y},A=a)}{P(A=a)}\\ &\overset{(\ref{eq:lema2eq1})}{=} \ddfrac{P(\hat{Y}=\hat{y})P(A=a)}{P(A=a)}\\
&= P(\hat{Y}=\hat{y}).
\end{split}
\end{equation}

Combinando las Ecuaciones (\ref{eq:lema2eq3}) y (\ref{eq:lema2eq4}), llegamos a la siguiente expresión:
\begin{equation}\label{eq:lema2eq5}
P(\hat{Y}=\hat{y}) = \sum_{y} P(Y=y \mid A=a)P(\hat{Y}=\hat{y} \mid Y=y).
\end{equation}

Por otro lado, aplicando el teorema de la probabilidad total sobre $P(\hat{Y}=\hat{y})$, tenemos que,
\begin{equation}\label{eq:lema2eq6}
P(\hat{Y}=\hat{y}) = \sum_{y} P(Y=y) P(\hat{Y}=\hat{y} \mid Y=y).
\end{equation}

Combinando las Ecuaciones (\ref{eq:lema2eq5}) y (\ref{eq:lema2eq6}), conseguimos la expresión dada por:
\begin{equation}\label{eq:lema2eq7}
\sum_{y} P(Y=y \mid A=a)P(\hat{Y}=\hat{y} \mid Y=y) = \sum_{y} P(Y=y) P(\hat{Y}=\hat{y} \mid Y=y).
\end{equation}

A continuación, y para que sea más cómodo de manipular la expresión anterior definiremos la siguiente notación:
\begin{equation*}
\begin{split}
    p&=P(Y=0),\\
    p_a&=P(Y=0 \mid A=a),\\
    \hat{y}_{y}&=P(\hat{Y}=\hat{y} \mid Y=y).
\end{split}
\end{equation*}

Por hipótesis del Lema, $Y$ es una variable binaria (supongamos que puede tomar los valores $0$ o $1$) y por tanto, podemos reescribir la Ecuación (\ref{eq:lema2eq7}) como:
\begin{equation*}
p\hat{y}_0+(1-p)\hat{y}_1=p_a\hat{y}_0+(1-p_a)\hat{y}_1.
\end{equation*}

Simplificando en la ecuación anterior, tenemos que,
\begin{equation*}
p(\hat{y}_0-\hat{y}_1)=p_a(\hat{y}_0-\hat{y}_1),
\end{equation*}
lo cual es equivalente a,
\begin{equation}\label{eq:lema2eq8}
(p-p_a)(\hat{y}_0-\hat{y}_1)=0.
\end{equation}

La igualdad de la Ecuación (\ref{eq:lema2eq8}), se satisface si se da alguno de los siguientes casos:

\begin{itemize}
    \item $p=p_a$, que es equivalente a $P(Y=0)=P(Y=0 \mid A=a)$, y por tanto tenemos,
    \begin{equation*}
    \begin{split}
    P(Y=1)&=1-P(Y=0)\\
    &=1-P(Y=0 \mid A=a)\\
    &=P(Y=1 \mid A=a),
    \end{split}
    \end{equation*}
    donde la expresión anterior equivale a que $Y\perp A$.\\
    
    \item $\hat{y}_0=\hat{y}_1$, que equivale a $P(\hat{Y}=\hat{y}\mid Y=0)=P(\hat{Y}=\hat{y}\mid Y=1)$, y por tanto tenemos que $\hat{Y}\perp Y$.
\end{itemize}


\end{proof}


\subsection{Probabilidades igualadas versus Tasa de paridad predictiva}

Por último, pasamos a la relación entre la tasa de paridad predictiva y el criterio de probabilidades igualadas. Ambas exigen una relación de independencia condicional no trivial entre las tres variables $A$, $\hat{Y}$ e $Y$. Imponer ambas simultáneamente conduce a un espacio de soluciones degenerado, como confirma el corolario siguiente.\\

\begin{corollary}
Supongamos que todos los sucesos en la distribución conjunta $(A,\hat{Y},Y)$ tienen probabilidad positiva y además, $A$ depende de $Y$. Entonces el criterio de probabilidades igualadas ($\hat{Y}\perp A \mid Y$) y la tasa de paridad predictiva ($Y\perp A \mid \hat{Y}$) no pueden verificarse simultáneamente.
\label{cor:probpositiva}
\end{corollary}

\begin{proof}
El enunciado del corolario es análogo a la siguiente expresión,
\begin{equation*}
Y \not \perp A \quad \Longrightarrow \quad \neg(\hat{Y} \perp A \mid Y \ \wedge \ Y \perp A \mid \hat{Y}).
\end{equation*}

Por el contrarrecíproco, tenemos que probar que,
\begin{equation*}
\hat{Y} \perp A \mid Y \ \wedge \ Y \perp A \mid \hat{Y} \quad \Longrightarrow \quad Y \perp A.
\end{equation*}

Por la propiedad simétrica de la independencia condicional, tenemos que,
\begin{equation*}
\begin{split}
\hat{Y} \perp A \mid Y &= A \perp \hat{Y} \mid Y,\\
Y \perp A \mid \hat{Y} &= A \perp Y \mid \hat{Y}.\\
\end{split}
\end{equation*}


La hipótesis dada por $A \perp \hat{Y} \mid Y$ satisface que,
\begin{equation}\label{eq:lema3eq1}
P(A=a \mid \hat{Y}=\hat{y}, Y=y)=P(A=a \mid Y=y).
\end{equation}

Por otro lado, sabemos que $A \perp Y \mid \hat{Y}$ cumple que,
\begin{equation}\label{eq:lema3eq2}
P(A=a \mid Y=y, \hat{Y}=\hat{y})=P(A=a\mid \hat{Y}=\hat{y}).
\end{equation}

Para que tengan sentido las Ecuaciones (\ref{eq:lema3eq1}) y (\ref{eq:lema3eq2}), es necesaria la hipótesis de que $P(\hat{Y}=\hat{y},Y=y)>0$ que a su vez, es consecuencia directa de que la distribución conjunta $(A,\hat{Y},Y)$ tenga probabilidad positiva.

Por la propiedad de unión débil de la independencia condicional, tenemos que las Ecuaciones (\ref{eq:lema3eq1}) y (\ref{eq:lema3eq2}) son equivalentes, y por tanto:
\begin{equation}\label{eq:lema3eq3}
P(A=a\mid Y=y)=P(A=a\mid \hat{Y}=\hat{y}).
\end{equation}

Usando la definición de probabilidad condicionada sobre $P(A=a\mid Y=y)$, tenemos que, 
\begin{equation}\label{eq:lema3eq5}
P(A=a\mid Y=y)=\ddfrac{P(A=a,Y=y)}{P(Y=y)},
\end{equation}
queremos demostrar que $Y\perp A$, es decir, $P(A=a,Y=y)=P(A=a)P(Y=y)$, que aplicado a la Ecuación (\ref{eq:lema3eq5}), equivale a probar:
\begin{equation*}
P(A=a\mid Y=y)=P(A=a).
\end{equation*}

Usando el teorema de la probabilidad total (T.P.T) sobre $P(A)$, conseguimos la siguiente expresión:
\begin{equation*}
\begin{split}
P(A=a)&\overset{\text{T.P.T}}{=}\sum_{\hat{y}}P(\hat{Y}=\hat{y})P(A=a \mid \hat{Y}=\hat{y})\\
&\overset{(\ref{eq:lema3eq3})}{=}\sum_{\hat{y}}P(\hat{Y}=\hat{y})P(A=a \mid Y=y)\\
&=P(A=a \mid Y=y)\sum_{\hat{y}}P(\hat{Y}=\hat{y})\\
&=P(A=a \mid Y=y).
\end{split}
\end{equation*}
\end{proof}

Para un objetivo binario, la hipótesis de no degeneración del Corolario  \ref{cor:probpositiva} establece que en todos los grupos, para todos los valores de predicción, tenemos casos positivos y negativos. En caso de que el clasificador sea binario, podemos debilitar la suposición exigiendo que el clasificador haga al menos una predicción falsa positiva. Lo atractivo de la afirmación resultante es que su prueba se basa esencialmente en una relación bastante popular entre la tasa de verdaderos positivos (\textit{Recall}) y el valor predictivo positivo (\textit{Precision}).\\

\begin{lemma}
Supongamos que $\hat{Y}$ es una variable binaria que toma valores de un clasificador con una tasa de falsos positivos no nula y además, $A$ dependiente de $Y$. Entonces el criterio de probabilidades igualadas ($\hat{Y}\perp A \mid Y$) y la tasa de paridad predictiva ($Y\perp A \mid \hat{Y}$) no pueden verificarse simultáneamente.
\label{lem:preigual}
\end{lemma}

\begin{proof}
Dado que $Y \not \perp A$, existirán dos grupos, a los que llamaremos $p_0$ y $p_1$ cumpliendo que $p_0\neq p_1$, donde $p_a=P(Y=1 \mid A=a)$.

Supondremos que se satisface el criterio de las probabilidades igualadas. Por hipótesis el clasificador tendrá la misma tasa para todos los grupos de falsos positivos FPR $> 0$ y de verdaderos positivos TPR $> 0$. Procederemos a demostrar que la tasa de paridad predictiva no se satisface en estas condiciones.

En el caso binario, la tasa de paridad predictiva implica que todos los grupos tienen el mismo de PPV (ver Apartado \ref{subsec:suficiencia}). El valor predictivo positivo en el grupo $a$, denotado PPV$_a$ satisface
\begin{equation*}
    \text{PPV}_a = \ddfrac{\text{TPR}p_a}{\text{TPR}p_a+\text{FPR}(1-p_a)}.
\end{equation*}

En la expresión anterior podemos ver que PPV$_0$ = PPV$_1$ si, y solo si, TPR $= 0$ o FPR $= 0$. Descartaremos esto último por hipótesis. Por tanto, se debe cumplir que TPR $= 0$.  Sin embargo, podemos deducir que NPV$_0 \neq$ NPV$_1$ a partir de la siguiente expresión,
\begin{equation*}
    \text{NPV}_a = \ddfrac{(1-\text{FPR})(1-p_a)}{(1-\text{TPR})p_a+(1-\text{FPR})(1-p_a)}.
\end{equation*}

Por tanto, la tasa de paridad predictiva no se satisface.
\end{proof}

\subsection{Enunciado y demostración}

Una vez demostrados los resultados previos, estamos preparados para enunciar y demostrar la versión del teorema de imposibilidad para la equidad de grupo.\\

\begin{theorem}[Teorema de imposibilidad de la equidad] \label{th:teoimpos}
Consideremos un problema de clasificación binaria con una tasa de falsos positivos no nula donde se cumple la siguiente relación de dependencia entre las variables: $Y \not \perp A$ e $\hat{Y} \not \perp Y$. Si existe una asignación de riesgo, entonces ésta no puede satisfacer los criterios de paridad demográfica, probabilidades igualadas y paridad predictiva simultáneamente dos a dos.
\end{theorem}

\begin{proof}
Bajo las hipótesis del teorema, aplicamos los Lemas \ref{lem:demopred}, \ref{lem:demoigu} y \ref{lem:preigual}.
\end{proof}

Podemos consultar una versión diferente de la demostración del Teorema \ref{th:teoimpos} desde la perspectiva de la inferencia causal en el artículo propuesto por \cite{impossibilityth2021}.

\textcolor{red}{Añadir si hay tiempo los criterios de mejora que propone Saravakumar}


\chapter{Medidas causales}

En este capítulo realizaremos un estudio del modelo contrafactual y cómo sirve de base para diferentes medidas causales, en particular para la equidad contrafactual que discutiremos más detalladamente. 

\section{Contrafactuales}

Una vez definidos los modelos causales estructurales, podemos formular preguntas más delicadas que el mero efecto de una acción. En concreto, preguntas contrafactuales como: ¿Habría evitado el atasco si hubiese tomado otra ruta diferente? o ¿Me habrían concedido el préstamo si mi raza o edad fuesen distintas? Podemos dar respuesta a estas preguntas a partir de un modelo causal estructural. Sin embargo, el procedimiento para extraer la respuesta del modelo necesita del cálculo de \textit{contrafactuales}.

\subsection{Ejemplo: Modelo de decisión contrafactual}

Supongamos un problema de decisión entre dos modelos de caja negra para resolver un problema. Denotaremos por $X$ a la variable indicadora de cada algoritmo. Si el problema es irresoluble ($U=1$) ninguno de los algoritmos podrá encontrar una solución. Si el problema es resoluble ($U=0$), un algoritmo obtendrá mejores resultados que el otro. El rendimiento es independientemente en cualquiera de los dos modelos con una probabilidad de $\frac{1}{2}$. Definiremos dos variables aleatorias $U_0, U_1$ que nos informarán del rendimiento para los algoritmos $X=0$ y $X=1$, respectivamente. El modelo seleccionado entre los dos existentes será elegido al azar por una variable $U_X$ con probabilidad $\frac{1}{2}$. Supondremos también una variable $Y \in \{0,1\}$ que nos dirá si el algoritmo ha encontrado una solución óptima ($Y=0$) o no ($Y=1$). A continuación especificaremos el modelo discutido con el siguiente programa:

\clearpage

\begin{algorithm}[h]
\caption{Programa distribución contrafactual.}
    Muestras de variables aleatorias independientes de Bernoulli: 
    
    $U,U_0,U_1,U_X \sim  Bernoulli\left(\frac{1}{2}\right)$\;
    $X:=U_X$\;
    $Y:=X \cdot \max\{U,U_1\}+(1-X)\cdot \max\{U,U_0\}$ \;
    \label{alg:programa 4}
\end{algorithm}

El grafo asociado al modelo anterior viene dado por:

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (z) {X};
\node[mynode,right=of z] (x) {Y};

\path (x) edge[latex-] (z);

\end{tikzpicture}
\caption{Grafo causal asociado al ejemplo.}
\end{figure}

Supongamos que elegimos el algoritmo $X = 1$ y observamos que no consigue una solución $Y = 1$. A continuación, nos hacemos la siguiente pregunta: ¿Habría sido mejor elegir el otro algoritmo? Para responder a ello, calcularemos la probabilidad $P_{M[X:=0]}(Y = 0)$. Dada la sustitución $X := 0$ en nuestro modelo, para que el algoritmo encontrase una solución óptima necesitamos que $\max\{U, U_0\} = 0$. Esto sólo ocurre cuando $U=0$ (con probabilidad $\frac{1}{2}$) y $U_0=0$ (también con probabilidad $\frac{1}{2}$). Concluimos que $P_{M[X:=0]}(Y = 0)=\frac{1}{2}\cdot \frac{1}{2}=\frac{1}{4}$.

Aunque pudiera parecerlo, esta no es la respuesta correcta a nuestra pregunta. La razón es que hicimos los cálculos sin considerar la decisión previa $\{X = 1, Y = 1\}$. A partir de esta observación ciertas configuraciones de las variables de ruido $(U, U_0, U_1)$ ya no son factibles. En concreto si $U$ y $U_1$ hubieran sido ambos cero, habríamos encontrado una solución correcta con el algoritmo $X = 1$, pero esto es contrario a nuestra observación $Y=1$. De hecho, la configuración $\{X = 1, Y = 1\}$ sólo permite los siguientes valores para $U$ y $U_1$:

\begin{table}[h]
\centering
\resizebox{2.0cm}{!} {
\begin{tabular}{cc}
\hline
$U$ & $U_1$ \\ \hline
$0$ & $1$   \\
$1$ & $0$   \\
$1$ & $1$   \\ \hline
\end{tabular}
}
\caption{Posibles valores de las variables de ruido dada la evidencia observada.}
\end{table}

Cada uno de estos tres casos es igualmente probable, lo que en particular significa que el evento $U = 1$ tiene una probabilidad de $\frac{2}{3}$. Sin considerar la observación, recordemos que $U = 1$ tenía una probabilidad de $\frac{1}{2}$. Esto significa que la evidencia observada $\{X = 1, Y = 1\}$ ha sesgado la distribución de la variable de ruido $U$ hacia el valor $1$. Utilizaremos la letra $U'$ para referirnos a esta versión sesgada de $U$.

Podemos volver a considerar el efecto de la acción $X := 0$ sobre el resultado $Y$ trabajando ahora con la nueva variable $U'$. Para $Y = 0$ necesitamos que $\max \{U', U_0\} = 0$. Esto significa que $U' = 0$, un suceso que ahora tiene probabilidad $\frac{1}{3}$ y $U_0 = 0$ (con probabilidad $\frac{1}{2}$, igual que antes). Por lo tanto, obtenemos que una vez actualizado el modelo, $P_{M'[X:=0]}(Y = 0)=\frac{1}{3}\cdot \frac{1}{2}=\frac{1}{6}$.

Si comparamos el nuevo valor con el resultado anterior, tenemos que la incorporación de las observaciones disponibles en nuestro cálculo disminuyó la probabilidad de que encontrásemos la solución óptima con el otro algoritmo,
\begin{equation*}
    P_{M'[X:=0]}(Y = 0)=\frac{1}{6} < \frac{1}{4}=P_{M[X:=0]}(Y = 0).
\end{equation*}

La razón de ello es que el evento observado sesga la distribución de las variables de ruido haciendo que fuese incluso más probable que en general, el problema no tuviese solución. Llamaremos al resultado que acabamos de calcular: el valor contrafactual al elegir el algoritmo alternativo dado que el algoritmo seleccionado no encontró una solución óptima.


\subsection{Formalización del cálculo contrafactual}

Sea $M=(U,V,F)$ un modelo causal estructural, la especificación de $F$ es un supuesto fuerte que permite el cálculo de valores contrafactuales. Si queremos obtener el valor de $Y$ si $Z$ hubiera tomado valor $z$, para dos variables observadas $Z$ e $Y$. El valor contrafactual se modela como la solución de $Y$ para un $U := u$ dado en el que las ecuaciones de $Z$ se sustituyen por $Z := z$. Denotaremos la expresión anterior por $Y_{Z\leftarrow z}(U)$ (\cite{causality2000}).

La \textit{inferencia contrafactual}, especificada por un modelo causal $M$ dada la evidencia $E$, equivale al cálculo de $P(Y_{Z\leftarrow z}(U) \mid E = e)$, donde $E, Z, Y \subseteq V$. Existen tres pasos esenciales en el cálculo de contrafactuales: En primer lugar, incorporamos las observaciones sesgando las variables de ruido mediante una operación de condicionamiento. En segundo lugar, realizamos una operación do en el modelo causal después de sustituir las variables de ruido sesgadas. Finalmente, calculamos la distribución para una variable objetivo. Estos tres pasos suelen denominarse \textit{abducción, acción} y \textit{predicción} y se describen de la siguiente manera:\\

\begin{definition}[Cálculo del contrafactual] \label{def:pasoscontra}
Dado un modelo causal estructural $M$, un evento observado $E$, una intervención $Z := z$ y una variable objetivo $Y$, definimos el \textit{contrafactual} $Y_{Z \leftarrow z}(E)$ mediante los siguientes pasos:

\begin{itemize}
    \item \textit{Abducción}: Ajustar las variables de ruido al evento observado. Formalmente, condicionar la distribución conjunta de $U = (U_1,\dots,U_n)$ al suceso $E$. Esto da lugar a una distribución sesgada $U'=P(U \mid E=e)$.
    \item \textit{Acción}: Utilizar el operador do para realizar la intervención $Z := z$ en el modelo causal estructural $M$ obteniendo el modelo $M' = M[Z := z]$.
    \item \textit{Predicción}: Calcular el objetivo contrafactual $Y_{Z \leftarrow z}(E)$ usando $U'$ como semilla aleatoria en $M'$.
\end{itemize}
\end{definition}


\section{Equidad contrafactual}

Todos los criterios de equidad definidos en el Capítulo \ref{ch:formalmedeq} tienen limitaciones en su aplicación a problemas reales. Mientras que la equidad por desconocimiento es un criterio insuficiente debido a la gran cantidad de características correlacionadas con los atributos sensibles, la equidad individual tiene el problema de ser directamente dependiende de una distancia fiable entre individuos. Por otro lado, los criterios de equidad de grupo son observacionales y no pueden utilizarse para encontrar la causa de la disparidad entre grupos. 

Como solución a estos problemas, aparecen las \textit{medidas causales} donde, en este trabajo, profundizaremos en la \textit{equidad contrafactual} (\cite{worlds2017}) que puede ser considerada como una subclase de las mismas. Este concepto considera que para un individuo, una decisión es justa si coincide en el mundo real y en un mundo ''contrafactual'' en el que el individuo perteneciese a un grupo demográfico diferente. Esta suposición construye un método para comprobar el tratamiento dispar que surge al sustituir únicamente el atributo sensible y además, proporciona una explicación del impacto del sesgo a través de un grafo causal.\\

\begin{definition}[Equidad contrafactual]
Dado $A\in \mathcal{A}$ un atributo sensible multivaluado, $g\colon \mathcal{X} \to \mathcal{Y}$ un clasificador arbitrario, $X \in \mathcal{X}\setminus \mathcal{A}$ una variable observada cualquiera y $(U,V,F)$ un modelo causal donde $V \equiv A \cup X$. Se dice que $g$ satisface la \textit{equidad contrafactual} si, y solo si, $$P(\hat{Y}_{A\shortleftarrow a}(U)=y \mid X=x, A=a)=P(\hat{Y}_{A\shortleftarrow a'}(U)=y \mid X=x, A=a),$$
para todo $y$ y $a'\neq a$. 
\end{definition}\

\begin{figure}[h]
\centering   
\begin{tikzcd}[m]
  & |[opacity=0]|\mm{A}{raza} & \\
   \mm{S}{estudios} \arrow[drr] & &\arrow[ll] \mm{J}{trabajo}\\
  |[w]| \mm{U}{motivación} \arrow[u] \arrow[rr]& & \mm{Y}{graduado}\\
\end{tikzcd}\qquad
\begin{tikzcd}[m]
  & \mm{A}{raza}\arrow[dr] & \\
   \mm{S}{estudios} \arrow[drr] & &\arrow[ll] \mm{J}{trabajo}\\
  |[w]| \mm{U}{motivación} \arrow[u] \arrow[rr]& & \mm{Y}{graduado}\\
\end{tikzcd}\qquad
\begin{tikzcd}[m]
  & \mm{A}{raza}\arrow[dl]\arrow[dr] & \\
   \mm{S}{estudios} \arrow[drr] & &\arrow[ll] \mm{J}{trabajo}\arrow[d]\\
  |[w]| \mm{U}{motivación} \arrow[u] \arrow[rr]& & \mm{Y}{graduado}\\
\end{tikzcd}
\caption{Ejemplo de grafo causal en un problema real.}
\label{fig:caugraph}
\end{figure}

La Figura \ref{fig:caugraph} muestra varios ejemplos sobre un caso relacionado con la admisión a la universidad (\cite{worlds2017}). Si sustituimos el atributo sensible (raza) por su valor contrafactual, todas las características correlacionadas con él también se verían influidas (estudios y trabajo), propagándose hacia abajo en el grafo causal a través de las ecuaciones estructurales. Cualquier atributo que no descienda del atributo sensible permanecerá igual.

\subsection{Implicaciones de la definición de equidad}

En este apartado, presentaremos algunas implicaciones de la definición de equidad contrafactual y algunos resultados que proporcionan una forma directa de satisfacer esta noción de justicia para un modelo dado (\cite{counterfactual2018}).\\

\begin{lemma} \label{lem:nodescent}
Sea $G$ el grafo causal del modelo dado por $(U,V,F)$, $A\in \mathcal{A}$ un atributo sensible multivaluado y $g\colon \mathcal{X} \to \mathcal{Y}$ un clasificador arbitrario. Entonces $g$ satisface la equidad contrafactual si es una función que no depende de los nodos descendientes de $A$.
\end{lemma}

\begin{proof}
Sea $W$ una variable no descendiente de $A$ en $G$. Entonces $W_{A\leftarrow a}(U)$ y $W_{A\leftarrow a'}(U)$ tienen la misma distribución para los tres pasos de la Definición \ref{def:pasoscontra}. Por lo tanto, la distribución de cualquier función $g$ de los nodos no descendientes de $A$ es invariante con respecto a los valores contrafactuales de $A$.
\end{proof}

\subsection*{Solución a problemas de otras nociones de equidad}

La equidad contrafactual también proporciona una respuesta a algunos problemas sobre la incompatibilidad de los criterios de equidad. Supongamos el caso en el que nos gustaría que nuestro clasificador cumpliese el criterio de igualdad de oportunidades y la tasa de paridad predictiva simultáneamente. Se demostró en el Capítulo \ref{ch:teoremaimposibilidad} que esto es imposible. La equidad contrafactual nos aporta una solución en este escenario, sugiriendo que tanto la igualdad de oportunidades como la paridad predictiva pueden ser insuficientes si $A$ e $Y$ están asociados: suponiendo que las variables no son confusas, esto es el resultado de que $A$ sea una causa de $Y$. 

Como estamos en el ámbito de la equidad contrafactual, no deberíamos utilizar $Y$ como base para nuestras decisiones, en lugar de ello, deberíamos buscar una función de variables que no sean causadas por $A$ pero que puedan predecir $Y$, a este conjunto de funciones lo denotaremos por $Y_{\perp A}$. Definiremos entonces $\hat{Y}$ como una estimación de la $Y_{\perp A}$ más ''cercana'' a $Y$ usando como guía alguna función de riesgo. Esto hace que la incompatibilidad entre el criterio de igualdad de oportunidades y paridad predictiva sea irrelevante, ya que $A$ e $Y_{\perp A}$ serán independientes por como las hemos construido dadas las suposiciones del modelo.

\subsection*{Tratamiento de los prejuicios históricos y la paradoja de la equidad existente}

La diferencia explícita entre $\hat{Y}$ e $Y$ nos permite abordar los sesgos históricos. Por ejemplo, supongamos que $Y$ es un indicador de si un cliente no devuelve un préstamo, mientras que $\hat{Y}$ es la decisión real de conceder el préstamo. Consideremos el grafo causal que se muestra en la Figura \ref{fig:grafotratam} con la inclusión explícita del conjunto $U$ de variables de ruido independientes. En principio, $Y$ es la medida objetivamente ideal para la toma de decisiones, ya que indica si el cliente dejó de pagar o no el préstamo. Si $A$ es un atributo protegido, entonces el clasificador definido por $\hat{Y} = Y = f_Y (A, U)$ no satisface la equidad contrafactual, siendo la flecha $A \to Y$ el resultado de un mundo que perjudica a los individuos de una manera que está fuera de su control. Por tanto, el principio de equidad contrafactual nos prohíbe utilizar $Y$.\\

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode, fill={rgb:black,1;white,2}] (z) {A};
\node[mynode,right=of z, fill={rgb:black,1;white,2}] (x) {Y};
\node[mynode,left=of z] (y) {$U_A$};
\node[mynode,right=of x] (w) {$U_Y$};

\path (x) edge[latex-] (z);
\path (y) edge[-latex] (z);
\path (x) edge[latex-] (w);

\end{tikzpicture}
\caption{Grafo causal donde $A$ es un atributo sensible e $Y$ un resultado de interés.}
\label{fig:grafotratam}
\end{figure}

Por el contrario, cualquier función de variables no descendientes de $A$ puede utilizarse como base para la toma de decisiones justas. Esto significa que cualquier variable $\hat{Y}$ definida por $\hat{Y} = g(U)$ satisface la equidad contrafactual para cualquier función $g(\cdot)$. Por lo tanto, dado un modelo causal, el funcional definido por $g(\cdot)$ que minimiza algún error de predicción para $Y$ cumplirá la equidad contrafactual, como se propone en la Sección \ref{sec:algoritmo}. En esencia, estamos aprendiendo una proyección de $Y$ en el espacio de las decisiones justas, eliminando de paso los posibles sesgos históricos existentes.

\subsection*{Defectos y limitaciones}

El concepto de equidad contrafactual, teóricamente hablando, puede parecer una buena idea para eliminar todos los defectos que surgen del resto de criterios estudiados. En la práctica, la dependencia de conceptos como la inferencia causal o el estudio de contrafactuales lo hacen una de las nociones de equidad más complejas de implementar. 

Otro problema surge cuando queremos acordar cómo debería ser el grafo causal o decidir qué características vamos a utilizar incluso disponiendo de dicho grafo. Esto se debe a que podríamos perder precisión si rechazamos variables de estudio que pudiesen ser problemáticas a la hora de realizar el estudio causal.

\ctparttext{\color{black}\begin{center}
Elaboración de un ejemplo práctico sobre la equidad contrafactual y su discusión frente a otras nociones de equidad estudiadas.
\end{center}}
\part{Análisis experimental}  \label{part:analisis_exp}


\chapter{Descripción y diseño}

\label{ch:descdise}

En este capítulo se describirá un problema real de justicia en el ámbito de la educación y se propondrán varios diseños de modelos causales que puedan ser tratados por un algoritmo de equidad contrafactual con el objetivo de eliminar el tratamiento dispar en el problema.

\section{Algoritmo de aprendizaje justo}

\label{sec:algoritmo}

El algoritmo propuesto por \cite{counterfactual2018} parte de la necesidad de relacionar $\hat{Y}$ con $Y$. Para ello restringiremos $\hat{Y}$ para que funcione como una función parametrizada de los nodos no descendientes de $A$ apoyándonos en el Lema \ref{lem:nodescent}. 

Calculemos la predicción $\hat{Y}$ a partir de un clasificador parametrizado por $\btheta$ al que denominaremos como $g_{\btheta}(U,X_{\not \succ A})$, donde $X_{\not \succ A} \subseteq X$ denota el conjunto de no descendientes de $A$. Dada una función de pérdida $l(\cdot,\cdot)$ (\textit{squared} o \textit{logistic loss}) y un conjunto de datos $\mathcal{D}=\{(A^{(i)},X^{(i)},Y^{(i)}) \ : \ i=1,\dots,n\}$. Definimos $L(\btheta)=\displaystyle \frac{1}{n} \sum_{i=1}^n \mathbb{E}[l(y^{(i')},g_{\btheta}(U^{(i')},x_{\not \succ A}^{(i')}))\mid x^{(i)},a^{(i)}]$ como la pérdida empírica al minimizar sobre $\btheta$. Cada esperanza es respecto a la variable $U^{(i)} \sim P_{\mathcal{M}}(U \mid x^{(i)},a^{(i)})$ donde $P_{\mathcal{M}}(U \mid x,a)$ es la distribución condicional de las variables de ruido dada por el modelo causal $\mathcal{M}$. En tiempo de predicción, crearemos una nueva variable $\tilde{Y}=\mathbb{E}[\hat{Y}(U^\ast,x_{\not \succ A}^\ast)\mid x^\ast, a^\ast]$ para un nuevo elemento del conjunto de datos $(a^\ast,x^\ast)$. 

Para calcular la esperanza, utilizaremos el \textit{método de Monte Carlo basado en cadenas de Markov} (MCMC) para aproximarla (\cite{mcmc2003}). Los métodos MCMC son una clase de algoritmos de simulación para el muestreo y estimación de distribuciones de probabilidad a posteriori. Al construir un cadena de Markov usando una distribución deseada como distribución de equilibrio de la cadena, se puede hacer un muestreo de la distribución registrando los diferentes estados del grafo. Cuantas más iteraciones realicemos con el método de Monte Carlo, la distribución de la cadena, se acercará más a la distribución deseada real.

\begin{algorithm}[h]
\caption{FairLearning($\mathcal{D},\mathcal{M}$)}
\label{alg:fairlearn}
    \KwIn{$\mathcal{D}$, conjunto de datos y $\mathcal{M}$, modelo causal.}
    \KwOut{parámetros aprendidos $\htheta$.}
    \For{$ i\in \mathcal{D}$}{
    muestrear $m$ ejemplos MCMC $U_1^{(i)},\dots,U_m^{(i)} \sim P_{\mathcal{M}}(U \mid x^{(i)},a^{(i)})$.
    }
    Creamos $\mathcal{D}'$ donde cada punto $(a^{(i)},x^{(i)},y^{(i)})$ en $\mathcal{D}$ es sustituido por los 
    
    correspondientes $m$ puntos $\{(a^{(i)},x^{(i)},y^{(i)},u_j^{(i)})\}$\;
    $\htheta \leftarrow \text{argmin}_{\btheta} \sum_{i'\in \mathcal{D}'} l(y^{(i')},g_{\btheta}(U^{(i')},x_{\not \succ A}^{(i')}))$\;
   \textbf{devolver} $g_{\htheta}$\;
\end{algorithm}

\section{Diseño del modelo causal de entrada}

\label{sec:modeloentrada}

El modelo $\mathcal{M}$ debe proporcionarse al Algoritmo \ref{alg:fairlearn}. Recordemos que los modelos causales requieren de fuertes suposiciones cuando se hacen afirmaciones contrafactuales (\cite{fairnesslearning2019}). Existen infinitas ecuaciones estructurales compatibles con la misma distribución observable por lo que, teóricamente estos modelos deberían ser propensos a modificaciones si, por ejemplo se incorporan nuevos datos anteriormente no observados que contradijesen el modelo actual.

En nuestro trabajo, no será necesario especificar un modelo totalmente determinista y relajaremos las ecuaciones estructurales a partir de la definición de distribución condicional. En particular, el concepto de equidad contrafactual es válido bajo los siguientes tres niveles que vienen especificados en el trabajo de \cite{counterfactual2018} como:

\begin{itemize}
    \item \textbf{Nivel 1}: predecir $\hat{Y}$ usando sólo las variables observables no descendientes de $A$. Esta consideración no requiere de ninguna suposición causal, pero en la mayoría de los problemas la mayor parte de las variables observables serán descendientes de atributos protegidos y por tanto tendremos menos información manejable.
    \item \textbf{Nivel 2}: suponer variables de ruido que actúan como causas no deterministas de las variables observables, basadas en el conocimiento explícito del dominio y en algoritmos de aprendizaje. La información sobre $X$ pasará a $\hat{Y}$ a través de $P(U \mid x, a)$.
    \item \textbf{Nivel 3:} construir un modelo totalmente determinista con variables de ruido. Por ejemplo, la distribución $P(V_i \mid pa_i)$ puede tratarse como un modelo de error aditivo, $V_i =f_i(pa_i)+\epsilon_i$ (\cite{noise2014}). El término de error $\epsilon_i$ sirve como una entrada a $\hat{Y}$ calculada a partir de las variables observadas. Esto maximizará la información extraída por el clasificador.
\end{itemize}

\section{Aplicación en un problema real}

Ilustraremos la aplicación de justicia contrafactual sobre un problema del mundo real que requiere equidad. El objetivo de este experimento es cuantificar el comportamiento del Algoritmo \ref{alg:fairlearn} con tamaños de muestra finitos sobre una suposición real compatible con un modelo sintético.

\subsection{Descripción del problema}

El Consejo de Admisión de las Facultades de Derecho realizó una encuesta en 163 facultades de Derecho de Estados Unidos (\cite{lsac1998}). Contiene información sobre 21.790 estudiantes de Derecho, tales como las puntuaciones de su examen de acceso (LSAT), su media del expediente (GPA) antes de entrar en la facultad, y su nota media del primer año (FYA) en la carrera de Derecho.

A partir de estos datos, una escuela podría querer predecir si un solicitante tendrá un FYA alto. También podría ser interesante asegurarse de que estas predicciones no están sesgadas por la raza y el sexo del individuo. Sin embargo, es bastante probable que los resultados del LSAT, GPA y FYA estén sesgados por factores sociales. Nuestro trabajo consistirá en aplicar las herramientas aprendidas para equidad contrafactual en diversos escenarios y comparar su actuación con el rendimiento de otras nociones de equidad estudiadas.

\subsection{Escenarios de predicción}

Utilizaremos el mismo escenario de experimentación que el propuesto en \cite{counterfactual2018}. Dividiremos el conjunto de datos en un 80-20 (entrenamiento-test) para evaluar los modelos, preservando el equilibrio de las etiquetas. Para predecir los resultados utilizaremos un predictor basado en regresión lineal y mediremos la exactitud alcanzada por cada modelo a con la métrica RMSE.  

Según hemos descrito en la Sección \ref{sec:modeloentrada}, existen tres aproximaciones a partir de las cuales podemos construir un predictor de FYA que satisfaga la equidad contrafactual: 

\begin{itemize}
    \item \textbf{Nivel 1}: usaremos cualquier característica que no sea descendiente de la raza y el sexo para la predicción. Como creemos que el LSAT, el GPA y el FYA están sesgados por la raza y el sexo, no podremos utilizar ninguna de las características observadas para construir un clasificador justo contrafactual.
    
    \item \textbf{Nivel 2} (\textit{Fair K}): supondremos que una variable de ruido: los conocimientos del estudiante (K), afecta a las puntuaciones de GPA, LSAT y FYA. El gráfico causal correspondiente a este modelo se muestra en la Figura \ref{fig:practicausales}. Emplearemos las siguientes distribuciones:
    \begin{equation*}
    \begin{split}
        \text{GPA} &\sim \mathcal{N}(b_G+K \nm{w}^K_G+[A_R,A_S] \nm{w}^A_G, \sigma_G),\\
        \text{LSAT} &\sim Poisson(\text{exp}(b_L+K \nm{w}^K_L +[A_R,A_S] \nm{w}^A_L)),\\ 
        \text{FYA}&\sim \mathcal{N}(K \nm{w}^K_F+[A_R,A_S] \nm{w}^A_F, 1),\\
        \text{K}&\sim \mathcal{N}(0,1).
    \end{split}
    \end{equation*}
    Realizamos la inferencia sobre este modelo utilizando un conjunto de entrenamiento observado para estimar la distribución posterior de K.
    
    \item \textbf{Nivel 3} (\textit{Fair Add}): modelaremos las puntuaciones de GPA, LSAT y FYA como variables continuas con términos de error aditivos independientes de la raza y el sexo. Estimamos los términos de error $\epsilon_G$, $\epsilon_L$ ajustando primero dos modelos que utilizan la raza y el sexo para predecir individualmente el GPA y LSAT. A continuación, calculamos los residuos de cada modelo (por ejemplo, $\epsilon_G =\text{GPA}-\hat{Y}_{\text{GPA}}(A_R, A_S)$). Finalmente, usaremos las estimaciones residuales de $\epsilon_G$, $\epsilon_L$ para predecir FYA. Este modelo se muestra en la Figura \ref{fig:practicausales}. En este caso las distribuciones vienen dadas por:
    \begin{equation*}
    \begin{split}
        \text{GPA} &= b_G+[A_R,A_S] \nm{w}^A_G +\epsilon_G, \ \ \epsilon_G \sim P(\epsilon_G)\\
        \text{LSAT} &= b_L+[A_R,A_S] \nm{w}^A_L +\epsilon_L, \quad \epsilon_L \sim P(\epsilon_L),\\
        \text{FYA}&= b_F+[A_R,A_S] \nm{w}^A_F +\epsilon_F, \quad \epsilon_F \sim P(\epsilon_F).
    \end{split}
    \end{equation*}
\end{itemize}\

\begin{figure}[h]
	\centering
	\includegraphics[width=10.8cm]{causales_pract.png}
	\caption{Grafos causales de los escenarios para los niveles 2 y 3, respectivamente.}
    \label{fig:practicausales}
\end{figure}

Compararemos el escenario propuesto por la equidad contrafactual con dos líneas de base injustas: 

\begin{itemize}
    \item \textbf{Total}: utilizará todas las características disponibles para el individuo, incluidos los atributos sensibles.
    \item \textbf{Desconocimiento}: aplicaremos la noción de equidad por desconocimiento discutida en la Sección \ref{sec:eqdesconocimiento}.
\end{itemize} 

\chapter{Implementación y resultados}

En este capítulo explicaremos los procedimientos empleados en la implementación de los diseños y algoritmos definidos en el capítulo anterior. Todo lo relativo a implementación se encuentra disponible en: \href{https://github.com/danibolanos/TFG-Guarantee_Fairness_in_ML.git}{danibolanos/TFG-Guarantee\_Fairness\_in\_ML}

\section{Obtención y tratamiento de los datos}

El conjunto de datos ha sido extraído del repositorio Ethik, que contiene diferentes conjuntos de datos útiles para el estudio de equidad en aprendizaje automático y que está disponible en: \url{https://github.com/XAI-ANITI/ethik/tree/master/ethik/data}.

Es recomendable que antes de aplicar cualquier modelo o algoritmo sobre un conjunto de datos, se realice un preprocesamiento sobre estos. En nuestro caso, haciendo uso de la biblioteca Pandas (\url{https://pandas.pydata.org/docs/}) para Python, hemos realizado las siguientes modificaciones sobre el conjunto de datos original:

\begin{itemize}
    \item Categorizar cada valor del atributo raza de manera que obtengamos una columna para cada tipo de raza que tome el valor 1 o 0 para cada individuo, indicando la pertenencia o no del mismo a la raza indicada por la columna en concreto (\textit{get\_dummies()}).
    \item Sustituir el atributo sexo por dos columnas ('Male'-'Female') que indiquen con 1 o 0 la característica del individuo concreto.
    \item Discretizar el valor de 'LSAT' (convertir cada valor a tipo entero).
\end{itemize}

\section{Implementación del código}

El lenguaje de programación seleccionado para la implementación del proyecto ha sido Python (\url{https://www.python.org/}) en su versión 3.8.5. La elección de Python se debe a que es un lenguaje muy popular en el ámbito de la ciencia de datos, con una gran cantidad de bibliotecas útiles para la visualización de datos y altamente compatible con otros lenguajes de programación que nos pueden ser útiles tanto como para el tratamiento de datos como para la construcción de modelos causales.

\section{Contraste de los resultados}

\section{Condiciones de la experimentación}

\subsection{Entorno de ejecución}

Los experimentos realizados en este capítulo se han ejecutado en un equipo con las siguientes características:

\begin{itemize}
    \item Arquitectura x86\_64.
    \item AMD Ryzen 7 4800H with Radeon Graphics 2.90 GHz.
    \item 8 núcleos con 2 hilos de procesamiento por núcleo.
    \item 16 GB RAM DDR4.
    \item Sistema Operativo: Ubuntu 20.04.2 LTS.
\end{itemize}

\subsection{Entorno de programación}

Spyder.

\subsection{Bibliotecas y herramientas auxiliares}

Algunas de las bibliotecas más importantes utilizadas junto al número de sus versiones han sido:

\begin{itemize}
    \item Pandas 1.2.4
    \item Numpy 1.19.2
    \item Scikit-learn 0.24.2
    \item \textcolor{red}{pickle}, %path 15.0.0.
\end{itemize}

Además se ha utilizado el módulo PyStan 2.19.1.1 (\url{https://pystan.readthedocs.io/en/latest}) que funciona como una interfaz de Python para el lenguaje de programación probabilística Stan (\url{https://mc-stan.org}). El cual permite generar muestras de datos a partir de métodos que trabajan con cadenas de Markov (como MCMC) y crear modelos causales sobre los que operar gracias a la inferencia estadística Bayesiana.

\section{Manual de ejecución del experimento}


\ctparttext{\color{black}\begin{center}
Conclusiones extraídas a lo largo del desarrollo del proyecto y debate de posibles rutas de trabajo futuras para el mismo.
\end{center}}
\part{Conclusiones y Vías Futuras} \label{part:debate_fut}

\chapter{Conclusión}

\textcolor{red}{conclusiones obtenidas del trabajo en general}

\chapter{Trabajos futuros}

\textcolor{red}{Equidad individual se ha quedado sin tratar, trabajo futuro para TFM de matemáticas estudio de clase de funciones que puedan ser interesantes para implementar la métrica de distancia entre individuos}

\textcolor{red}{Pensar si incluir el esquema y prototipo en este apartado o en uno nuevo. Preguntar!}

\begin{minted}[frame=lines]{python}
import numpy as np
import cv2
from matplotlib import pyplot as plt
import math

import library

def leeimagen(filename, flagColor):
    im = cv2.imread(filename)
  return im
\end{minted}



\ctparttext{\color{black}\begin{center}
\end{center}}
\part*{\textsc{Apéndices}}

\appendix
\chapter{Herramientas para garantizar justicia en AA}\label{ap:herr_just}
\input{appendixes/A}

\chapter{Estimación del coste y planificación}\label{ap:coste_plan}
\input{appendixes/B}

\addtocontents{toc}{\vspace{1\baselineskip}}

\clearpage

\printglossary[title={\textsc{notación}}]
\glsaddallunused

\addtocontents{toc}{\vspace{1\baselineskip}}

\clearpage

% Añade sección de referencias al final del documento.
% Selecciona un estilo de cita.
\bibliographystyle{plainnat}
% En research.bib están las entradas de los artículos que citamos.
% Podemos cambiar el nombre del archivo aquí.
\bibliography{bibliography/research.bib}   

\addcontentsline{toc}{chapter}{\textsc{bibliografía}}


\end{document}