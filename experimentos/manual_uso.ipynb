{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa7ca17",
   "metadata": {},
   "source": [
    "# Manual de uso para la ejecución de los experimentos\n",
    "\n",
    "**Trabajo Fin de Grado**: Herramientas para garantizar justicia en aprendizaje automático\n",
    "\n",
    "**Autor**: Daniel Bolaños Martínez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f13dfa",
   "metadata": {},
   "source": [
    "Importaremos las bibliotecas utilizadas a lo largo del experimento, entre las cuales se encuentran:\n",
    "\n",
    "* Pandas 1.2.4\n",
    "* NumPy 1.19.2\n",
    "* Scikit-learn 0.24.2\n",
    "* matplotlib 3.4.3\n",
    "* seaborn 0.11.2\n",
    "* Aequitas 0.42.0\n",
    "* pathlib2 2.3.6\n",
    "* Módulos usados incluidos en Python: pickle, copy, os.\n",
    "\n",
    "Además se ha utilizado el módulo PyStan 2.19.1.1 que funciona como una interfaz de Python para el lenguaje de programación probabilística Stan (https://mc-stan.org). El cual permite generar muestras de datos a partir de métodos que trabajan con cadenas de Markov (como MCMC) y crear modelos causales sobre los que operar gracias a la inferencia estadística Bayesiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e3ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pystan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.fairness import Fairness\n",
    "from aequitas.plotting import Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fad8fb1",
   "metadata": {},
   "source": [
    "## Modelos de equidad contrafactual\n",
    "\n",
    "*Basado en el trabajo de Counterfactual Fairness de Kusner et al. (2018)*\n",
    "\n",
    "**Descripción del conjunto de datos utilizado**: El Consejo de Admisión de las Facultades de Derecho realizó una encuesta en 163 facultades de Derecho de Estados Unidos. Contiene información sobre 21.790 estudiantes de Derecho, tales como las puntuaciones de su examen de acceso (LSAT), su media del expediente (GPA) antes de entrar en la facultad, y su nota media del primer año (FYA) en la carrera de Derecho. A partir de estos datos, una escuela querría predecir si un solicitante tendrá un FYA alto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e980d53",
   "metadata": {},
   "source": [
    "### Implementación y diseño de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0946c179",
   "metadata": {},
   "source": [
    "En primer lugar, definiremos aquellas funciones que se utilizarán para el tratamiento de los datos:\n",
    "\n",
    "La función $\\texttt{preprocess_data}$ realiza las siguientes funciones:\n",
    "\n",
    "* Categorizar cada valor del atributo raza obteniendo una columna para que cada tipo de raza, tome el valor 1 o 0 para cada individuo, señalando la pertenencia o no del mismo a la raza indicada por la columna específica $\\texttt{get_dummies}$.\n",
    "* Sustituir el atributo sexo por dos columnas ('Male'-'Female') que indiquen con 1 o 0 la característica del individuo concreto.\n",
    "* Discretizar el valor de 'LSAT' (convertir cada valor a tipo entero).\n",
    "* Agrupar los individuos de raza mexicana y puertoriqueña en la raza hispánica, y los de raza amerindia en otras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e379d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento del conjunto de datos 'law_data.csv'\n",
    "def preprocess_data(data, protected_attr):    \n",
    "\n",
    "    data['race'] = data['race'].apply(lambda a: 'Hispanic' if a == 'Mexican' else a)\n",
    "    data['race'] = data['race'].apply(lambda a: 'Hispanic' if a == 'Puertorican' else a)\n",
    "    data['race'] = data['race'].apply(lambda a: 'Other' if a == 'Amerindian' else a)\n",
    "    data['first_pf'] = data['first_pf'].apply(lambda a: 0 if a == 0.0 else 1)\n",
    "    \n",
    "    # Convertimos la columna 'LSAT' a tipo entero\n",
    "    data['LSAT'] = data['LSAT'].apply(lambda a: int(a))\n",
    "    \n",
    "    # Creamos una columna que indique con 0 o 1 la pertenencia al sexo Masculino o Femenino\n",
    "    data['Female'] = data['sex'].apply(lambda a: 1 if a == 1 else 0)\n",
    "    data['Male'] = data['sex'].apply(lambda a: 1 if a == 2 else 0)\n",
    "    \n",
    "    # Creamos una columna que indique con 0 o 1 la pertenencia al sexo Masculino o Femenino\n",
    "    data['sex'] = data['sex'].apply(lambda a: 'Male' if a == 2 else 'Female')\n",
    "\n",
    "    # Realizamos una división 80-20 de los conjuntos de entrenamiento y test\n",
    "    train, test = train_test_split(data, random_state = 76592621, train_size = 0.8);\n",
    "    \n",
    "    train_orig = copy.deepcopy(train)\n",
    "    test_orig  = copy.deepcopy(test)\n",
    "    \n",
    "    # Separamos cada atributo de raza en una columna con 1 si el individuo \n",
    "    # pertenece a ella o 0 si el individuo no pertenece\n",
    "    train = pd.get_dummies(train, columns=['race'], prefix='', prefix_sep='')\n",
    "    test = pd.get_dummies(test, columns=['race'], prefix='', prefix_sep='')\n",
    "    train = train.drop(['sex'], axis=1)  \n",
    "    test = test.drop(['sex'], axis=1)  \n",
    "    \n",
    "    # Creamos un diccionario compatible con pystan para los conjuntos creados anteriormente\n",
    "    dic_train = create_pystan_dic(train, protected_attr)\n",
    "    dic_test = create_pystan_dic(test, protected_attr)\n",
    "\n",
    "    return dic_train, dic_test, train_orig, test_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43feb6",
   "metadata": {},
   "source": [
    "Además, utilizaremos la función $\\texttt{create_pystan_dic}$ para crear un diccionario que contendrá las siguientes características del conjunto de datos:\n",
    "\n",
    "* **N** - Número total de ejemplos para ese diccionario.\n",
    "* **C** - Número de categorías diferentes de atributos sensibles.\n",
    "* **A** - Array con el contenido de cada una de las filas de los atributos protegidos.\n",
    "* **GPA** - Array con los valores de UGPA.\n",
    "* **LSAT** - Array con los valores de LSAT.\n",
    "* **FYA** - Array con los valores de ZFYA.\n",
    "\n",
    "Esta elección la realizamos, ya que el método StanModel, que construye el modelo causal, trabaja con diccionarios en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b37e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un diccionario compatible con pystan para el conjunto de datos y los atributos protegidos dados\n",
    "def create_pystan_dic(data, protected_attr, train=True):\n",
    "    dic_data = {}\n",
    "    dic_data['N'] = len(data)\n",
    "    dic_data['C'] = len(protected_attr)\n",
    "    dic_data['A'] = np.array(data[protected_attr])\n",
    "    dic_data['GPA'] = list(data['UGPA'])\n",
    "    dic_data['LSAT'] = list(data['LSAT'])\n",
    "    if train:\n",
    "        dic_data['FYA'] = list(data['ZFYA'])\n",
    "    \n",
    "    return dic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503a398",
   "metadata": {},
   "source": [
    "Para la evaluación posterior de cada modelo, leeremos el conjunto de datos original y trataremos los datos tal y cómo hemos explicado previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88de5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    data = pd.read_csv('./datos/law_data.csv', index_col=0)    \n",
    "    \n",
    "    # Guardamos en un vector todos los atributos protegidos\n",
    "    protected_attr = ['Asian','Black','Hispanic','Other','White','Male','Female']\n",
    "\n",
    "    # Obtiene en un diccionario el conjunto de datos y en una partición 80 (train) 20 (test)\n",
    "    dic_train, dic_test, train, test = preprocess_data(data, protected_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47ad8bf",
   "metadata": {},
   "source": [
    "#### Modelo Completo\n",
    "\n",
    "Utilizará todas las características disponibles para el individuo, incluidos los atributos sensibles. Usaremos los datos de entrenamiento relativos a los atributos protegidos y los valores de GPA y LSAT. Construiremos el modelo completo, y usaremos el regresor lineal para entrenarlo. Una vez hecho esto, podremos calcular las predicciones sobre el conjunto de datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e671cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Completo: usa todos los atributos para la predicción\n",
    "def mod_full(dic_train, dic_test):\n",
    "    # Construcción de los conjuntos de entrenamiento y tests para el modelo\n",
    "    x_train = np.hstack((dic_train['A'], np.array(dic_train['GPA']).reshape(-1,1), \n",
    "                         np.array(dic_train['LSAT']).reshape(-1,1)))\n",
    "    x_test = np.hstack((dic_test['A'], np.array(dic_test['GPA']).reshape(-1,1), \n",
    "                        np.array(dic_test['LSAT']).reshape(-1,1)))\n",
    "    y = dic_train['FYA']\n",
    "\n",
    "    # Entrenamiento del modelo sobre el conjunto x_train\n",
    "    lr_full = LinearRegression()\n",
    "    lr_full.fit(x_train, y)\n",
    "    \n",
    "    # Predicción de las etiquetas sobre el conjunto x_test\n",
    "    preds_test = lr_full.predict(x_test)\n",
    "    \n",
    "    # Predicción de las etiquetas sobre el conjunto x_train\n",
    "    preds_train = lr_full.predict(x_train)\n",
    "    \n",
    "    # Cálculo del score sobre el conjunto x_test\n",
    "    score = lr_full.score(x_test, dic_test['FYA'])\n",
    "\n",
    "    return preds_test, preds_train, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0e90d",
   "metadata": {},
   "source": [
    "Ejecutamos el modelo sobre los datos tal y como hemos detallado. Finalmente calculamos el valor de RMSE y el coeficiente de determinación ($R^2$) para el modelo en cuestión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d93164",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Obtiene las predicciones para el modelo\n",
    "    preds_full_test, preds_full_train, score_full = mod_full(dic_train, dic_test)\n",
    "    # Imprime los valores de RMSE y score resultantes\n",
    "    print('Full RMSE: %.4f' % np.sqrt(mean_squared_error(preds_full_test, dic_test['FYA'])))\n",
    "    print('Full score: %.4f' % score_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd6703",
   "metadata": {},
   "source": [
    "#### Modelo por Desconocimiento\n",
    "\n",
    "Aplicaremos la noción de equidad por desconocimiento discutida en el proyecto. Usaremos los valores de LSAT y GPA del conjunto de entrenamiento para entrenar el modelo. Finalmente, comprobaremos su actuación sobre el conjunto de datos de prueba. En este caso, no utilizamos los atributos protegidos en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055758e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo equidad por desconocimiento: no usa los atributos sensibles en predicción\n",
    "def mod_unaware(dic_train, dic_test):\n",
    "    # Construcción de los conjuntos de entrenamiento y tests para el modelo\n",
    "    x_train = np.hstack((np.array(dic_train['GPA']).reshape(-1,1), \n",
    "                         np.array(dic_train['LSAT']).reshape(-1,1)))\n",
    "    x_test = np.hstack((np.array(dic_test['GPA']).reshape(-1,1), \n",
    "                        np.array(dic_test['LSAT']).reshape(-1,1)))\n",
    "    y = dic_train['FYA']\n",
    "    \n",
    "    # Entrenamiento del modelo sobre el conjunto x_train\n",
    "    lr_unaware = LinearRegression()\n",
    "    lr_unaware.fit(x_train, y)\n",
    "    \n",
    "    # Predicción de las etiquetas sobre el conjunto x_test\n",
    "    preds_test = lr_unaware.predict(x_test)\n",
    "    \n",
    "    # Predicción de las etiquetas sobre el conjunto x_train\n",
    "    preds_train = lr_unaware.predict(x_train)\n",
    "    \n",
    "    # Cálculo del score sobre el conjunto x_test\n",
    "    score = lr_unaware.score(x_test, dic_test['FYA'])\n",
    "\n",
    "    return preds_test, preds_train, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e907e12",
   "metadata": {},
   "source": [
    "Ejecutamos el modelo y calculamos el valor de RMSE y $R^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58a727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Obtiene las predicciones para el modelo\n",
    "    preds_unaware_test, preds_unaware_train, score_unaware = mod_unaware(dic_train, dic_test)\n",
    "    # Imprime los valores de RMSE y score resultantes\n",
    "    print('Unaware RMSE: %.4f' % np.sqrt(mean_squared_error(preds_unaware_test, dic_test['FYA'])))\n",
    "    print('Unaware score: %.4f' % score_unaware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28ece7a",
   "metadata": {},
   "source": [
    "#### Modelo de variable latente (*Fair K*)\n",
    "\n",
    "Supondremos que una variable de ruido: los conocimientos del estudiante (*K*), afecta a las puntuaciones de GPA, LSAT y FYA. Emplearemos las siguientes distribuciones:\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\text{GPA} &\\sim \\mathcal{N}(b_G+K \\textbf{w}^K_G+[A_R,A_S] \\textbf{w}^A_G, \\sigma_G),\\\\\n",
    "        \\text{LSAT} &\\sim Poisson(\\text{exp}(b_L+K \\textbf{w}^K_L +[A_R,A_S] \\textbf{w}^A_L)),\\\\ \n",
    "        \\text{FYA}&\\sim \\mathcal{N}(K \\textbf{w}^K_F+[A_R,A_S] \\textbf{w}^A_F, 1),\\\\\n",
    "        \\text{K}&\\sim \\mathcal{N}(0,1).\n",
    "    \\end{split}\n",
    "    \\end{equation*}\n",
    "    \n",
    "Realizamos la inferencia sobre este modelo utilizando un conjunto de entrenamiento observado para estimar la distribución posterior de $K$.\n",
    "\n",
    "Para la creación de este modelo, definiremos las siguientes funciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404c261",
   "metadata": {},
   "source": [
    "Obtenemos las muestras para un modelo Stan con los datos contenidos en el diccionario. Construiremos el modelo usando la función $\\texttt{StanModel}$ de la biblioteca PyStan, lo entrenaremos haciendo uso del método $\\texttt{sampling}$ y finalmente extraeremos las muestras con $\\texttt{extract}$. Usaremos 2000 iteraciones y 1 cadena de Markov para replicar el ejemplo de Kusner. Para evitar ejecutar el modelo cada vez, guardaremos los modelos entrenados usando las funciones $\\texttt{load}$ y $\\texttt{dump}$ del módulo $\\texttt{pickle}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e228eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def MCMC(dic_post, path_model, path_stan):\n",
    "        model_fit = Path(path_model)\n",
    "\n",
    "        # Comprobamos si ya existe un archivo con el modelo entrenado\n",
    "        if model_fit.is_file():\n",
    "            file = open(path_model, \"rb\")\n",
    "            fit_samples = pickle.load(file)\n",
    "        else:\n",
    "            # Obtiene muestras desde cero a partir del modelo pasado como parámetro\n",
    "            model = pystan.StanModel(file = path_stan)\n",
    "            fit_data = model.sampling(data = dic_post, seed=76592621, chains=1, iter=2000)\n",
    "            fit_samples = fit_data.extract()\n",
    "            # Guardamos el modelo entrenado\n",
    "            file = open(path_model, \"wb\")\n",
    "            pickle.dump(fit_samples, file, protocol=-1)\n",
    "\n",
    "        return fit_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c12db",
   "metadata": {},
   "source": [
    "Creamos un diccionario con la media de los parámetros obtenidos en el entrenamiento previo del modelo, almacenados en $\\texttt{samples}$. Mantendremos la estructura del diccionario para los datos de prueba para los parámetros que no varíen como son $[N, C, A, GPA, LSAT]$ y haremos la media para el resto de parámetros que no dependan de FYA, a saber $[\\textbf{w}^K_G, \\textbf{w}^A_G, \\textbf{w}^K_L, \\textbf{w}^A_L, sigma_G, b_G, b_L]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_mean_param(samples, dic):\n",
    "        dic_data = {}\n",
    "        # Añadimos los parámetros comunes que comparte con el diccionario original    \n",
    "        param_base = ['N', 'C', 'A', 'GPA', 'LSAT']\n",
    "        for param in param_base:\n",
    "            dic_data[param] = dic[param]\n",
    "\n",
    "        # Guardamos la media del vector de valores para los parámetros que utiliza el modelo '*only_k.stan'\n",
    "        for param in samples.keys():\n",
    "            if param not in ['K', 'wK_F', 'wA_F', 'sigma2_G', 'lp__']:\n",
    "                dic_data[param] = np.mean(samples[param], axis=0)\n",
    "\n",
    "        return dic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6608510",
   "metadata": {},
   "source": [
    "Utilizamos el método $\\texttt{MCMC}$ sobre el modelo $\\texttt{law_school_train.stan}$ para obtener las muestras para cada punto del conjunto de datos de entrenamiento. Guardamos la media de la variable $K$ para train. Usamos la distribución de $K$ aprendida y hacemos las medias del resto de variables para el conjunto de prueba. Volvemos a inferir sobre el modelo esta vez usando $\\texttt{law_school_only_k.stan}$ para el conjunto de prueba. Finalmente, calculamos la media de la variable $K$ para test y la devolveremos junto con la de train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3755ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def fair_learning(dic_train, dic_test):\n",
    "        modelos_dir = Path(\"./datos/modelos\")\n",
    "\n",
    "        # Comprobamos si ya existe un archivo con el modelo entrenado\n",
    "        if not modelos_dir.exists():\n",
    "            crear_borrar_directorio(modelos_dir, True)\n",
    "\n",
    "        # Entrenamos el modelo utilizando el MCMC y obtenemos las muestras para cada punto\n",
    "        train_samples = MCMC(dic_train, \"./datos/modelos/train_k_model.pkl\", \"./datos/stan/law_school_train.stan\")\n",
    "\n",
    "        # Obtenemos la media de la variable K para train\n",
    "        train_k = np.mean(train_samples[\"K\"], axis=0).reshape(-1, 1)\n",
    "\n",
    "        # Usamos la distribución de K aprendida y hacemos las medias del resto de variables para test\n",
    "        #dic_means = get_mean_params(train_samples, dic_test)\n",
    "        dic_means=dic_test\n",
    "\n",
    "        # Volvemos a inferir sobre el modelo esta vez usando el modelo sin FYA para test\n",
    "        test_samples = MCMC(dic_means, \"./datos/modelos/test_k_model.pkl\", \"./datos/stan/law_school_only_k.stan\")\n",
    "        # Obtenemos la media de la variable K para test\n",
    "        test_k = np.mean(test_samples[\"K\"], axis=0).reshape(-1, 1)\n",
    "\n",
    "        return train_k, test_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e91064",
   "metadata": {},
   "source": [
    "Usaremos los arrays con las medias de la variable $K$ inferida en el método anterior, para el conjunto de entrenamiento y el conjunto de prueba. Entrenaremos el modelo haciendo uso de la información contenida en $\\texttt{train_k}$ y las etiquetas reales de FYA. Finalmente usaremos el regresor lineal para calcular las predicciones sobre el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeed455",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Modelo no determinista: suponemos variable de ruido 'K' para generar la distribución del resto\n",
    "    def mod_fair_k(dic_train, dic_test):\n",
    "        train_k, test_k = fair_learning(dic_train, dic_test)\n",
    "        \n",
    "        # Construcción de los conjuntos de entrenamiento y tests para el modelo\n",
    "        x_train = train_k\n",
    "        x_test = test_k\n",
    "        y = dic_train['FYA']\n",
    "\n",
    "        # Entrenamiento del modelo sobre el conjunto x_train\n",
    "        lr_fair_k = LinearRegression()\n",
    "        lr_fair_k.fit(x_train, y)\n",
    "\n",
    "        # Predicción de las etiquetas sobre el conjunto x_test\n",
    "        preds_test = lr_fair_k.predict(x_test)\n",
    "\n",
    "        # Predicción de las etiquetas sobre el conjunto x_train\n",
    "        preds_train = lr_fair_k.predict(x_train)\n",
    "\n",
    "        # Cálculo del score sobre el conjunto x_test\n",
    "        score = lr_fair_k.score(x_test, dic_test['FYA'])\n",
    "\n",
    "        return preds_test, preds_train, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a64c918",
   "metadata": {},
   "source": [
    "Ejecutamos el modelo y calculamos el valor de RMSE y $R^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5cc927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Obtiene las predicciones para el modelo\n",
    "    preds_fair_k_test, preds_fair_k_train, score_fair_k = mod_fair_k(dic_train, dic_test)\n",
    "    # Imprime los valores de RMSE y score resultantes\n",
    "    print('Fair K RMSE: %.4f' % np.sqrt(mean_squared_error(preds_fair_k_test, dic_test['FYA'])))\n",
    "    print('Fair K score: %.4f' % score_fair_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce98817",
   "metadata": {},
   "source": [
    "#### Modelo de error aditivo (*Fair Add*)\n",
    "\n",
    "Modelaremos las puntuaciones de GPA, LSAT y FYA como variables continuas con términos de error aditivos independientes de la raza y el sexo. Estimamos los términos de error $\\epsilon_G$, $\\epsilon_L$ ajustando primero dos modelos que utilizan la raza y el sexo para predecir individualmente el GPA y LSAT. A continuación, calculamos los residuos de cada modelo (aplicando por ejemplo, $\\epsilon_G =\\text{GPA}-\\hat{Y}_{\\text{GPA}}(A_R, A_S)$). Finalmente, utilizaremos las estimaciones residuales de $\\epsilon_G$, $\\epsilon_L$ para predecir FYA. En este caso las distribuciones vienen dadas por:\n",
    "\n",
    "   \\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\text{GPA} &= b_G+[A_R,A_S] \\textbf{w}^A_G +\\epsilon_G, \\ \\ \\epsilon_G \\sim P(\\epsilon_G)\\\\\n",
    "        \\text{LSAT} &= b_L+[A_R,A_S] \\textbf{w}^A_L +\\epsilon_L, \\quad \\epsilon_L \\sim P(\\epsilon_L),\\\\\n",
    "        \\text{FYA}&= b_F+[A_R,A_S] \\textbf{w}^A_F +\\epsilon_F, \\quad \\epsilon_F \\sim P(\\epsilon_F).\n",
    "    \\end{split}\n",
    "    \\end{equation*}\n",
    "    \n",
    "Para la creación de este modelo, definiremos las siguientes funciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6619a8b9",
   "metadata": {},
   "source": [
    "Estima el error $\\epsilon$, entrenando el modelo utilizando el conjunto total de datos ($\\texttt{dic_train+dic_test}$) para una variable observada $\\texttt{var}$ usando además los atributos protegidos. Calculamos los residuos de cada modelo como $$\\epsilon_{\\texttt{var}} = \\texttt{var} - \\hat{Y}_\\texttt{var}\\text{(A)},$$ donde $\\texttt{var}$ puede tomar el valor de LSAT o GPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54441f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def calculate_eps(dic_train, dic_test, var):\n",
    "        # Reconstruimos el conjunto total para las variables que vamos a usar\n",
    "        data_a = np.vstack((dic_train['A'], dic_test['A']))\n",
    "        data_var = dic_train[var] + dic_test[var]\n",
    "\n",
    "        # Entrenamos un modelo para estimar el error para el parámetro var\n",
    "        lr_eps = LinearRegression()\n",
    "        lr_eps.fit(data_a, data_var)\n",
    "\n",
    "        # Calculamos los residuos de cada modelo como eps_var = var - Ŷ_var(A)\n",
    "        eps_train = dic_train[var] - lr_eps.predict(dic_train['A'])\n",
    "        eps_test = dic_test[var] - lr_eps.predict(dic_test['A'])\n",
    "\n",
    "        return eps_train, eps_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93177fb3",
   "metadata": {},
   "source": [
    "Definimos una función auxiliar para mostrar las distribuciones de las gráficas de los residuos para las variables GPA y LSAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44739a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Calcula los histogramas para los errores de cada variable calculada\n",
    "    def graph_eps(eps_train_G, eps_train_L, eps_test_G, eps_test_L):\n",
    "        eps_G = np.vstack((eps_train_G.reshape(-1,1), eps_test_G.reshape(-1,1)))\n",
    "        eps_L = np.vstack((eps_train_L.reshape(-1,1), eps_test_L.reshape(-1,1)))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(eps_G, color=\"red\", bins=100)\n",
    "        plt.title(\"$\\epsilon_{GPA}$\")\n",
    "        plt.xlabel(\"$\\epsilon_{GPA}$\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(eps_L, color=\"green\", bins=100)\n",
    "        plt.title(\"$\\epsilon_{LSAT}$\")\n",
    "        plt.xlabel(\"$\\epsilon_{LSAT}$\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcaa29f",
   "metadata": {},
   "source": [
    "Estimamos el error para GPA ($\\epsilon_\\text{GPA}$) y LSAT ($\\epsilon_\\text{LSAT}$) para el conjunto de entrenamiento utilizando el método previo. Usamos los valores de $\\epsilon_\\text{GPA}$ y $\\epsilon_\\text{LSAT}$ para entrenar el modelo. Finalmente, usaremos el regresor lineal para calcular las predicciones sobre el conjunto de datos de prueba. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146826e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Modelo determinista: añadimos términos de error aditivos independientes de los atributos protegidos\n",
    "    def mod_fair_add(dic_train, dic_test):\n",
    "        # Estimamos el error para GPA\n",
    "        eps_train_G, eps_test_G = calculate_eps(dic_train, dic_test, 'GPA')\n",
    "        # Estimamos el error para LSAT\n",
    "        eps_train_L, eps_test_L = calculate_eps(dic_train, dic_test, 'LSAT')\n",
    "\n",
    "        graph_eps(eps_train_G, eps_train_L, eps_test_G, eps_test_L)\n",
    "\n",
    "        x_train = np.hstack((eps_train_G.reshape(-1,1), eps_train_L.reshape(-1,1)))\n",
    "        x_test = np.hstack((eps_test_G.reshape(-1,1), eps_test_L.reshape(-1,1)))\n",
    "        y = dic_train['FYA']\n",
    "\n",
    "        # Entrenamiento del modelo usando los errores de train\n",
    "        lr_fair_add =  LinearRegression()\n",
    "        lr_fair_add.fit(x_train, y)\n",
    "\n",
    "        # Predicción de las etiquetas usando los errores para test\n",
    "        preds_test = lr_fair_add.predict(x_test)\n",
    "\n",
    "        # Predicción de las etiquetas usando los errores para test\n",
    "        preds_train = lr_fair_add.predict(x_train)\n",
    "\n",
    "        # Cálculo del score sobre el conjunto x_test\n",
    "        score = lr_fair_add.score(x_test, dic_test['FYA'])\n",
    "\n",
    "        return preds_test, preds_train, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a464801",
   "metadata": {},
   "source": [
    "Ejecutamos el modelo y calculamos el valor de RMSE y $R^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c126c255",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    # Obtiene las predicciones para el modelo\n",
    "    preds_fair_add_test, preds_fair_add_train, score_fair_add = mod_fair_add(dic_train, dic_test)\n",
    "    # Imprime los valores de RMSE y score resultantes\n",
    "    print('Fair Add RMSE: %.4f' % np.sqrt(mean_squared_error(preds_fair_add_test, dic_test['FYA'])))\n",
    "    print('Fair Add score: %.4f' % score_fair_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b7a28f",
   "metadata": {},
   "source": [
    "Finalmente mostraremos todos los resultados para el RMSE y el $R^2$ para los cuatro modelos definidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Imprime los valores de RMSE y score resultantes\n",
    "    print(\"Valores RMSE:\")\n",
    "    print('Full RMSE: %.4f' % np.sqrt(mean_squared_error(preds_full_test, dic_test['FYA'])))\n",
    "    print('Unaware RMSE: %.4f' % np.sqrt(mean_squared_error(preds_unaware_test, dic_test['FYA'])))\n",
    "    print('Fair K RMSE: %.4f' % np.sqrt(mean_squared_error(preds_fair_k_test, dic_test['FYA'])))\n",
    "    print('Fair Add RMSE: %.4f' % np.sqrt(mean_squared_error(preds_fair_add_test, dic_test['FYA'])))\n",
    "    print(\"\\nValores Coeficiente de determinación:\")\n",
    "    print('Full score: %.4f' % score_full)\n",
    "    print('Unaware score: %.4f' % score_unaware)\n",
    "    print('Fair K score: %.4f' % score_fair_k)\n",
    "    print('Fair Add score: %.4f' % score_fair_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b39a2",
   "metadata": {},
   "source": [
    "### Experimentación con Aequitas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682c06e",
   "metadata": {},
   "source": [
    "Aequitas (http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/) es una herramienta de auditoría desarrollada por el *Center for Data Science and Public Policy* de la Universidad de Chicago. Es una herramienta de código abierto que consta de diversas utilidades de soporte para la auditoría de sesgos creado para ser utilizado por analistas de todo tipo relacionados con el ámbito del aprendizaje automático y cuyo principal objetivo es auditar los modelos de *machine learning* con el fin de encontrar posibles discriminaciones en ellos y evitarlas en un futuro.\n",
    "\n",
    "Definimos una función para convertir los datos obtenidos a partir de las predicciones de un modelo en un conjunto de datos tratable por el software Aequitas. La estructura de los datos contendrá:\n",
    "\n",
    "* *score*: representa la conclusión a la que llega un modelo, puede ser puede ser binaria ($0$ o $1$) o continua (decimal entre $0$ y $1$). Esta decisión representa si el sujeto es apto o no, por ejemplo, si se le concede un crédito bancario.\n",
    "* *label_value*: representa los datos reales, es decir, si la predicción realizada por el modelo fue correcta. Por ejemplo, el sujeto fue capaz de devolver el crédito en su totalidad. Es por esto, por lo que el modelo solo puede ser auditado después de su aplicación y no antes. Se representa como un valor binario, $1$ significa que la predicción fue correcta, $0$ que no lo fue.\n",
    "* *protected attributes*: categorías de los atriburtos utilizados para decidir la equidad del modelo. Algunos ejemplos de atributos son la raza, sexo, educación, edad o ingresos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f99882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene a partir de las predicciones de un modelo un dataFrame con la estructura de Aequitas\n",
    "def get_aequitas_data(train_orig, preds_train, test_orig, preds_test):\n",
    "    # Realizamos una copia de los conjuntos de entrenamiento\n",
    "    train = copy.deepcopy(train_orig)\n",
    "    test  = copy.deepcopy(test_orig)\n",
    "    # Creamos la columna de score a partir de las predicciones\n",
    "    train['score'] = preds_train\n",
    "    test['score'] = preds_test\n",
    "    # Concatenamos ambos subconjuntos en un conjunto total\n",
    "    dataset = pd.concat([train,test])\n",
    "    dataset.rename(columns={'first_pf':'label_value'}, inplace=True)\n",
    "    # Utilizamos el criterio negativo o 0 a 0.0 y positivo a 1.0 para la puntuacion\n",
    "    dataset['score'] = dataset['score'].apply(lambda a: 0.0 if a <= 0 else 1.0)\n",
    "    # Creamos la columna entity_id\n",
    "    dataset['entity_id']= np.arange(1,len(dataset.iloc[:,0])+1,1)\n",
    "    # Eliminamos las columnas que no vayamos a usar\n",
    "    dataset = dataset.drop(['ZFYA'], axis=1)\n",
    "    dataset = dataset.drop(['LSAT'], axis=1)\n",
    "    dataset = dataset.drop(['UGPA'], axis=1)\n",
    "    dataset = dataset.drop(['sander_index'], axis=1)\n",
    "    dataset = dataset.drop(['region_first'], axis=1)\n",
    "    dataset = dataset.drop(['Male'], axis=1)\n",
    "    dataset = dataset.drop(['Female'], axis=1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbe919",
   "metadata": {},
   "source": [
    "Definiremos diferentes métodos para mostrar las tablas de métricas de grupo y sesgo y medidas de equidad a partir de las funciones que nos aporta Aequitas. Además crearemos una función para mostrar los gráficos de barras para los atributos de *score* y *label_value* de cada nuevo conjunto de datos generado por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4643ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda un gráfico para los atributos raza y sexo dado un dataframe\n",
    "def print_graficas(data, name, aq_palette, attr, title):\n",
    "    sns.countplot(x=\"race\", hue=attr, data=data[dataset_full.race.isin(['Hispanic', 'White', 'Black', 'Other'])], \n",
    "                  palette=aq_palette).set(title=title)\n",
    "    plt.show()\n",
    "    sns.countplot(x=\"sex\", hue=attr, data=data, palette=aq_palette).set(title=title)\n",
    "    plt.show()\n",
    "\n",
    "# Devuelve la tabla de las métricas de grupo para el conjunto pasado por parámetro\n",
    "def tabla_metrica_grupo(data):\n",
    "    g = Group()\n",
    "    xtab, _ = g.get_crosstabs(data)\n",
    "    absolute_metrics = g.list_absolute_metrics(xtab)\n",
    "    tabla_grupo = xtab[['attribute_name', 'attribute_value'] + absolute_metrics].round(2)\n",
    "    return tabla_grupo\n",
    "\n",
    "# Devuelve la tabla de las métricas de sesgo para el conjunto pasado por parámetro\n",
    "# También podemos especificar los atributos de referencia\n",
    "def tabla_metrica_sesgo(data, attr_ref):\n",
    "    # Calculamos las métricas de grupo\n",
    "    g = Group()\n",
    "    xtab, _ = g.get_crosstabs(data)\n",
    "    # Calculamos las metricas de sesgo\n",
    "    b = Bias()\n",
    "    # Establecemos los atributos de referencia\n",
    "    bdf = b.get_disparity_predefined_groups(xtab, original_df=data, ref_groups_dict=attr_ref, \n",
    "                                            alpha=0.05, mask_significance=True)\n",
    "    calculated_disparities = b.list_disparities(bdf)\n",
    "    disparity_significance = b.list_significance(bdf)\n",
    "    tabla_sesgo = bdf[['attribute_name', 'attribute_value'] +  calculated_disparities + disparity_significance]\n",
    "    return tabla_sesgo\n",
    "\n",
    "# Devuelve una tabla con si se sumplen o no las medidas de equidad para un cierto umbral\n",
    "def tabla_medidas_equidad(data, attr_ref, tau=0.8):\n",
    "    # Calculamos las métricas de grupo\n",
    "    g = Group()\n",
    "    xtab, _ = g.get_crosstabs(data)\n",
    "    # Calculamos las metricas de sesgo\n",
    "    b = Bias()\n",
    "    # Establecemos los atributos de referencia\n",
    "    bdf = b.get_disparity_predefined_groups(xtab, original_df=data, ref_groups_dict=attr_ref, \n",
    "                                            alpha=0.05, mask_significance=True)\n",
    "    # Definimos las medidas de equidad a partir de la tabla de metricas de sesgo\n",
    "    f = Fairness()\n",
    "    # Establecemos el valor del umbral con la variable tau\n",
    "    fdf = f.get_group_value_fairness(bdf, tau=tau)\n",
    "    # Tabla con si se cumplen las medidas de equidad para cada atributo\n",
    "    tabla_equidad = f.get_group_attribute_fairness(fdf)\n",
    "    return tabla_equidad, fdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa0ccd",
   "metadata": {},
   "source": [
    "Calculamos los nuevos conjuntos de datos para cada modelo siguiendo la estructura impuesta por Aequitas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6232a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Cálculo de las distribuciones del score\n",
    "    dataset_full = get_aequitas_data(train, preds_full_train, test, preds_full_test)\n",
    "    dataset_unaware = get_aequitas_data(train, preds_unaware_train, test, preds_unaware_test)\n",
    "    dataset_fair_k = get_aequitas_data(train, preds_fair_k_train, test, preds_fair_k_test)\n",
    "    dataset_fair_add = get_aequitas_data(train, preds_fair_add_train, test, preds_fair_add_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af585ed",
   "metadata": {},
   "source": [
    "Definimos las paletas de colores para las gráficas. Utilizaremos las mismas para el resto de experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d46bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Definimos las paletas de colores\n",
    "    aq_palette_score = sns.diverging_palette(255, 125, n=2)\n",
    "    aq_palette_label = sns.diverging_palette(5, 140, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f65c91",
   "metadata": {},
   "source": [
    "Mostramos las gráficas para las predicciones de los cuatro modelos. La gráfica de la distribución de las etiquetas siempre será la misma para los cuatro modelos, por lo que solo la imprimiremos una vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272e930",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Guardamos las gráficas de distribución del score para los diferentes conjuntos\n",
    "    print_graficas(dataset_full, \"label\", aq_palette_label, \"label_value\", \"Etiquetas reales\")\n",
    "    print_graficas(dataset_full, \"score_full\", aq_palette_score, \"score\", \"Modelo Completo\")\n",
    "    print_graficas(dataset_unaware, \"score_unaware\", aq_palette_score, \"score\", \"Modelo Desconocimiento\")\n",
    "    print_graficas(dataset_fair_k, \"score_fair_k\", aq_palette_score, \"score\", \"Modelo Fair K\")\n",
    "    print_graficas(dataset_fair_add, \"score_fair_add\", aq_palette_score, \"score\", \"Modelo Fair Add\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b1e36",
   "metadata": {},
   "source": [
    "Calculamos la tabla de las métricas de grupo para cada uno de los modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ee43c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Cálculo de las métricas de grupo\n",
    "    grupo_full = tabla_metrica_grupo(dataset_full)\n",
    "    grupo_unaware = tabla_metrica_grupo(dataset_unaware)\n",
    "    grupo_fair_k = tabla_metrica_grupo(dataset_fair_k)\n",
    "    grupo_fair_add = tabla_metrica_grupo(dataset_fair_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f74435",
   "metadata": {},
   "source": [
    "Calculamos las métricas de sesgo para cada modelo. Usaremos como grupo de referencia la raza *White* y el sexo *Male* por ser históricamente considerados como el grupo aventajado, respecto al resto de grupos demográficos. Las métricas de sesgo se calculan a partir de la siguiente fórmula:\n",
    "\n",
    "\\begin{equation*} \n",
    "\\text{Métrica Grupo }_{G(a_o)} =\\frac{\\text{Métrica Grupo }{a_o}}{\\text{Métrica Grupo }{a_r}}.\n",
    "\\end{equation*}\n",
    "\n",
    "Donde $a_o$ representa al grupo objetivo y $a_r$ al de referencia, y métrica de grupo corresponde con las presentadas en la tabla anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d90091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Establecemos los grupos de referencia para los atributos protegidos\n",
    "    attr_ref = {'race':'White', 'sex':'Male'}\n",
    "    # Cálculo de las métricas de sesgo\n",
    "    sesgo_full = tabla_metrica_sesgo(dataset_full, attr_ref)\n",
    "    sesgo_unaware = tabla_metrica_sesgo(dataset_unaware, attr_ref)\n",
    "    sesgo_fair_k = tabla_metrica_sesgo(dataset_fair_k, attr_ref)\n",
    "    sesgo_fair_add = tabla_metrica_sesgo(dataset_fair_add, attr_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb66d1d",
   "metadata": {},
   "source": [
    "Finalmente, calculamos la tabla con todas las medidas de equidad que implementa Aequitas considerando si se cumplen o no. La aceptación o denegación del criterio de equidad se realiza en base a la siguiente fórmula:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tau \\leq \\text{Métrica de sesgo }_{G} \\leq \\frac{1}{\\tau}.\n",
    "\\end{equation}\n",
    "\n",
    "Usamos $\\tau \\in (0,1]$ para controlar el rango de valores de disparidad que pueden considerarse justos. Para aplicar la regla del 80\\%, simplemente bastaría con tomar $\\tau=0.8$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a9aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Cálculo de las medidas de equidad\n",
    "    equidad_full, fdf_full = tabla_medidas_equidad(dataset_full, attr_ref)\n",
    "    equidad_unaware, fdf_unaware = tabla_medidas_equidad(dataset_unaware, attr_ref)\n",
    "    equidad_fair_k, fdf_fair_k = tabla_medidas_equidad(dataset_fair_k, attr_ref)\n",
    "    equidad_fair_add, fdf_fair_add = tabla_medidas_equidad(dataset_fair_add, attr_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e491f42",
   "metadata": {},
   "source": [
    "Para concluir con este apartado, imprimiremos la información relativa a las métricas de grupo, sesgo y equidad para los cuatro modelos. También lo complementaremos con unas cuantas gráficas para las métricas más importantes que trataremos:\n",
    "\n",
    "* **Paridad estadística** - Aequitas utiliza la tasa de positivos predichos (PPR).\n",
    "* **Probabilidades igualadas** - Cumplir simultáneamente las métricas asociadas a las tasas de falsos y verdaderos positivos (FPR y TPR, respectivamente).\n",
    "* **Tasa de paridad predictiva** - Cumplir simultáneamente los valores positivo y negativo predictivo (PPV y NPV, respectivamente). En Aequitas el PPV viene almacenado por su otro nombre: precision.\n",
    "* **Paridad tipo II** - Al encontrarnos con un problema de tipo asistencial, podría ser relevante el estudio de esta medida de equidad. Esta medida de equidad se cumple, cuando se aceptan como justas simultáneamente las métricas relativas a la tasa de falsa omisión (FOR) y falsos negativos (FNR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd209bb8",
   "metadata": {},
   "source": [
    "#### Modelo completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"Tabla métricas de grupo:\")\n",
    "    print(grupo_full)\n",
    "    print(\"\\nTabla métricas de sesgo:\")\n",
    "    print(sesgo_full)\n",
    "    print(\"\\nTabla medidas de equidad:\")\n",
    "    print(equidad_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc55d34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    # Creamos una instancia de la clase Plot() de Aequitas para dibujar gráficas\n",
    "    aqp = Plot()\n",
    "    # Mostramos el grafo para las métricas en las que nos vamos a enfocar para el problema\n",
    "    fg1 = aqp.plot_fairness_group_all(fdf_full, ncols=2, \n",
    "                                     metrics = ['ppr', 'fpr', 'tpr', 'precision','npv','for','fnr'])\n",
    "    m1 = aqp.plot_fairness_disparity_all(fdf_full, metrics=['ppr', 'fpr', 'tpr', 'precision','npv','for','fnr'], \n",
    "                                    attributes=['race','sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8baebcb",
   "metadata": {},
   "source": [
    "#### Modelo por desconocimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3134a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"Tabla métricas de grupo:\")\n",
    "    print(grupo_unaware)\n",
    "    print(\"\\nTabla métricas de sesgo:\")\n",
    "    print(sesgo_unaware)\n",
    "    print(\"\\nTabla medidas de equidad:\")\n",
    "    print(equidad_unaware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a10d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    # Mostramos el grafo para las métricas en las que nos vamos a enfocar para el problema\n",
    "    fg2 = aqp.plot_fairness_group_all(fdf_unaware, ncols=2, \n",
    "                                     metrics = ['ppr', 'fpr', 'tpr', 'precision','npv','for','fnr'])\n",
    "    m2 = aqp.plot_fairness_disparity_all(fdf_unaware, metrics=['ppr', 'fpr', 'tpr', 'precision','npv','for','fnr'], \n",
    "                                    attributes=['race','sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad2561",
   "metadata": {},
   "source": [
    "#### Modelo de variable latente (*Fair K*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031bcbdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    print(\"Tabla métricas de grupo:\")\n",
    "    print(grupo_fair_k)\n",
    "    print(\"\\nTabla métricas de sesgo:\")\n",
    "    print(sesgo_fair_k)\n",
    "    print(\"\\nTabla medidas de equidad:\")\n",
    "    print(equidad_fair_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef16a758",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    # Mostramos el grafo para las métricas en las que nos vamos a enfocar para el problema\n",
    "    fg3 = aqp.plot_fairness_group_all(fdf_fair_k, ncols=2, \n",
    "                                     metrics = ['ppr', 'fpr', 'tpr', 'precision','npv','for','fnr'])\n",
    "    m3 = aqp.plot_fairness_disparity_all(fdf_fair_k, metrics=['ppr', 'fpr', 'tpr', 'precision','npv','for','fnr'], \n",
    "                                    attributes=['race','sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df6d731",
   "metadata": {},
   "source": [
    "#### Modelo de error aditivo (*Fair Add*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21ab07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    print(\"Tabla métricas de grupo:\")\n",
    "    print(grupo_fair_add)\n",
    "    print(\"\\nTabla métricas de sesgo:\")\n",
    "    print(sesgo_fair_add)\n",
    "    print(\"\\nTabla medidas de equidad:\")\n",
    "    print(equidad_fair_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69afa74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    # Mostramos el grafo para las métricas en las que nos vamos a enfocar para el problema\n",
    "    fg4 = aqp.plot_fairness_group_all(fdf_fair_add, ncols=2, \n",
    "                                     metrics = ['ppr', 'fpr', 'tpr', 'precision','npv','for','fnr'])\n",
    "    m4 = aqp.plot_fairness_disparity_all(fdf_fair_add, metrics=['ppr', 'fpr', 'tpr', 'precision','npv',\n",
    "                                                                'for','fnr'], attributes=['race','sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eacb2e8",
   "metadata": {},
   "source": [
    "## Usando Aequitas sobre diferentes *datasets*\n",
    "\n",
    "Mostraremos dos ejemplos de uso de Aequitas haciendo uso de las diferentes alternativas que ofrece para auditar un *dataset*. Haremos uso de la API de Python que incluye un mayor número de métricas y gráficas para interpretar los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e95fc",
   "metadata": {},
   "source": [
    "### Puntuación del riesgo de reincidencia delictiva (COMPAS)\n",
    "\n",
    "**Descripción de los datos**: Los datos están basados en las estadísticas recogidas en el condado de Broward y puestas a disposición del público por la organización ProPublica. Este conjunto de datos contiene una puntuación del riesgo de reincidencia para 7.214 individuos, sus resultados reales de reincidencia a los dos años y una serie de atributos recogidos entre 2013 y 2014. Para tratar la puntuación de riesgo con Aequitas, convertiremos el *score* original (1-10) a uno binario donde 0 indica riesgo bajo y 1 medio o alto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fc4e85",
   "metadata": {},
   "source": [
    "1. Primero cargaremos el conjunto de los datos y mostraremos la distribución de sus puntuaciones (*score*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Cargamos el conjunto de datos\n",
    "    dataset_compas = pd.read_csv('./datos/compas_for_aequitas.csv', index_col=0)  \n",
    "    \n",
    "    # Hacemos un estudio previo de los individuos por score y etiqueta real\n",
    "    score_race = sns.countplot(x=\"race\", hue=\"score\", \n",
    "                data=dataset_compas[dataset_compas.race.isin(['African-American', 'Caucasian', \n",
    "                                                              'Hispanic', 'Other'])], palette=aq_palette_score)\n",
    "    plt.show()\n",
    "    score_sex = sns.countplot(x=\"sex\", hue=\"score\", data=dataset_compas, palette=aq_palette_score)\n",
    "    plt.show()\n",
    "    score_age = sns.countplot(x=\"age_cat\", hue=\"score\", data=dataset_compas, palette=aq_palette_score)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f5a86b",
   "metadata": {},
   "source": [
    "y sus etiquetas (*label_value*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c87e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    label_race = sns.countplot(x=\"race\", hue=\"label_value\", \n",
    "                data=dataset_compas[dataset_compas.race.isin(['African-American', 'Caucasian', \n",
    "                                                              'Hispanic', 'Other'])], palette=aq_palette_label)\n",
    "    plt.show()\n",
    "    label_sex = sns.countplot(x=\"sex\", hue=\"label_value\", data=dataset_compas, palette=aq_palette_label)\n",
    "    plt.show()\n",
    "    label_sex = sns.countplot(x=\"age_cat\", hue=\"label_value\", data=dataset_compas, palette=aq_palette_label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b0caab",
   "metadata": {},
   "source": [
    "2. Calcularemos el valor de las métricas de grupo, en su formato tabla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d63da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "    g = Group()\n",
    "    xtab, _ = g.get_crosstabs(dataset_compas)\n",
    "    absolute_metrics = g.list_absolute_metrics(xtab)\n",
    "    # Mostramos la tabla por pantalla\n",
    "    tabla_grupo_compas = xtab[['attribute_name', 'attribute_value'] + absolute_metrics].round(2)\n",
    "    print(tabla_grupo_compas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d35bf40",
   "metadata": {},
   "source": [
    "También podemos mostrar un gráfico para una o varias métricas de grupo específicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Calculamos diferentes gráficas para el conjunto de datos\n",
    "    aqp = Plot()\n",
    "    # Plot de los valores de las metricas de grupo para FNR\n",
    "    fnr = aqp.plot_group_metric(xtab, 'fnr')\n",
    "    # Plot de los valores de las metricas de grupo para FNR eliminando poblaciones con umbral de individuos\n",
    "    fnr = aqp.plot_group_metric(xtab, 'fnr', min_group_size=0.05)\n",
    "    # Metricas de grupo para todas las elegidas\n",
    "    p = aqp.plot_group_metric_all(xtab, metrics=['ppr','pprev','fnr','fpr'], ncols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8042f8",
   "metadata": {},
   "source": [
    "3. Calculamos la tabla para las métricas de sesgo. Recordemos que la métrica de sesgo se calcula utilizando la siguiente fórmula:\n",
    "\n",
    "\\begin{equation*} \n",
    "\\text{Métrica Grupo }_{G(a_o)} =\\frac{\\text{Métrica Grupo }{a_o}}{\\text{Métrica Grupo }{a_r}}.\n",
    "\\end{equation*}\n",
    "\n",
    "Tomaremos como referencia para el atributo raza el valor *Caucasian*, para el atributo sexo el valor *Male* y para la edad el intervalo *25-45*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Calculamos las metricas de sesgo\n",
    "    b = Bias()\n",
    "    # Establecemos los atributos de referencia\n",
    "    bdf = b.get_disparity_predefined_groups(xtab, original_df=dataset_compas, ref_groups_dict={'race':'Caucasian', \n",
    "          'sex':'Male', 'age_cat':'25 - 45'}, alpha=0.05, mask_significance=True)\n",
    "    calculated_disparities = b.list_disparities(bdf)\n",
    "    disparity_significance = b.list_significance(bdf)\n",
    "    # Mostramos la tabla de metricas de sesgo\n",
    "    print(bdf[['attribute_name', 'attribute_value'] +  calculated_disparities + disparity_significance])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9fd5e4",
   "metadata": {},
   "source": [
    "Podemos obtener gráficas de la disparidad sin tener en cuenta, por ahora, ningún concepto de equidad.\n",
    "Simplemente, comparando los valores obtenidos en la tabla anterior para los grupos objetivo, sobre el grupo\n",
    "de referencia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d235062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    aqp.plot_disparity(bdf, group_metric='fpr_disparity', attribute_name='race', significance_alpha=0.05)\n",
    "    j = aqp.plot_disparity_all(bdf, metrics=['precision_disparity', 'fpr_disparity'], attributes=['age_cat'], \n",
    "                               significance_alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2e5afa",
   "metadata": {},
   "source": [
    "4. Finalmente usamos $\\tau=0.8$ para calcular si se cumplen o no los diferentes criterios de justicia implementados en Aequitas. Decidiremos si cumple o no el criterio de justicia basándonos en la ya presentada fórmula:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tau \\leq \\text{Métrica de sesgo }_{G} \\leq \\frac{1}{\\tau}, \\quad \\tau \\in (0,1].\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c4ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Definimos las medidas de equidad a partir de la tabla de metricas de sesgo\n",
    "    f = Fairness()\n",
    "    # Establecemos el valor del umbral con la variable tau\n",
    "    fdf = f.get_group_value_fairness(bdf, tau=0.8)\n",
    "    #parity_detrminations = f.list_parities(fdf)\n",
    "    # Tabla con si se cumplen las medidas de equidad para cada atributo\n",
    "    gaf = f.get_group_attribute_fairness(fdf)\n",
    "    print(gaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342112b8",
   "metadata": {},
   "source": [
    "Podemos mostrar las gráficas para las métricas de sesgo, pero esta vez usando los colores verde y rojo indicando si cumplen o no la disparidad para el umbral establecido.\n",
    "\n",
    "Dado que en el marco del *COMPAS* las predicciones se utilizan para tomar decisiones de liberación previa al juicio, las intervenciones serán punitivas (proporcionar esta intervención a los individuos que son falsos positivos les perjudicará), así que deberemos tener en cuenta las tasas de falsos descubrimientos (FDR) y las tasas de falsos positivos (FPR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb84ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    # Metricas de grupo y de sesgo una vez aplicados los umbrales de equidad\n",
    "    fg = aqp.plot_fairness_group_all(fdf, ncols=2, metrics = ['ppr','pprev','fdr','fpr','for','fnr'])\n",
    "    m = aqp.plot_fairness_disparity_all(fdf, metrics=['fdr','fpr'], attributes=['race','sex','age_cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5ff423",
   "metadata": {},
   "source": [
    "### Predicción de notas en la facultad de derecho (law_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4256bc26",
   "metadata": {},
   "source": [
    "**Descripción de los datos**: El Consejo de Admisión de las Facultades de Derecho realizó una encuesta en 163 facultades de Derecho de Estados Unidos. Contiene información sobre 21.790 estudiantes de Derecho, tales como las puntuaciones de su examen de acceso (LSAT), su media del expediente (GPA) antes de entrar en la facultad, y su nota media del primer año (FYA) en la carrera de Derecho. A partir de estos datos, una escuela querría predecir si un solicitante tendrá un FYA alto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc988a",
   "metadata": {},
   "source": [
    "1. Cargamos el conjunto de los datos el cúal hemos preprocesado previamente para adaptarlo a la estructura de Aequitas, y mostraremos la distribución de sus puntuaciones (*score*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ec25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Cargamos el conjunto de datos\n",
    "    dataset_law = pd.read_csv('./datos/law_for_aequitas.csv', index_col=0)  \n",
    "    \n",
    "    # Hacemos un estudio previo de los individuos por score y etiqueta real\n",
    "    aq_palette_score = sns.diverging_palette(255, 125, n=2)\n",
    "    aq_palette_label = sns.diverging_palette(5, 140, n=2)\n",
    "    score_race = sns.countplot(x=\"race\", hue=\"score\", \n",
    "                data=dataset_law[dataset_law.race.isin(['Black', 'White', 'Hispanic', 'Other'])], \n",
    "                palette=aq_palette_score)\n",
    "    plt.show()\n",
    "    score_sex = sns.countplot(x=\"sex\", hue=\"score\", data=dataset_law, palette=aq_palette_score)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea51fcf",
   "metadata": {},
   "source": [
    "y sus etiquetas (*label_value*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7483ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "    label_race = sns.countplot(x=\"race\", hue=\"label_value\", \n",
    "                data=dataset_law[dataset_law.race.isin(['Black', 'White', 'Hispanic', 'Other'])], \n",
    "                palette=aq_palette_label)\n",
    "    plt.show()\n",
    "    label_sex = sns.countplot(x=\"sex\", hue=\"label_value\", data=dataset_law, palette=aq_palette_label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d08dee",
   "metadata": {},
   "source": [
    "2. Calcularemos el valor de las métricas de grupo, en su formato tabla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61abc1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    g = Group()\n",
    "    xtab, _ = g.get_crosstabs(dataset_law)\n",
    "    absolute_metrics = g.list_absolute_metrics(xtab)\n",
    "    # Mostramos la tabla por pantalla\n",
    "    tabla_grupo_law = xtab[['attribute_name', 'attribute_value'] + absolute_metrics].round(2)\n",
    "    print(tabla_grupo_law)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fece94",
   "metadata": {},
   "source": [
    "Mostramos un gráfico para una o varias métricas de grupo específicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Calculamos diferentes gráficas para el conjunto de datos\n",
    "    aqp = Plot()    \n",
    "    # Plot de los valores de las metricas de grupo para FNR\n",
    "    fnr = aqp.plot_group_metric(xtab, 'fnr')\n",
    "    # Plot de los valores de las metricas de grupo para FNR eliminando poblaciones con umbral de individuos\n",
    "    fnr = aqp.plot_group_metric(xtab, 'fnr', min_group_size=0.05)\n",
    "    # Metricas de grupo para todas las elegidas\n",
    "    p = aqp.plot_group_metric_all(xtab, metrics=['ppr','tnr','fpr','precision'], ncols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74505c1f",
   "metadata": {},
   "source": [
    "3. Calculamos la tabla para las métricas de sesgo. Tomamos como referencia para el atributo raza el valor *White* y para el atributo sexo el valor *Male*. Recordemos que la métrica de sesgo se calcula utilizando la siguiente fórmula:\n",
    "\n",
    "\\begin{equation*} \n",
    "\\text{Métrica Grupo }_{G(a_o)} =\\frac{\\text{Métrica Grupo }{a_o}}{\\text{Métrica Grupo }{a_r}}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Calculamos las metricas de sesgo\n",
    "    b = Bias()\n",
    "    # Establecemos los atributos de referencia\n",
    "    bdf = b.get_disparity_predefined_groups(xtab, original_df=dataset_law, ref_groups_dict={'race':'White', \n",
    "          'sex':'Male'}, alpha=0.05, mask_significance=True)\n",
    "    calculated_disparities = b.list_disparities(bdf)\n",
    "    disparity_significance = b.list_significance(bdf)\n",
    "    # Mostramos la tabla de metricas de sesgo\n",
    "    print(bdf[['attribute_name', 'attribute_value'] +  calculated_disparities + disparity_significance])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e1206",
   "metadata": {},
   "source": [
    "Mostraremos las gráficas de la disparidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f751d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    aqp.plot_disparity(bdf, group_metric='fpr_disparity', attribute_name='sex', significance_alpha=0.05)\n",
    "    j = aqp.plot_disparity_all(bdf, metrics=['for_disparity', 'ppr_disparity'], attributes=['race'], \n",
    "                               significance_alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd7c3c",
   "metadata": {},
   "source": [
    "4. Finalmente usamos $\\tau=0.8$ para calcular si se cumplen o no los diferentes criterios de justicia implementados en Aequitas. Decidiremos si cumple o no el criterio de justicia basándonos en la ya presentada fórmula:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tau \\leq \\text{Métrica de sesgo }_{G} \\leq \\frac{1}{\\tau}, \\quad \\tau \\in (0,1].\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Definimos las medidas de equidad a partir de la tabla de metricas de sesgo\n",
    "    f = Fairness()\n",
    "    # Establecemos el valor del umbral con la variable tau\n",
    "    fdf = f.get_group_value_fairness(bdf, tau=0.8)\n",
    "    #parity_detrminations = f.list_parities(fdf)\n",
    "    # Tabla con si se cumplen las medidas de equidad para cada atributo\n",
    "    gaf = f.get_group_attribute_fairness(fdf)\n",
    "    print(gaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b0b89a",
   "metadata": {},
   "source": [
    "Podemos mostrar las gráficas para las métricas de sesgo, pero esta vez usando los colores verde y rojo indicando si cumplen o no la disparidad para el umbral establecido.\n",
    "\n",
    "En el ámbito del problema dado que las predicciones se utilizan para tomar decisiones de asistenciales (proporcionar esta asistencia a los individuos que son falsos positivos no les perjudicará), así que deberemos tener en cuenta las tasas de falsas omisiones (FOR) y las tasas de falsos negativos (FNR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b0b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Metricas de grupo y de sesgo una vez aplicados los umbrales de equidad\n",
    "    fg = aqp.plot_fairness_group_all(fdf, ncols=2, metrics = ['ppr','pprev','fdr','fpr','for','fnr'])\n",
    "    m = aqp.plot_fairness_disparity_all(fdf, metrics=['for','fnr'], attributes=['race','sex'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
